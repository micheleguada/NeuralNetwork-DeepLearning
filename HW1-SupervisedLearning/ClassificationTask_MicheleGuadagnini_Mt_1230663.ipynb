{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9nMupgOkVa8"
   },
   "source": [
    "# NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "---\n",
    "A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "---\n",
    "\n",
    "\n",
    "# Homework 1 - Supervised Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx94AFKGkVbQ"
   },
   "source": [
    "### Author: Michele Guadagnini - Mt.1230663"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXE5SgM9EhQL"
   },
   "outputs": [],
   "source": [
    "# Total running time on Google Colab with GPU backend: < 30 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYQGjzJlkVbT"
   },
   "source": [
    "# Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TC5QuD24kVbW"
   },
   "outputs": [],
   "source": [
    "### ADDITIONAL LIBRARIES THAT NEED INSTALLATION (uncomment if needed)\n",
    "\n",
    "#!pip install torchinfo\n",
    "#!pip install optuna\n",
    "#!pip install pytorch-lightning\n",
    "#!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-xNC0ynkVba"
   },
   "outputs": [],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# python imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "# additional libraries\n",
    "from torchinfo import summary\n",
    "import optuna\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAkt_6G5nXcl"
   },
   "outputs": [],
   "source": [
    "# reduce verbosity \n",
    "logging.getLogger(\"optuna\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-r0HxCV0nXcq"
   },
   "outputs": [],
   "source": [
    "# setting the device\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU available')\n",
    "    device = torch.device(\"cuda\")\n",
    "    USE_GPU = True\n",
    "else:\n",
    "    print('GPU not available')\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Available CPU cores:\", os.cpu_count())\n",
    "    USE_GPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YEpqszUbUP1"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "\n",
    "# seed to set random states\n",
    "magic_num = 23 \n",
    "\n",
    "### set random state to have reproducible results\n",
    "seed_everything(seed=magic_num) \n",
    "### the function above internally calls the followings:\n",
    "#    random.seed(seed)\n",
    "#    np.random.seed(seed)\n",
    "#    torch.manual_seed(seed)\n",
    "#    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPTFyWeOyiR-"
   },
   "source": [
    "## Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8E-82AGylxj"
   },
   "source": [
    "* The goal is to train a neural network that maps an input image (from fashionMNIST) to one of ten classes (multi-class classification problem with mutually exclusive classes).\n",
    "* Define a proper loss (e.g. [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss))\n",
    "* Also here, consider to create a validation set from you training data, or use a k-fold cross-validation strategy.\n",
    "* Pay attention to the shape, data type and output values range. If needed, modify them accordingly to your implementation (read carefully the documentation of the layers that you use, e.g. [torch.nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)).\n",
    "* Explore different optimizers, acivation functions, network architectures. Analyze the effect of different regularization methods, such as dropout layers, random transformations (image rotation, scaling, add noise...) or L2 regularization (weight decay)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHf-4soYV0z7"
   },
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvuju0INkVbm"
   },
   "source": [
    "## Outline of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZijsTt1wnXc1"
   },
   "source": [
    "1. **Dataset download and organization**: FashionMNIST dataset is downloaded directly with the PyTorch utility inside `torchvision` package. The custom datamodule class to use with *PyTorch_Lightning* framework is defined, together with some random transformations to apply to data samples with the aim of improving generalization capabilities of trained model.\n",
    "1. **Building candidates models**: a Convolutional Neural Networks (CNN) model is created. It is allowed to assume different hyperparameters that will be tuned in the following section.\n",
    "1. **Selection of best model & hyper-parameters set**: architectures and hyper-parameters sets are coarsly sampled from a defined range of values and evaluated using `optuna` package.\n",
    "1. **Training the selected model**: the best model is trained again using the full dataset and also the random transformations defined in section 1. Also a checkpointing callback is implemented to store in memory the model with best validation loss. \n",
    "1. **Evaluation of final performance**: the model is tested over the test dataset, computing loss, accuracy and confusion matrix.\n",
    "1. **Analysis of the network**: the trained model is investigated by visualizing the learned filters in the convolutional layers and the feature maps produced in the forward pass of a test image. Also, some synthetic images are produced by maximizing the mean activation of a particular filter or output neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsvBiTw7ykT5"
   },
   "source": [
    "## Dataset download and organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYaGs-I_zRk-"
   },
   "source": [
    "Download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wRuRn08zVO9"
   },
   "outputs": [],
   "source": [
    "# folder to contain dataset\n",
    "DATA_DIR_NAME = 'classifier_data'\n",
    "\n",
    "# check if download is needed\n",
    "to_download = not os.path.isdir(DATA_DIR_NAME) \n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(DATA_DIR_NAME, \n",
    "                                                  train=True , \n",
    "                                                  download=to_download,\n",
    "                                                 )\n",
    "\n",
    "label_names = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRymSexGnXc7"
   },
   "outputs": [],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUIZu5YpzMT9"
   },
   "source": [
    "How to get an image and the corresponding label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGXmy4mNwfk4"
   },
   "outputs": [],
   "source": [
    "sample_index = 6\n",
    "image = train_dataset[sample_index][0]\n",
    "label = train_dataset[sample_index][1]\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "plt.imshow(image, cmap='Greys')\n",
    "print(f\"SAMPLE AT INDEX {sample_index}\")\n",
    "print(f\"LABEL: {label} ({label_names[label]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SesSIfKyzMJ"
   },
   "source": [
    "The output of the dataset is a PIL Image, a python object specifically developed to manage and process images. PyTorch supports this format, and there are useful transforms available natively in the framework: https://pytorch.org/docs/stable/torchvision/transforms.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7yHQxxxnXdC"
   },
   "source": [
    "Since we will make use of `PyTorch-Lightning`, we define here the datamodule object we will use to create a separate validation set and the dataloaders. Also, we introduce the parameters:\n",
    "* `Nsamples`, to eventually reduce the size of the dataset;\n",
    "* `transform`, to optionally pass a customized transformation to use on training data;\n",
    "* `valid_frac`, fraction of train dataset to use for validation;\n",
    "* `random_state`, seed number for the random split generator;\n",
    "\n",
    "We define also the class `DatasetFromSubset` to create dataset objects from the subsets of data returned by the splitter function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Zy7zEnanXdD"
   },
   "outputs": [],
   "source": [
    "class DatasetFromSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "class FashionMNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size, Nsamples=None, valid_frac=None, \n",
    "                 random_state=42, transform=None ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.Nsamples = Nsamples # number of samples to load from the train dataset\n",
    "        self.frac = valid_frac   # fraction of samples to use as validation set\n",
    "        self.seed = random_state\n",
    "        \n",
    "        if transform is None:\n",
    "            self.transform = transforms.ToTensor()\n",
    "        else:\n",
    "            self.transform = transform\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        ### train dataset\n",
    "        self.full = torchvision.datasets.FashionMNIST(\n",
    "            self.data_dir, train=True, download=False,\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # restricting to Nsamples \n",
    "        if self.Nsamples is not None:\n",
    "            self.full = Subset(self.full, np.random.choice(range(self.full.__len__()), size=self.Nsamples))\n",
    "            \n",
    "        # split into train and validation \n",
    "        split_tr  = round( self.full.__len__()*(1-self.frac) )\n",
    "        split_val = round( self.full.__len__()*self.frac )\n",
    "        \n",
    "        train, val = random_split(\n",
    "            self.full,\n",
    "            [split_tr, split_val], \n",
    "            generator=torch.Generator().manual_seed(self.seed),\n",
    "        )\n",
    "        \n",
    "        # add transforms\n",
    "        self.mnist_train = DatasetFromSubset(subset=train, transform=self.transform)\n",
    "        self.mnist_val   = DatasetFromSubset(subset=val  , transform=transforms.ToTensor())    \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_train, batch_size=self.batch_size, shuffle=True, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        batch_size = min(self.mnist_val.__len__(), 1024)\n",
    "        return DataLoader(\n",
    "            self.mnist_val, batch_size=batch_size, shuffle=False, pin_memory=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7Xgik3QbUQG"
   },
   "source": [
    "Here we create a set of custom random transformations to test as regularization:\n",
    "* `AddGaussianNoise`: add a gaussian noise with given mean and std with a certain probability;\n",
    "* `AddOcclusion`: add a rectangular occlusion to image with a certain probability.\n",
    "\n",
    "We will make use also of **horizontal and vertical random flipping** exploiting provided functions inside `torchvision` package.\n",
    "\n",
    "Also other transformations could have been applied, like blurring, random rotation, random cropping ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfdvyOdobUQH"
   },
   "outputs": [],
   "source": [
    "# transform that add a gaussian noise with given mean and std with a certain probability.\n",
    "class AddGaussianNoise():\n",
    "    def __init__(self, mean=0., std=1., prob=0.5):\n",
    "        \"\"\"\n",
    "        mean : mean of the gaussian noise\n",
    "        std  : std of the gaussian noise\n",
    "        prob : occurring probability of the transformation\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std  = std\n",
    "        self.prob = prob\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        if torch.rand(1) < self.prob:\n",
    "            # generating and adding noise\n",
    "            tensor += torch.randn(tensor.size())*self.std + self.mean\n",
    "            \n",
    "            # returning pixels values in range [0,1]\n",
    "            min_val = torch.min(tensor)\n",
    "            tensor -= min_val\n",
    "            max_val = torch.max(tensor)\n",
    "            tensor /= max_val\n",
    "            return tensor\n",
    "        else:\n",
    "            return tensor\n",
    "    \n",
    "# transform that add a rectangular occlusion to image with a certain probability\n",
    "class AddOcclusion():\n",
    "    def __init__(self, max_area=0.5, prob=0.5):\n",
    "        \"\"\"\n",
    "        max_area : maximum fraction of image area that is allowed to be covered by occlusion\n",
    "        prob     : occurring probability of the transformation\n",
    "        \"\"\"\n",
    "        self.max_area = max_area\n",
    "        self.prob     = prob\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        if torch.rand(1) < self.prob:\n",
    "            # taking random box vertices\n",
    "            xs = np.rint( np.random.rand(2)*tensor.size()[1] )\n",
    "            ys = np.rint( np.random.rand(2)*tensor.size()[2] ) \n",
    "            \n",
    "            # ordering the vertices\n",
    "            xs = np.sort(xs.astype(int))\n",
    "            ys = np.sort(ys.astype(int))\n",
    "            \n",
    "            # checking if occluded area is greater than max_area\n",
    "            max_area_pxs = tensor.size()[1]*tensor.size()[2] * self.max_area\n",
    "            if (xs[1]-xs[0])*(ys[1]-ys[0]) > max_area_pxs:\n",
    "                xs[1] = xs[1]//2\n",
    "                ys[1] = ys[1]//2\n",
    "            \n",
    "            tensor[:,xs[0]:xs[1],ys[0]:ys[1]] = 0.\n",
    "            return tensor\n",
    "        else:\n",
    "            return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yy0r2IJfbUQI"
   },
   "source": [
    "In the following cell we check that the transformations defined above works properly and plot an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxyLPcrpbUQJ"
   },
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "img = to_tensor(train_dataset[0][0])\n",
    "\n",
    "# add gaussian noise\n",
    "noiser = AddGaussianNoise(0., 0.1, prob=1.)\n",
    "img_noisy = noiser(img.detach().clone())\n",
    "\n",
    "# add occlusion\n",
    "occluder = AddOcclusion(max_area=0.5, prob=1.)\n",
    "img_occluded = occluder(img.detach().clone())\n",
    "\n",
    "# add horizontal flipping\n",
    "flipper = transforms.RandomVerticalFlip(p=1.)\n",
    "img_flipped = flipper(img.detach().clone())\n",
    "\n",
    "print(\"Original: \", img.size(), \", \", img.dtype)\n",
    "print(\"Noisy   : \", img_noisy.size(), \", \", img_noisy.dtype)\n",
    "print(\"Occluded: \", img_occluded.size(), \", \", img_occluded.dtype)\n",
    "print(\"Flipped : \", img_flipped.size(), \", \", img_flipped.dtype)\n",
    "\n",
    "# plot \n",
    "fig, axs = plt.subplots(1,4,figsize=(16,4))\n",
    "axs[0].imshow(img.squeeze(), cmap=\"Greys\")\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(img_noisy.squeeze(), cmap=\"Greys\")\n",
    "axs[1].set_title(\"Noisy\")\n",
    "axs[2].imshow(img_occluded.squeeze(), cmap=\"Greys\")\n",
    "axs[2].set_title(\"Occluded\")\n",
    "axs[3].imshow(img_flipped.squeeze(), cmap=\"Greys\")\n",
    "axs[3].set_title(\"Flipped\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCSIAJqknXdE"
   },
   "source": [
    "## Building Candidate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2UiZurDnXdF"
   },
   "source": [
    "Here we create the models that will be tested on our classification task. The model created is called `ConsecutiveConv` and it is a composition of a variable number of consecutive convolutional layers. Also, all convolutional parameters (feature maps, kernel sizes, strides, padding) can be passed as arguments in the `__init__` function (inside the `params` dict). The activation function is fixed to `ReLU`.  <br>\n",
    "The model is completed by a fully-connected part composed of 2 hidden layers and an output layer, whose hyper-parameters are only the number of neurons in each layer. <br>\n",
    "It is also possible to activate batch normalization after each layer by mean of the boolean flag `batch_norm` in the model initialization function. <br>\n",
    "\n",
    "Hyper-parameters will be tuned by running a study with the `optuna` framework. <br>\n",
    "The search for a good architecture of course is not exhaustive, and also other things could have been tested (like dropout for example, or pooling layers), but it would have required more time. Still, we should be able to solve the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwgbk1-QnXdG"
   },
   "outputs": [],
   "source": [
    "def output_shape(dim, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    \"\"\"\n",
    "    Utility function that computes the output of a convolutional or pooling layer for a given input.\n",
    "        Shape of the image and kernel are assumed to be square.\n",
    "        Also, stride, pad and dilation are assumed to be symmetric.\n",
    "    \"\"\"\n",
    "    out_dim = int( ((dim + (2*pad) - (dilation*(kernel_size-1)) -1 )/ stride) +1 )\n",
    "    return out_dim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmJJD4vqnXdK"
   },
   "outputs": [],
   "source": [
    "# consecutive convolutional layers without pooling\n",
    "class ConsecutiveConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, params=None):\n",
    "        '''\n",
    "        Convolutional hyper-parameters\n",
    "            input_size    : int representing image size in pixels (assumed to be squared)\n",
    "            n_conv        : int representing number of convolutional layers\n",
    "            conv_features : list containing number of feature maps in each conv. layer\n",
    "            kernel_s      : kernel size of conv. layers (list)\n",
    "            stride_s      : stride of conv. layers (list)\n",
    "            padding_s     : paddings of conv. layers (list)\n",
    "        \n",
    "        Fully-connected hyper-parameters\n",
    "            Nl1 & Nl2 : number of units in 1st & 2nd linear layers\n",
    "            No        : number of output units\n",
    "\n",
    "        batch_norm : boolean flag to activate batch normalization on both conv. and linear layers\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        if params is not None:\n",
    "            self.hp = params\n",
    "        else:\n",
    "            self.hp = {\n",
    "                \"n_conv\"        : 3,\n",
    "                \"conv_features\" : [8,8,8],\n",
    "                \"kernel_s\"      : [5,3,3],\n",
    "                \"stride_s\"      : [2,2,2],\n",
    "                \"padding_s\"     : [0,0,0],\n",
    "                \"Nl1\"           : 16,\n",
    "                \"Nl2\"           : 16,\n",
    "                \"No\"            : 10,\n",
    "                \"batch_norm\"    : False,\n",
    "            }\n",
    "        \n",
    "#       ### convolutional layers\n",
    "        conv_list = []       \n",
    "        channels = self.hp[\"conv_features\"].copy()\n",
    "        channels.append(1)  # to be used as number of input channels in the 1st layer\n",
    "        for it in range(self.hp[\"n_conv\"]):\n",
    "            conv_list.append(nn.Conv2d(in_channels=channels[it-1], \n",
    "                                       out_channels=channels[it], \n",
    "                                       kernel_size=self.hp[\"kernel_s\"][it], \n",
    "                                       stride=self.hp[\"stride_s\"][it], \n",
    "                                       padding=self.hp[\"padding_s\"][it],\n",
    "                                      )\n",
    "                            )      \n",
    "            if self.hp[\"batch_norm\"]:\n",
    "                conv_list.append(nn.BatchNorm2d(self.hp[\"conv_features\"][it]))\n",
    "            conv_list.append(nn.ReLU(inplace=True))\n",
    "         \n",
    "        self.conv = nn.Sequential(*conv_list)\n",
    "        \n",
    "        # Ni is the number of units needed after the flatten layer\n",
    "        out_dim = input_size\n",
    "        for it in range(self.hp[\"n_conv\"]):\n",
    "            out_dim = output_shape(out_dim, self.hp[\"kernel_s\"][it], \n",
    "                                   self.hp[\"stride_s\"][it], self.hp[\"padding_s\"][it]\n",
    "                                  )\n",
    "        Ni = self.hp[\"conv_features\"][self.hp[\"n_conv\"]-1]*out_dim*out_dim\n",
    "        \n",
    "        # flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        \n",
    "#       ### fully-connected layers\n",
    "        fc_list = []\n",
    "        Nunits = [Ni, self.hp[\"Nl1\"], self.hp[\"Nl2\"], self.hp[\"No\"]]\n",
    "        \n",
    "        for it in range(len(Nunits)-1):            \n",
    "            fc_list.append(nn.Linear(in_features=Nunits[it], out_features=Nunits[it+1]))\n",
    "            if it != (len(Nunits)-2):\n",
    "                if self.hp[\"batch_norm\"]:\n",
    "                    fc_list.append(nn.BatchNorm1d(Nunits[it+1]))\n",
    "                fc_list.append(nn.ReLU(inplace=True))\n",
    "                \n",
    "        self.fc = nn.Sequential(*fc_list)\n",
    "        \n",
    "            \n",
    "    def forward(self, x, additional_out=False):\n",
    "        \n",
    "        # convolutional part\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "           \n",
    "        # fully-connected part\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efhwsuE6nXdM"
   },
   "source": [
    "Below we create `pytorch-lightning` module to wrap up the model defined above and train it inside this framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7SofvRHnXdN"
   },
   "outputs": [],
   "source": [
    "# pytorch-lightning module \n",
    "class Lit_CNN(pl.LightningModule): \n",
    "    def __init__(self, input_size, arch, params=None, optimizer=None, learning_rate=0.001, L2_penalty=0.):\n",
    "        '''\n",
    "        input_size    : size of the image in pixels (assumed square)\n",
    "        arch          : model class (torch.nn.Module)\n",
    "        params        : model hyper-parameters\n",
    "        optimizer     : optimizer object (if None, optim.Adam is used)\n",
    "        learning_rate : initial learning rate\n",
    "        L2_penalty    : coefficient of L2 regularization\n",
    "        '''    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = arch(input_size, params)\n",
    "        \n",
    "        if optimizer is not None:\n",
    "            self.optim = optimizer\n",
    "        else:\n",
    "            self.optim = optim.Adam\n",
    "            \n",
    "        self.lr = learning_rate\n",
    "        self.L2 = L2_penalty\n",
    "\n",
    "        self.min_val_loss = 10000.0\n",
    "\n",
    "    def forward(self, x, additional_out=False):\n",
    "        return self.network(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx=None):\n",
    "        data, target = batch\n",
    "        output = self(data)  \n",
    "        \n",
    "        train_loss = nn.functional.cross_entropy(output, target)\n",
    "        self.log(\"train_loss\", train_loss.item(), on_step=False, on_epoch=True)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx=None):\n",
    "        data, target = batch\n",
    "        output = self(data)\n",
    "        \n",
    "        val_loss = nn.functional.cross_entropy(output, target)\n",
    "        self.log(\"val_loss\", val_loss.item(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # store minimum reached validation loss\n",
    "        if self.min_val_loss > val_loss.item():\n",
    "            self.min_val_loss = val_loss.item()\n",
    "            self.log(\"min_val_loss\", self.min_val_loss, on_step=False, on_epoch=True)\n",
    "        return\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return self.optim(self.network.parameters(), self.lr, weight_decay=self.L2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxS4DOWQnXdN"
   },
   "source": [
    "## Best model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS_Nir1dnXdP"
   },
   "source": [
    "To select a proper model for this task we use the `optuna` package to run a search over hyperparameters space, which is defined in the cells below. <br>\n",
    "The `objective` function takes care of sampling the hyperparameters, then it defines the model, the trainer and the datamodule and finally it runs the optimization. This function will be called by the optuna `study` object. <br>\n",
    "The hyper-parameters for each iteration are sampled randomly only in the first few steps, then optuna uses by default a [TPESampler](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.TPESampler.html), which uses information obtained from previous trials to choose new hyper-parameters configuration to test with a technique called *Tree-structured Parzen Estimator* (TPE). <br>\n",
    "Since optimizing all the hyper-parameters would have required too much time, for some hyper-parameters that we expect to be less relevant we set a value which we suppose to be good and optimized only the remaining ones. We ended up testing the architectures configurations created, optimizing hyper-parameters of the convolutional part of the networks and the learning rate of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4hQyHVjbUQl"
   },
   "outputs": [],
   "source": [
    "# defining list of possible values for hyper-parameters\n",
    "\n",
    "n_feature_maps = [32,48,64,96,128]\n",
    "\n",
    "optim_list = [optim.Adam, optim.Adadelta, optim.SGD]\n",
    "\n",
    "batch_sizes = [256,512,1024]        \n",
    "\n",
    "# architecture configs\n",
    "configs = [{\"n_conv\"   : 2,    # shape through network: 28  -> 12  -> 8\n",
    "            \"kernel_s\" : [5,5],\n",
    "            \"stride_s\" : [2,1],\n",
    "            \"padding_s\": [0,0],\n",
    "           },\n",
    "           {\"n_conv\"   : 2,    # shape through network: 28  -> 8  -> 8\n",
    "            \"kernel_s\" : [7,3],\n",
    "            \"stride_s\" : [3,1],\n",
    "            \"padding_s\": [0,1],\n",
    "           },\n",
    "           {\"n_conv\"   : 2,    # shape through network: 28  -> 13  -> 6\n",
    "            \"kernel_s\" : [3,3],\n",
    "            \"stride_s\" : [2,2],\n",
    "            \"padding_s\": [0,0],\n",
    "           },\n",
    "           {\"n_conv\"   : 3,      # shape through network: 28  -> 13  -> 7  -> 7\n",
    "            \"kernel_s\" : [5,3,3],\n",
    "            \"stride_s\" : [2,2,1],\n",
    "            \"padding_s\": [1,1,1],\n",
    "           },\n",
    "           {\"n_conv\"   : 3,      # shape through network: 28  -> 13  -> 8  -> 8  \n",
    "            \"kernel_s\" : [7,3,3],\n",
    "            \"stride_s\" : [2,2,1],\n",
    "            \"padding_s\": [2,2,1], \n",
    "           },\n",
    "           {\"n_conv\"   : 3,      # shape through network: 28  -> 6  -> 6  -> 6 \n",
    "            \"kernel_s\" : [9,5,3],\n",
    "            \"stride_s\" : [4,1,1],\n",
    "            \"padding_s\": [2,2,1], \n",
    "           },           \n",
    "           {\"n_conv\"   : 3,      # shape through network: 28  -> 13  -> 7  -> 7  \n",
    "            \"kernel_s\" : [5,5,3],\n",
    "            \"stride_s\" : [2,2,1],\n",
    "            \"padding_s\": [1,2,1], \n",
    "           },  \n",
    "           {\"n_conv\"   : 3,      # shape through network: 28  -> 10  -> 10  -> 10  \n",
    "            \"kernel_s\" : [3,3,3],\n",
    "            \"stride_s\" : [3,1,1],\n",
    "            \"padding_s\": [1,1,1], \n",
    "           },           \n",
    "           {\"n_conv\"   : 4,        # shape through network: 28  -> 13  -> 7  -> 5  -> 5\n",
    "            \"kernel_s\" : [7,5,5,3],\n",
    "            \"stride_s\" : [2,2,1,1],\n",
    "            \"padding_s\": [2,2,1,1], \n",
    "           },           \n",
    "           {\"n_conv\"   : 4,        # shape through network: 28  -> 13  -> 7  -> 5  -> 5 \n",
    "            \"kernel_s\" : [5,3,3,3],\n",
    "            \"stride_s\" : [2,2,1,1],\n",
    "            \"padding_s\": [1,1,0,1], \n",
    "           },\n",
    "           {\"n_conv\"   : 4,        # shape through network: 28  -> 8  -> 8  -> 8  -> 8 \n",
    "            \"kernel_s\" : [9,3,3,3],\n",
    "            \"stride_s\" : [3,1,1,1],\n",
    "            \"padding_s\": [2,1,1,1], \n",
    "           },\n",
    "           {\"n_conv\"   : 4,        # shape through network: 28  -> 13  -> 8  -> 6  -> 6  \n",
    "            \"kernel_s\" : [7,3,3,3],\n",
    "            \"stride_s\" : [2,2,1,1],\n",
    "            \"padding_s\": [2,2,0,1], \n",
    "           },\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9rufSuNnXdP"
   },
   "outputs": [],
   "source": [
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# optuna objective\n",
    "def objective(trial):\n",
    "    \n",
    "    print(f\"Trial [{trial.number}] started at:\", datetime.datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\"))\n",
    "    \n",
    "    ### define model architecture and hyper-parameters\n",
    "    # select model architecture\n",
    "    arch = ConsecutiveConv\n",
    "\n",
    "    conf = configs[trial.suggest_int(\"config_id\", 0, len(configs)-1)]\n",
    "    params = {\"n_conv\"        : conf[\"n_conv\"],\n",
    "              \"conv_features\" : [trial.suggest_categorical(f\"conv_features{i}\", n_feature_maps) for i in range(conf[\"n_conv\"])],\n",
    "              \"kernel_s\"      : conf[\"kernel_s\"],\n",
    "              \"stride_s\"      : conf[\"stride_s\"],\n",
    "              \"padding_s\"     : conf[\"padding_s\"],\n",
    "              \"Nl1\"           : 1024, #trial.suggest_int(\"Nl1\", 256, 1024, step=16),\n",
    "              \"Nl2\"           : 320,  #trial.suggest_int(\"Nl2\", 64 , 320 , step=8 ),\n",
    "              \"No\"            : 10,\n",
    "              \"batch_norm\"    : True, #fixed to True since in all experiments we saw that \n",
    "                                      # it always accelerates the convergence\n",
    "             }\n",
    "    \n",
    "    # others\n",
    "    optimizer     = optim_list[0]   #optim_list[trial.suggest_int(\"optim_id\", 0, len(optim_list)-1)]\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    L2_penalty    = trial.suggest_float(\"L2_penalty\", 1e-5, 1e-1, log=True)\n",
    "    batch_size    = batch_sizes[0]  #trial.suggest_categorical(\"batch_size\", batch_sizes)\n",
    "\n",
    "    model = Lit_CNN(input_size = 28,\n",
    "                    arch = arch,\n",
    "                    params = params,\n",
    "                    optimizer = optimizer,\n",
    "                    learning_rate = learning_rate,\n",
    "                    L2_penalty    = L2_penalty,\n",
    "                   )\n",
    "\n",
    "    ### define datamodule    \n",
    "    Nsamples = 8192  # while optimizing hyper-parameters we reduce to a fraction of the dataset to save time\n",
    "    datamodule = FashionMNISTDataModule(data_dir   = DATA_DIR_NAME, \n",
    "                                        batch_size = batch_size,\n",
    "                                        Nsamples   = Nsamples,\n",
    "                                        valid_frac = 1./8.,\n",
    "                                        random_state = magic_num,\n",
    "                                       )    \n",
    "    ### define trainer\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", \n",
    "                               min_delta = 0.001, \n",
    "                               patience  = 5,\n",
    "                               verbose   = False, \n",
    "                               check_on_train_epoch_end=False, # check early_stop at end of validation\n",
    "                              )\n",
    "    pl_pruning = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "\n",
    "    epochs = 30\n",
    "    trainer = pl.Trainer(logger     = False,\n",
    "                         max_epochs = epochs,\n",
    "                         gpus       = 1 if USE_GPU else None, #trainer will take care of moving model and datamodule to GPU\n",
    "                         callbacks  = [pl_pruning, early_stop],\n",
    "                         enable_checkpointing = False,\n",
    "                         enable_model_summary = False,\n",
    "                         deterministic = True,  #use deterministic algorithms to ensure reproducibility\n",
    "                        )\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "    \n",
    "    # storing hyper-parameters as user attribute of trial object for convienience\n",
    "    hypers = {\"arch\"         : arch,\n",
    "              \"params\"       : params,\n",
    "              \"optimizer\"    : optimizer,\n",
    "              \"learning_rate\": learning_rate,\n",
    "              \"L2_penalty\"   : L2_penalty,\n",
    "              \"batch_size\"   : batch_size,\n",
    "             }\n",
    "    trial.set_user_attr(\"hypers\", hypers)\n",
    "    \n",
    "    final_valid_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "    \n",
    "    # getting minimum reached validation loss\n",
    "    try:\n",
    "        min_valid_loss = trainer.callback_metrics[\"min_val_loss\"].item()\n",
    "    except:\n",
    "        print(\"INFO: No 'min_val_loss' value logged\")\n",
    "        min_valid_loss = final_valid_loss\n",
    "\n",
    "    print(f\"Trial [{trial.number}] ended at:\", datetime.datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")) \n",
    "    print(f\"    Valid. loss: {final_valid_loss}; min valid. loss: {min_valid_loss}\\n\")\n",
    "    return min_valid_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoCORHzl3CQ9"
   },
   "source": [
    "Below we define and run the study. We also set up a **pruner** to stop unpromising trials and save time and an **early stopping** callback monitoring validation loss. <br> The pruner is of type [`MedianPruner`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.MedianPruner.html#optuna.pruners.MedianPruner), which uses the *median stopping rule*: it prunes the current trial if the trialâ€™s best intermediate result is worse than median of intermediate results of previous trials at the same step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0V5XPvn4nXdR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(seed=magic_num) \n",
    "\n",
    "### MedianPruner\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=10, # trials to complete before starting to prune\n",
    "                                     n_warmup_steps=20,   # steps to take before evaluating pruning\n",
    "                                     interval_steps=10,   # steps between trail pruning checks\n",
    "                                    )\n",
    "\n",
    "# Make the default sampler behave in a deterministic way\n",
    "sampler = optuna.samplers.TPESampler(seed=magic_num,\n",
    "                                     n_startup_trials=10, # use random sampling at beginning\n",
    "                                     )\n",
    "\n",
    "### create study\n",
    "study = optuna.create_study(study_name = \"Lit_CNN_tuning\", \n",
    "                            direction  = \"minimize\",\n",
    "                            pruner     = pruner,\n",
    "                            sampler    = sampler,\n",
    "                           )\n",
    "\n",
    "### run optimization\n",
    "Ntrials = 30   #\n",
    "MaxTime = None # \n",
    "logging.captureWarnings(True)\n",
    "study.optimize(objective, \n",
    "               n_trials       = Ntrials, \n",
    "               timeout        = MaxTime, # timeout in seconds\n",
    "               gc_after_trial = True,    # run garbage collection \n",
    "              )\n",
    "logging.captureWarnings(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ro2j5TaunXdR"
   },
   "outputs": [],
   "source": [
    "print(\"Number of trials: {}\".format(len(study.trials)))\n",
    "\n",
    "trial = study.best_trial\n",
    "print(\"Best trial: #{}\".format(trial.number))\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Optimized params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"     {}: {}\".format(key, value))\n",
    "    if key == \"config_id\":  # print conv2d configs if present\n",
    "        for kk, vv in configs[value].items():\n",
    "            print(\"         {}: {}\".format(kk, vv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_dQ33EsnXdT"
   },
   "outputs": [],
   "source": [
    "import optuna.visualization as optunaplt\n",
    "\n",
    "optunaplt.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMYX0i33-QkR"
   },
   "outputs": [],
   "source": [
    "# print dataframe with study results\n",
    "study_df = study.trials_dataframe()\n",
    "\n",
    "study_df.drop(columns=\"user_attrs_hypers\", inplace=True)\n",
    "\n",
    "study_df = study_df.sort_values(by=\"value\")\n",
    "\n",
    "study_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So6e58mynXdU"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-sy5FQc19kV"
   },
   "source": [
    "Now we have a promising model that we will train on the whole train dataset. Below we report a summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iE9kpzRbUQs"
   },
   "outputs": [],
   "source": [
    "print(\"Summary of the best model resulting from the optuna search:\")\n",
    "\n",
    "hyper_pars = study.best_trial.user_attrs[\"hypers\"]\n",
    "\n",
    "model = Lit_CNN(input_size    = 28,\n",
    "                arch          = hyper_pars[\"arch\"],\n",
    "                params        = hyper_pars[\"params\"],\n",
    "                optimizer     = hyper_pars[\"optimizer\"],\n",
    "                learning_rate = hyper_pars[\"learning_rate\"],\n",
    "                L2_penalty    = hyper_pars[\"L2_penalty\"],\n",
    "               )\n",
    "\n",
    "dummy_batch_shape = [hyper_pars[\"batch_size\"], 1, 28, 28]\n",
    "summary(model, dummy_batch_shape, \n",
    "        col_width = 18, \n",
    "        col_names = (\"input_size\",\"output_size\",\"num_params\",\"mult_adds\",),\n",
    "        depth     = 3, \n",
    "        row_settings = (\"depth\",\"var_names\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcygWRph19kW"
   },
   "source": [
    "Then we define objects and functions we will use for training. <br>\n",
    "We created a callback `LossesTracker` to store on lists the losses (train and validation) during training. The class also has a function to plot them when training ended. <br>\n",
    "We also created the function `RunTraining` that takes care of initilizing everything needed for training: model, datamodule, trainer and callbacks (an early stopping callback and a checkpointing one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGjcE0v1bUQt"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class LossesTracker(Callback):\n",
    "\n",
    "    def __init__(self, ep_frac): \n",
    "        self.ep_frac = ep_frac\n",
    "        self.train = []\n",
    "        self.valid = []\n",
    "        self.tr_epochs = 0\n",
    "        self.val_epochs = 0\n",
    "    \n",
    "    def on_train_epoch_end(self, trainer, module):\n",
    "        self.train.append(trainer.logged_metrics[\"train_loss\"])\n",
    "        self.tr_epochs += 1\n",
    "        return\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, module):       \n",
    "        self.valid.append(trainer.logged_metrics[\"val_loss\"])\n",
    "        self.val_epochs += 1\n",
    "        return\n",
    "    \n",
    "    def plot_train_history(self, figsize=(8,5), ylog=True, save=None):\n",
    "        # plot of the losses\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "\n",
    "        plt.title(\"Losses history\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss value\")\n",
    "        if ylog:\n",
    "            plt.yscale(\"log\")\n",
    "\n",
    "        epochs = np.arange(self.tr_epochs)\n",
    "        val_epochs = np.arange(self.val_epochs)*self.ep_frac\n",
    "        plt.plot(epochs, self.train, \"-o\", label=\"train loss\")\n",
    "        plt.plot(val_epochs, self.valid, \"-o\", label=\"valid. loss\")\n",
    "\n",
    "        # plot horizontal line at minimum validation loss\n",
    "        min_valid = min(self.valid)\n",
    "        plt.plot([0, self.tr_epochs-1], [min_valid, min_valid], color=\"red\", \n",
    "                 ls=\"--\", label=\"Minimum valid. loss: %.3f \"%min_valid,\n",
    "                )\n",
    "\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        if save is not None:\n",
    "            fig.savefig(\"images/\"+save+\".pdf\", bbox_inches='tight')\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ge_met5lbUQt"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "def RunTraining(hypers, chkpt_prefix, transform=None, epochs=100, Nsamples=None, valid_frac=1./6.):\n",
    "    \n",
    "    # check validation every ep_frac (fraction of epoch)\n",
    "    ep_frac = 1.\n",
    "    \n",
    "    ### callbacks\n",
    "    losses_tracker = LossesTracker(ep_frac)\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", \n",
    "                                        min_delta=0.0001, \n",
    "                                        patience=6*(1./ep_frac), \n",
    "                                        verbose=False, \n",
    "                                        check_on_train_epoch_end=False, # check early_stop at end of validation\n",
    "                                       )\n",
    "    checkpoint = ModelCheckpoint(dirpath  = \"FashionMNIST_checkpoint\", \n",
    "                                 filename = chkpt_prefix+\"_{epoch}_{val_loss:.2f}\", \n",
    "                                 monitor  = \"val_loss\",\n",
    "                                )\n",
    "    \n",
    "    ### define model architecture and hyper-parameters\n",
    "    model = Lit_CNN(input_size    = 28,\n",
    "                    arch          = hypers[\"arch\"],\n",
    "                    params        = hypers[\"params\"],\n",
    "                    optimizer     = hypers[\"optimizer\"],\n",
    "                    learning_rate = hypers[\"learning_rate\"],\n",
    "                    L2_penalty    = hypers[\"L2_penalty\"],\n",
    "                   )\n",
    "    \n",
    "    ### define datamodule\n",
    "    datamodule = FashionMNISTDataModule(data_dir   = DATA_DIR_NAME, \n",
    "                                        batch_size = hypers[\"batch_size\"],\n",
    "                                        Nsamples   = Nsamples,\n",
    "                                        valid_frac = valid_frac,\n",
    "                                        random_state = magic_num,\n",
    "                                        transform  = transform,\n",
    "                                       )\n",
    "    ### define trainer\n",
    "    trainer = pl.Trainer(logger     = False,\n",
    "                         max_epochs = epochs,\n",
    "                         gpus       = 1 if USE_GPU else None,\n",
    "                         callbacks  = [early_stop_callback, losses_tracker, checkpoint],\n",
    "                         val_check_interval   = ep_frac,\n",
    "                         enable_model_summary = False,\n",
    "                         num_sanity_val_steps = 0,     # disable validation sanity check before training\n",
    "                        )\n",
    "    \n",
    "    # measure running time\n",
    "    fit_begin = time.time() \n",
    "    \n",
    "    trainer.fit(model, datamodule=datamodule) # run the training\n",
    "\n",
    "    fit_time = time.time() - fit_begin\n",
    "    print(f\"Fit time:\", str(datetime.timedelta(seconds=fit_time)) )\n",
    "    \n",
    "    return model, trainer, losses_tracker, checkpoint   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJalFVc119kZ"
   },
   "source": [
    "Now we are ready to run the training after resetting the random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAhfsJMQbUQv"
   },
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(seed=magic_num) \n",
    "\n",
    "model, trainer, losses, checkpoint = RunTraining(hyper_pars, \n",
    "                                                 chkpt_prefix=\"model\", \n",
    "                                                 Nsamples = None, # use full dataset\n",
    "                                                 epochs   = 100,\n",
    "                                                 valid_frac=1./6.,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeMs6tCrbUQx"
   },
   "outputs": [],
   "source": [
    "losses.plot_train_history(ylog=False, save=\"losses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PREfo4CnXdV"
   },
   "source": [
    "## Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFH4YNaQ19kd"
   },
   "source": [
    "To evaluate performances over the test set we built two functions: `TestPerformance`, that runs the model in evaluation mode over the test dataset and computes the total accuracy, and `PlotConfusionMatrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUrhaMkVbUQx"
   },
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import matplotlib\n",
    "\n",
    "def TestPerformance(model, dataloader, device=torch.device(\"cpu\")):\n",
    "    accuracy  = torchmetrics.Accuracy()\n",
    "    \n",
    "    # test\n",
    "    preds = []\n",
    "    labels = []\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, target in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            output = model(batch)\n",
    "\n",
    "            output = output.cpu()\n",
    "            # test loss\n",
    "            loss = nn.functional.cross_entropy(output, target)\n",
    "            losses.append(loss)\n",
    "\n",
    "            # accuracy        \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            test_acc  = accuracy(pred.view_as(target), target)\n",
    "            accuracies.append(test_acc)\n",
    "\n",
    "            # predictions and labels to return\n",
    "            preds.append(pred)\n",
    "            labels.append(target)\n",
    "            \n",
    "    final_test_loss = np.mean(losses)\n",
    "    final_test_acc  = np.mean(accuracies)\n",
    "    print(\"FINAL TEST LOSS VALUE: {}\".format(final_test_loss   ))\n",
    "    print(\"FINAL TEST ACCURACY  : {}\".format(final_test_acc    ))\n",
    "    \n",
    "    return torch.cat(preds), torch.cat(labels), final_test_acc\n",
    "    \n",
    "    \n",
    "def PlotConfusionMatrix(guesses, true_labels, num_classes=10, save=None):\n",
    "    # bulding confusion matrix\n",
    "    confusion_matrix = torchmetrics.ConfusionMatrix(num_classes)\n",
    "    mat = confusion_matrix(guesses, true_labels).numpy()\n",
    "\n",
    "    # plotting    \n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    im = ax.imshow(mat)\n",
    "\n",
    "    # Show all ticks and label them with the respective names\n",
    "    ax.set_xticks(np.arange(num_classes))\n",
    "    ax.set_xticklabels(label_names)\n",
    "    \n",
    "    ax.set_yticks(np.arange(num_classes))\n",
    "    ax.set_yticklabels(label_names)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    formatter = matplotlib.ticker.StrMethodFormatter(\"{x:d}\")\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            datum = mat[i, j].astype(int)\n",
    "            color = \"white\" if (datum < np.amax(mat)*0.5) else \"black\"\n",
    "            text = ax.text(j, i, formatter(datum), fontsize=12,\n",
    "                           ha=\"center\", va=\"center\", color=color)\n",
    "\n",
    "    ax.set_title(\"Confusion Matrix\", fontsize=14)\n",
    "    ax.set_xlabel(\"Predicted labels\")\n",
    "    ax.set_ylabel(\"True labels\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "            \n",
    "    if save is not None:\n",
    "        fig.savefig(\"images/\"+save+\".pdf\", bbox_inches='tight')\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKdgQX7W19kf"
   },
   "source": [
    "In the cells below we load the test dataset in memory, create the dataloader object, we restore the model to the checkpoint with best validation accuracy and then use the functions defined above to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tp1Pvr-cbUQy"
   },
   "outputs": [],
   "source": [
    "# loading test dataset and creating dataloader\n",
    "test_dataset = torchvision.datasets.FashionMNIST(DATA_DIR_NAME, \n",
    "                                                 train=False, \n",
    "                                                 download=to_download, \n",
    "                                                 transform = transforms.ToTensor(),\n",
    "                                                )\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJqPOM7zbUQz"
   },
   "outputs": [],
   "source": [
    "# load best model from checkpoint\n",
    "model = Lit_CNN.load_from_checkpoint(checkpoint.best_model_path, \n",
    "                                     input_size    = 28,\n",
    "                                     arch          = hyper_pars[\"arch\"],\n",
    "                                     params        = hyper_pars[\"params\"],\n",
    "                                     optimizer     = hyper_pars[\"optimizer\"],\n",
    "                                     learning_rate = hyper_pars[\"learning_rate\"],\n",
    "                                     L2_penalty    = hyper_pars[\"L2_penalty\"],\n",
    "                                    )\n",
    "model.to(device)\n",
    "# evaluate performance\n",
    "preds, labels, model_acc = TestPerformance(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HKrmIyCbUQz"
   },
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "PlotConfusionMatrix(preds, labels, save=\"confmatrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSXQnTHVbUQ0"
   },
   "source": [
    "### Small change: retraining with added random transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EdU3fRb19kj"
   },
   "source": [
    "Here we repeat the training above by adding some random transformations to images when training. This should act as a regularizer for the model and should improve its generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_Tj7lIVbUQ1"
   },
   "outputs": [],
   "source": [
    "# define transformations composition to serve as regularization\n",
    "## the probability to have at least one transform applied is about 0.185\n",
    "TrainTransform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(mean=0., std=0.1, prob=0.05),\n",
    "    AddOcclusion(max_area=0.4, prob=0.05),\n",
    "    transforms.RandomVerticalFlip(p=0.05),\n",
    "    transforms.RandomHorizontalFlip(p=0.05),\n",
    "])\n",
    "\n",
    "# set random state\n",
    "seed_everything(seed=magic_num) \n",
    "\n",
    "model_tr, trainer_tr, losses_tr, checkpoint_tr = RunTraining(hyper_pars, \n",
    "                                                             chkpt_prefix=\"model_tr\",\n",
    "                                                             transform=TrainTransform,\n",
    "                                                             Nsamples = None, # use full dataset\n",
    "                                                             epochs   = 100,\n",
    "                                                             valid_frac=1./6.,\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tebCNIq7bUQ2"
   },
   "outputs": [],
   "source": [
    "losses_tr.plot_train_history(save=\"losses-tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJEZYdRvbUQ2"
   },
   "outputs": [],
   "source": [
    "#load from checkpoint \n",
    "model_tr = Lit_CNN.load_from_checkpoint(checkpoint_tr.best_model_path,\n",
    "                                        input_size    = 28,\n",
    "                                        arch          = hyper_pars[\"arch\"],\n",
    "                                        params        = hyper_pars[\"params\"],\n",
    "                                        optimizer     = hyper_pars[\"optimizer\"],\n",
    "                                        learning_rate = hyper_pars[\"learning_rate\"],\n",
    "                                        L2_penalty    = hyper_pars[\"L2_penalty\"],\n",
    "                                       )                                       \n",
    "model_tr.to(device)\n",
    "# evaluate performance\n",
    "preds_tr, labels_tr, model_tr_acc = TestPerformance(model_tr, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dn3fEshTbUQ2"
   },
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "PlotConfusionMatrix(preds_tr, labels_tr, save=\"confmatrix-tr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3U-6C1tnXdW"
   },
   "source": [
    "## Analysis of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEY98HvWbUQ4"
   },
   "source": [
    "### Visualization of filter kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oH36C10i19kr"
   },
   "source": [
    "First we try to visualize the learned filters of the convolutional layers. We just need to retrieve the weights of convolutional layers and plot them. <br> \n",
    "For the first layer we have just one input channel, so we can plot them all; for subsequent layers we must select one input channel and plot only filters belonging to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FW--AfN4bUQ4"
   },
   "outputs": [],
   "source": [
    "model.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Tm5tXUWbUQ5"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "def ShowFiltersGrid(conv_layer, title=\"\", figsize=(12,6), channel_id=0, save=None):\n",
    "    \n",
    "    # retrieve kernels from layer\n",
    "    kernels = conv_layer.weight.detach().clone()\n",
    "\n",
    "    if kernels.size(1) != 1:  # if there is more than 1 input channel, select channel_id\n",
    "        kernels = kernels[:, channel_id].unsqueeze(dim=1)\n",
    "        title = title + f\" - channel #{channel_id}\"\n",
    "    else:\n",
    "        channel_id = 0 # ignore channel_id\n",
    "    \n",
    "    print(title)\n",
    "    print(f\"  kernels shape: ({kernels.size(2)},{kernels.size(3)})\" + \n",
    "          f\" ; # of filters: {kernels.size(0)}\" \n",
    "         )\n",
    "    \n",
    "    # normalize to range [0,1] for better visualization\n",
    "    kmin = torch.min(kernels).item()\n",
    "    kmax = torch.max(kernels).item()\n",
    "    kernels = (kernels - kmin)/(kmax - kmin)\n",
    "    \n",
    "    # create grid of filters images\n",
    "    cols = 16\n",
    "    rows = kernels.size(0) // cols    \n",
    "    \n",
    "    # modify figsize height\n",
    "    figsize = ( figsize[0], figsize[0]*rows/(cols-1) ) \n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    grid = ImageGrid(fig  = fig, \n",
    "                     rect = 111,  # as in subplot(111)\n",
    "                     nrows_ncols = (rows, cols), \n",
    "                     axes_pad    = 0.05,  # pad between axes in inch.\n",
    "                    )\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    \n",
    "    for ax, im in zip(grid, kernels):       \n",
    "        ax.imshow(im.squeeze().cpu(), cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if save is not None:\n",
    "        fig.savefig(\"images/\"+save+\".pdf\", bbox_inches='tight')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e_5Lj85bUQ6"
   },
   "outputs": [],
   "source": [
    "# retrieve all convolutional layers inside network\n",
    "ConvLayers = [module for module in model.network.modules() if isinstance(module, nn.Conv2d)]\n",
    "print(f\"Model has {len(ConvLayers)} convolutional layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1EqhMVEbUQ6"
   },
   "outputs": [],
   "source": [
    "# plot filters \n",
    "for it, layer in enumerate(ConvLayers):\n",
    "    ShowFiltersGrid(conv_layer = layer,\n",
    "                    title = f\"Filters of Convolutional layer #{it+1}\",\n",
    "                    channel_id = np.random.randint(layer.in_channels),  # random channel id\n",
    "                    save = f\"kernels-layer{it+1}\",\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asyXVgQXnXdZ"
   },
   "source": [
    "### Visualization of feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-8DedaQ19k-"
   },
   "source": [
    "Here instead we select a sample from the test dataset and pass it to the model. By mean of a hook for each convolutional layer we can collect the filtered images that are forwarded through the net and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RH19gCpYbUQ8"
   },
   "outputs": [],
   "source": [
    "def ShowFeatureMapsGrid(maps, title=\"\", figsize=(16,6), save=None):\n",
    "    \n",
    "    print(title) \n",
    "    maps = maps.squeeze(dim=0) # remove first dim: from [1,C,H,W] to [C,H,W]    \n",
    "    print(f\"  feature maps shape: ({maps.size(1)},{maps.size(2)})\" + \n",
    "          f\" ; # of feature maps: {maps.size(0)}\" \n",
    "         )\n",
    "    \n",
    "    # normalize to range [0,1] for better visualization\n",
    "    fmin = torch.min(maps).item()\n",
    "    fmax = torch.max(maps).item()\n",
    "    maps = (maps - fmin)/(fmax - fmin)\n",
    "    \n",
    "    # create grid of filters images\n",
    "    cols = 16\n",
    "    rows = maps.size(0) // cols    \n",
    "    \n",
    "    figsize = ( figsize[0], figsize[0]*rows/cols )  # modify figsize height  \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    grid = ImageGrid(fig  = fig, \n",
    "                     rect = 111,  # as in subplot(111)\n",
    "                     nrows_ncols = (rows, cols), \n",
    "                     axes_pad    = 0.05,  # pad between axes in inch.\n",
    "                    )    \n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    \n",
    "    for ax, im in zip(grid, maps):\n",
    "        ax.imshow(im.cpu(), cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if save is not None:\n",
    "        fig.savefig(\"images/\"+save+\".pdf\", bbox_inches='tight')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1QqaPC5bUQ8"
   },
   "outputs": [],
   "source": [
    "# select image from test dataset\n",
    "\n",
    "img_id = 9\n",
    "\n",
    "img, true_label = test_dataset[img_id][0], test_dataset[img_id][1]\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "plt.imshow(img.squeeze(), cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVJLOOnHbUQ9"
   },
   "outputs": [],
   "source": [
    "class OutputFeatures():\n",
    "    def __init__(self, network):\n",
    "        self.net = network\n",
    "        \n",
    "        self.outputs = []\n",
    "        self.hook_handles = []\n",
    "        \n",
    "        # Register hook on each conv. layer activations\n",
    "        layers = list(self.net.modules())\n",
    "        for it in range(len(layers)):\n",
    "            if isinstance(layers[it], nn.Conv2d):\n",
    "                self.hook_handles.append(layers[it+1].register_forward_hook(self._get_output_features))\n",
    "                \n",
    "        print(f\"Registered {len(self.hook_handles)} hooks on network.\")\n",
    "        \n",
    "    def _get_output_features(self, layer, input, output):\n",
    "        self.outputs.append(output)\n",
    "\n",
    "    def close(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMnYK7vSbUQ9"
   },
   "outputs": [],
   "source": [
    "# register hooks in convolutional layers\n",
    "hooks = OutputFeatures(model.network)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # pass an image to the network\n",
    "    pred_label = model(img.unsqueeze(dim=0).to(device)).argmax(dim=1, keepdim=True).item()\n",
    "    \n",
    "# remove hooks\n",
    "hooks.close()\n",
    "\n",
    "feature_maps = hooks.outputs.copy()\n",
    "    \n",
    "print(f\"True label     : {true_label} ({label_names[true_label]})\")\n",
    "print(f\"Predicted label: {pred_label} ({label_names[pred_label]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSLdkf5ubUQ9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot feature maps for each conv. layer\n",
    "for it, fmaps in enumerate(feature_maps):   \n",
    "    ShowFeatureMapsGrid(maps  = fmaps,\n",
    "                        title = f\"Feature maps of Convolutional layer #{it+1}\",\n",
    "                        save  = f\"featuremaps-layer{it+1}\",\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMKro9KWbUQ_"
   },
   "source": [
    "### Optimize an image by maximizing activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xq3chP3k19lN"
   },
   "source": [
    "To further investigate what the network has learned we create the class `MaximizeActivation`. It receives in input the model and, with its method `optimize_sample`, it generates a synthetic image optimized to maximally activate a particular convolutional filter (when `layer_id` is passed to the function) or a particular output neuron (when `class_id` is provided). <br>\n",
    "To control the quality of generated images some parameters should be adjusted, like learning rate, L2 penalty, .. or some technique can be applied during optimization, like gaussian blurring, clipping, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPQQHp_tbUQ_"
   },
   "outputs": [],
   "source": [
    "model.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8m0I1dzxbURA"
   },
   "outputs": [],
   "source": [
    "class MaximizeActivation():\n",
    "    def __init__(self, model, img_size=[28, 28, 1], device=None, mean=[0.], std=[1.], \n",
    "                 verbose=False,\n",
    "                ):\n",
    "        \n",
    "        self.model  = model\n",
    "        self.shape  = img_size\n",
    "        self.verbose= verbose\n",
    "\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        # store mean and std for normalization\n",
    "        self.mean = mean\n",
    "        self.std  = std\n",
    "        \n",
    "        # compute reverse mean and std for inverting normalization\n",
    "        self.reverse_mean = list( -np.asarray(mean)/np.asarray(std) )\n",
    "        self.reverse_std  = list( 1./np.asarray(std) )\n",
    "        \n",
    "        # hook handle\n",
    "        self.hook_handle = []\n",
    "        \n",
    "### hooks management\n",
    "    def register_conv_hook(self, layer_id):\n",
    "        # register hook on conv layer\n",
    "        if isinstance(self.model.conv[layer_id], nn.Conv2d):\n",
    "            self.hook_handle.append( self.model.conv[layer_id].register_forward_hook(self._hook_f) )\n",
    "        else:\n",
    "            raise ValueError(f\"Layer {layer_id} is NOT a 'Conv2d' instance.\")\n",
    "        if self.verbose:\n",
    "            print(f\"Registered hook on conv. layer [{layer_id}].\")\n",
    "        \n",
    "    def register_fc_hook(self):\n",
    "        # register hook on last linear layer\n",
    "        self.hook_handle.append( self.model.fc[-1].register_forward_hook(self._hook_f) )\n",
    "        if self.verbose:\n",
    "            print(f\"Registered hook on last linear layer output.\")\n",
    "        \n",
    "    def _hook_f(self, layer, input, output):\n",
    "        self.activ = output.clone()\n",
    "        self.activ.requires_grad_(True)\n",
    "        \n",
    "    def remove_hooks(self):\n",
    "        if len(self.hook_handle) > 0:\n",
    "            [hh.remove() for hh in self.hook_handle]\n",
    "            \n",
    "        self.hook_handle = []\n",
    "        \n",
    "### loss function       \n",
    "    def loss_f(self, class_id, layer_id, channel_id):\n",
    "              \n",
    "        if class_id is not None:\n",
    "            loss = -1.* self.activ.squeeze()[class_id]\n",
    "        else:\n",
    "            loss = -1.* (self.activ[0, channel_id]).mean()\n",
    "            \n",
    "        return loss\n",
    "        \n",
    "### optimization \n",
    "    def optimize_sample(self, niter=100, input_img=None, lr=0.001, L2_c=1e-6,\n",
    "                        class_id=None, layer_id=None, channel_id=None, \n",
    "                        blurrer=None, clipping=False, optimizer=torch.optim.Adam,\n",
    "                       ): \n",
    "        if input_img is None:\n",
    "            # generate noise image\n",
    "            noise     = np.uint8( np.random.uniform(50, 80, (self.shape)) )\n",
    "            # transform into torch float tensor (dividing by 255)\n",
    "            input_img = transforms.functional.to_tensor(noise)\n",
    "            \n",
    "        if len(input_img.size()) < 4:\n",
    "            input_img = input_img.unsqueeze(dim=0)\n",
    "        \n",
    "        # store initial image\n",
    "        init_img = input_img.detach().clone()\n",
    "\n",
    "        # normalize with mean and std provided to __init__\n",
    "        input_img = transforms.functional.normalize(input_img, self.mean, self.std)\n",
    "        \n",
    "        # activate gradient tracking for image\n",
    "        input_img = input_img.detach().clone().to(self.device).requires_grad_(True)\n",
    "        \n",
    "        # setup optimizer\n",
    "        optim = optimizer([input_img], lr=lr, weight_decay=L2_c)\n",
    "        \n",
    "        # setup hooks\n",
    "        if class_id is not None:\n",
    "            self.register_fc_hook()\n",
    "            print(f\"Maximizing class {class_id} activation ...\")\n",
    "        elif layer_id is not None:\n",
    "            self.register_conv_hook(layer_id)\n",
    "            \n",
    "            if channel_id is None:\n",
    "                # sample a random channel index\n",
    "                channel_id = np.random.randint(self.model.conv[layer_id].out_channels)\n",
    "            print(f\"Maximizing mean activation of conv. layer: {layer_id}, channel: {channel_id} ...\")\n",
    "        else:\n",
    "            raise RuntimeError(\"Both 'class_id' and 'layer_id' are None. Provide one of them.\")\n",
    "        \n",
    "        # run optimization\n",
    "        self.model.eval()\n",
    "        self.loss_history = []\n",
    "        for it in range(niter):\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            if blurrer is not None:\n",
    "                # blur (and clip) before passing img to model (should help to get a more regular sample)\n",
    "                if clipping:\n",
    "                    output = self.model(blurrer(input_img).clamp(0., 1.))\n",
    "                else:\n",
    "                    output = self.model(blurrer(input_img))\n",
    "            else:\n",
    "                output = self.model(input_img)\n",
    "            \n",
    "            # compute loss\n",
    "            loss = self.loss_f(class_id, layer_id, channel_id)\n",
    "            self.loss_history.append(loss.item())\n",
    "            \n",
    "            # optimize image\n",
    "            loss.backward()\n",
    "            optim.step()         \n",
    "            \n",
    "        # delete hooks\n",
    "        self.remove_hooks()\n",
    "        \n",
    "        # invert normalization\n",
    "        input_img = transforms.functional.normalize(input_img, self.reverse_mean, self.reverse_std)\n",
    "        \n",
    "        out_img = input_img.detach().clone()\n",
    "        \n",
    "        return out_img, init_img, [class_id, layer_id, channel_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdlTnaka3sHV"
   },
   "outputs": [],
   "source": [
    "# some utils functions\n",
    "from PIL import Image\n",
    "\n",
    "    \n",
    "def plot_loss_history(losses):\n",
    "\n",
    "    niter = len(losses)\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    plt.title(\"Loss history\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss value\")\n",
    "    plt.plot(range(niter), losses)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def tensor_to_image(tensor, clip=False, brightness=0.):\n",
    "    \n",
    "    # brightness adjust\n",
    "    out = tensor.add(brightness)        \n",
    "\n",
    "    if clip: #clipping to [0,1] range\n",
    "        out = out.clamp(min=0., max=1.)   \n",
    "        out = out / torch.max(out)\n",
    "\n",
    "    if len(out.size()) == 4:\n",
    "        out = out.squeeze(dim=0)\n",
    "\n",
    "    out = out.detach().numpy()        \n",
    "    out = np.uint8( np.round(out * 255) )\n",
    "    return out.transpose(1,2,0).squeeze()\n",
    "\n",
    "def save_image_png(image, title=\"image.png\"):\n",
    "    \n",
    "    pil_img = Image.fromarray(image)\n",
    "    pil_img.save(title)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CvXq865bURB"
   },
   "outputs": [],
   "source": [
    "inspector = MaximizeActivation(model.network, \n",
    "                               device = model.device,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhjewC9e3sHX"
   },
   "source": [
    "### 1) maximize activation of some filters on convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnaCYkEq3sHY"
   },
   "outputs": [],
   "source": [
    "# get ids of conv layers\n",
    "conv_layers_id = [it-1 for it, module in enumerate(model.network.conv.modules()) if isinstance(module, nn.Conv2d)]\n",
    "print(\"Indices of conv. layers: \", conv_layers_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sht6EJ7U3sHZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layers_dict = {}\n",
    "Niters = [20,20,20,20]  #iterations for each layer (deeper layers require more iterations)\n",
    "\n",
    "Nfilters = 8  # number of channel_id for each layer\n",
    "for k, idx in enumerate(conv_layers_id):\n",
    "    output = []\n",
    "    settings = []\n",
    "    for ff in range(Nfilters):\n",
    "        generated, initial, info = inspector.optimize_sample(niter    = Niters[k],\n",
    "                                                             layer_id = idx,\n",
    "                                                             channel_id = None, # channel_id is randomly chosen\n",
    "                                                             lr   = 0.2,\n",
    "                                                             L2_c = 1e-6,\n",
    "                                                             blurrer = None,\n",
    "                                                             optimizer = torch.optim.Adam,\n",
    "                                                            )\n",
    "        output.append(generated)\n",
    "        settings.append(info) \n",
    "    # store images and info in dict\n",
    "    layers_dict.update({ f\"{idx}\" : {\"out\": output, \"info\": settings} })       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rrist133sHa"
   },
   "outputs": [],
   "source": [
    "# plot generated images\n",
    "tmp = 1\n",
    "for key, layer in layers_dict.items():\n",
    "    \n",
    "    h_size = 16\n",
    "    v_size = h_size/Nfilters +0.5\n",
    "    fig, axs = plt.subplots(1, Nfilters, figsize=(h_size-2, v_size))\n",
    "    fig.suptitle(\"Convolutional Layer id: \"+str(tmp), fontsize=14)\n",
    "    \n",
    "    for jdx, out in enumerate(layer[\"out\"]):\n",
    "        img_to_plot = tensor_to_image(out.detach().clone().cpu(), clip=True, brightness=0.1)\n",
    "        axs[jdx].imshow(img_to_plot, cmap=\"Greys\")\n",
    "        axs[jdx].set_title(f\"channel: {layer['info'][jdx][2]}\")\n",
    "        axs[jdx].axis('off')\n",
    " \n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    fig.savefig(\"images/patterns\"+str(tmp)+\".pdf\", bbox_inches='tight')\n",
    "    \n",
    "    tmp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, maximizing activation of convolutional filters generated some patterns that are very simple for the first layer and gradually increasing in complexity for deeper layers. This effect is more visible in the images reported in appendix, obtained from a pre-trained AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G17zFep93sHb"
   },
   "source": [
    "### 2) maximize activation of a neuron in the last linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlaMAnA919lU"
   },
   "outputs": [],
   "source": [
    "blurrer = transforms.GaussianBlur(kernel_size=(5,5), sigma=2.)\n",
    "\n",
    "classes = [0,1,2,3,4,5,6,7,8,9]  # target class_id list\n",
    "\n",
    "output = []\n",
    "for k, idx in enumerate(classes):\n",
    "    generated, initial, info = inspector.optimize_sample(niter    = 50,\n",
    "                                                         class_id = idx,\n",
    "                                                         input_img = None, # generate random noise image\n",
    "                                                         lr   = 0.05,\n",
    "                                                         L2_c = 0.99,\n",
    "                                                         blurrer = blurrer,\n",
    "                                                         clipping = True,\n",
    "                                                         optimizer = torch.optim.Adam,\n",
    "                                                        )\n",
    "    output.append(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6R5bXq8y19lV"
   },
   "outputs": [],
   "source": [
    "# plot generated images\n",
    "h_size = 16\n",
    "v_size = 4*h_size/len(classes)\n",
    "fig, axs = plt.subplots(2, len(classes)//2, figsize=(h_size, v_size))\n",
    "fig.suptitle(\"Optimized images\", fontsize=14)\n",
    "\n",
    "# applying some transform to generated images for better visualization\n",
    "images_to_plot = [tensor_to_image(oo.detach().clone().cpu(), clip=True) for oo in output]\n",
    "    \n",
    "for k, idx in enumerate(classes):\n",
    "    axs[k//5][k%5].imshow(images_to_plot[k], cmap=\"Greys\")\n",
    "    axs[k//5][k%5].set_title(f\"class: {idx} ({label_names[idx]})\")\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"images/generated.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-FpN4vl19lW"
   },
   "outputs": [],
   "source": [
    "# retrieving predictions of network over generated images\n",
    "for img in images_to_plot:\n",
    "    \n",
    "    to_tensor = transforms.ToTensor()\n",
    "    img = to_tensor(img).unsqueeze(dim=0)\n",
    "    \n",
    "    optim_out = model.network( img.to(device) )\n",
    "    optim_pred_id = torch.argmax(optim_out, dim=1).item()\n",
    "\n",
    "    print(f\"Optimized sample inferred class: {optim_pred_id} ({label_names[optim_pred_id]})\")\n",
    "    print(\"    with softmax activation:\", torch.max(optim_out.softmax(dim=1)).item() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images generated and plotted above present some of the features of the class they belong, however they are far from being a good quality sample, even if the network is highly confident on which label to assign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2TkeC-X3sHp"
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHkAsjG419lZ"
   },
   "source": [
    "In this work a Convolutional Neural Network (CNN) was implemented to classify images in the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. The hyper-parameters have been tuned using `optuna` package. The search has not been exhaustive, as a lot of other convolutional configurations could have been defined and also a lot of hyper-parameters sets could have been explored. However, the obtained model can solve the task, as the reached final accuracy is `0.906` for the model without random transformations applied to inputs. The addition of random transformations as augmentation has increased the accuracy to `0.911`. It would have been interesting to see if other transformations, like random rotation/cropping could have further increased the accuracy. <br>\n",
    "\n",
    "Regarding the learned convolutional filters, it is hard to assign to them a clear meaning, and also the plotted feature maps for an input sample seems to be noisy, suggesting that learned filters are not well trained. The reason for this could be that the fully-connected layers have too much complexity and overfit too quickly the output feature maps. Maybe reducing the number of neurons or removing batch normalization from the fully-connected classifier could improve the learned filters. <br>\n",
    "\n",
    "Finally, the generated images by maximizing a certain filter activation seems to reproduce some basic patterns that become more complex for inner layers. However, to obtain the visualization plotted some clipping and rescaling have been applied. <br>\n",
    "The images generated by maximizing the activation of an output neuron are not very similar to dataset images of the same class. Also, to obtain these images strong constraints have been applied to the optimization (very high L2 penalty, gaussian blurring, clipping). <br> \n",
    "Improving the quality of learned filters could improve the quality of generated images, although this is not a generative model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XP31YVbvbURS"
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the maximization of filter activations on a pretrained AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJ-VP-vCfTJ7"
   },
   "source": [
    "In order to ensure that the class `MaximizeActivation` created above is working properly we test it on a pretrained network from the `torchvision` package. We select the simplest model (`AlexNet`) to minimize download and running times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGJCtwBqbURT"
   },
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(seed=magic_num) \n",
    "\n",
    "# load alexnet (only convolutional part)\n",
    "alexnet = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "# since class 'MaximizeActivation' expects a network with a convolutional submodule called 'conv',\n",
    "## we adopted this workaround\n",
    "alexnet.conv = alexnet.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfyqwVeL19lj"
   },
   "outputs": [],
   "source": [
    "alexnet.conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cJcZxHq3sHw"
   },
   "outputs": [],
   "source": [
    "alexnet.to(device)\n",
    "alexnet.eval()\n",
    "inspector = MaximizeActivation(alexnet,\n",
    "                               device = device, \n",
    "                               img_size = [224,224,3], # ImageNet size, mean and std\n",
    "                               mean     = [0.485, 0.456, 0.406], \n",
    "                               std      = [0.229, 0.224, 0.225],\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDj-ebz23sHw"
   },
   "outputs": [],
   "source": [
    "# get ids of conv layers\n",
    "conv_layers_id = [it-1 for it, module in enumerate(alexnet.conv.modules()) if isinstance(module, nn.Conv2d)]\n",
    "print(\"Indices of conv. layers: \", conv_layers_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC1VfZyK3sHx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layers_dict = {}\n",
    "Niters = [5,8,16,20,30]  #iterations for each layer (deeper layers require more iterations)\n",
    "\n",
    "blurrer = transforms.GaussianBlur(kernel_size=(3,3), sigma=0.8)\n",
    "\n",
    "Nfilters = 5  # number of different channel_ids for each layer\n",
    "for k, idx in enumerate(conv_layers_id):\n",
    "    output = []\n",
    "    settings = []\n",
    "    for ff in range(Nfilters):\n",
    "        generated, initial, info = inspector.optimize_sample(niter    = Niters[k],\n",
    "                                                             layer_id = idx,\n",
    "                                                             channel_id = None, # channel_id is randomly chosen\n",
    "                                                             lr   = 0.1,\n",
    "                                                             L2_c = 1e-6,\n",
    "                                                             blurrer = blurrer,\n",
    "                                                             optimizer = torch.optim.Adam,\n",
    "                                                            )\n",
    "        output.append(generated)\n",
    "        settings.append(info) \n",
    "    # store images and info in dict\n",
    "    layers_dict.update({ f\"{idx}\" : {\"out\": output, \"info\": settings} })        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dqa9Ru43sHy"
   },
   "outputs": [],
   "source": [
    "# plot generated images\n",
    "for key, layer in layers_dict.items():\n",
    "    \n",
    "    h_size = 16\n",
    "    v_size = h_size/Nfilters\n",
    "    fig, axs = plt.subplots(1, Nfilters, figsize=(h_size-2, v_size))\n",
    "    fig.suptitle(\"Convolutional Layer id: \"+key, fontsize=14)\n",
    "    \n",
    "    for jdx, out in enumerate(layer[\"out\"]):\n",
    "        img_to_plot = tensor_to_image(out.detach().clone().cpu(), clip=True, brightness=0.3)\n",
    "        axs[jdx].imshow(img_to_plot)\n",
    "        axs[jdx].set_title(f\"channel: {layer['info'][jdx][2]}\")\n",
    "        axs[jdx].axis('off')\n",
    " \n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    fig.savefig(\"images/alexnet\"+str(key)+\".pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWe9cLzP19ln"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ClassificationTask_MicheleGuadagnini_Mt_1230663_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

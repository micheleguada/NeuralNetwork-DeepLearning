{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9nMupgOkVa8"
   },
   "source": [
    "# NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "---\n",
    "A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "---\n",
    "\n",
    "\n",
    "# Homework 1 - Supervised Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx94AFKGkVbQ"
   },
   "source": [
    "### Author: Michele Guadagnini - Mt.1230663"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total running time on low-end dual-core CPU: < 50 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYQGjzJlkVbT"
   },
   "source": [
    "# Regression Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TC5QuD24kVbW",
    "outputId": "fbc4c8a4-3409-4e63-8b57-0b8d15c7049f"
   },
   "outputs": [],
   "source": [
    "### ADDITIONAL LIBRARIES THAT NEED INSTALLATION (uncomment if needed)\n",
    "\n",
    "#!pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-xNC0ynkVba"
   },
   "outputs": [],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# python imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import datetime\n",
    "from scipy import stats\n",
    "\n",
    "# additional libraries\n",
    "import skorch\n",
    "\n",
    "# seed to set random states\n",
    "magic_num = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ph7X3N9WkVb0",
    "outputId": "cb243f59-5472-46e4-c382-669bc2d38035"
   },
   "outputs": [],
   "source": [
    "# setting the device\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU available')\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print('GPU not available')\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Available CPU cores:\", os.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_Rqh7BVd6iL",
    "outputId": "b291cbf4-406e-4101-9962-f77bcf8d2212"
   },
   "outputs": [],
   "source": [
    "print(\"device set to: \", device.type)\n",
    "\n",
    "# setting n_jobs to number of cores if running on CPU, None if on GPU\n",
    "Njobs = os.cpu_count() if (device.type==\"cpu\") else None\n",
    "### Njobs will be used as argument of the random search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting random state\n",
    "np.random.seed(magic_num)\n",
    "torch.manual_seed(magic_num)\n",
    "random.seed(magic_num)\n",
    "if (device.type == \"cuda\"): \n",
    "    torch.cuda.manual_seed(magic_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHeLKNYayXKO"
   },
   "source": [
    "## Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zs0AM6W_yoMs"
   },
   "source": [
    "* The goal is to train a neural network to approximate an unknown function:\n",
    "$$ \n",
    "f:\\mathbb{R}→\\mathbb{R} \\\\\n",
    "x↦y=f(x) \\\\\n",
    "\\text{network}(x) \\approx f(x)\n",
    "$$\n",
    "* As training point, you only have noisy measures from the target function.\n",
    "$$\n",
    "\\hat{y} = f(x) + noise\n",
    "$$\n",
    "* Consider to create a validation set from you training data, or use a k-fold cross-validation strategy. You may find useful these functions from the `scikit-learn` library:\n",
    "    - [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "    - [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHf-4soYV0z7"
   },
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvuju0INkVbm"
   },
   "source": [
    "## Outline of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLBxxBHfV0z_"
   },
   "source": [
    "The solution of this homework can be divided as follows:\n",
    "\n",
    "1. **Data download and organization**: data are downloaded in a local folder, then loaded in memory and shuffled. Also, the dataset object that will be used to train the network is created.\n",
    "\n",
    "1. **Definition of the network model** that will be fitted to the data, named `FC2_Net`. The implementation will provide the possibility to set some hyper-parameters (like number of units, which activation to use, dropout, ...).\n",
    "\n",
    "1. **Random search over the set of hyper-parameters** to identify a proper configuration for this task and dataset. The search will be done with scikit-learn function `RandomizedSearchCV` by wrapping up the PyTorch network model within the package `skorch`. Also, by setting `refit = True` inside the random search object, the found best-performing model is trained on the whole training set, providing a predictor.\n",
    "\n",
    "1. **Cross Validation mean prediction**: once done the random search, we test also a different approach: we take the resulting best set of hyper-parameters and we train 5 of copies of this model in a 5-fold cross validation setup. Then, at the end of the training we can use the mean of their predictions as actual prediction. \n",
    "\n",
    "1. **Performance over test dataset**: Both the obtained predictors are then applied to the test set and the loss function (MSE) is computed to evaluate their performance.\n",
    "\n",
    "1. **Network Analysis**: we select the best model returned from the random search and we visualize its weights distributions of each layer and also its activations profiles. Finally, we also try to change some hyper-parameters (like activation function, regularization intensity, ...) and train again the network in order to see the effects on the weights histograms and activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqZVgofTkVbd"
   },
   "source": [
    "## Data download and organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnLVXTW1kVbf",
    "outputId": "31d63fa6-ce7a-4e8a-ac7b-e95419b4689f"
   },
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "if not os.path.isdir(\"regression_dataset\"):\n",
    "    !wget -P regression_dataset https://gitlab.dei.unipd.it/michieli/nnld-2021-22-lab-resources/-/raw/main/homework1/train_data.csv\n",
    "    !wget -P regression_dataset https://gitlab.dei.unipd.it/michieli/nnld-2021-22-lab-resources/-/raw/main/homework1/test_data.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5LqsjIfkVbg"
   },
   "source": [
    "Now we load the dataset into a pandas dataframe and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "nrdodXg4kVbi",
    "outputId": "4331007e-a953-48f1-bc8f-fcfe8edc82a3"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('regression_dataset/train_data.csv')\n",
    "print(f\"train dataset shape: {train_df.shape}\")\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.scatter(train_df[\"input\"], train_df[\"label\"])\n",
    "plt.title(\"Training Points\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"trainpts.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oe5MnwoMV0zy"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# shuffling the dataset \n",
    "train_df = shuffle(train_df, random_state=magic_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "so8AwCCVkVb-"
   },
   "source": [
    "Here we create all the tools we need to deal with the data. First of all we define the class `NpDataset` inheriting from the `torch.utils.data.Dataset` creating a *map-style* dataset object by redefining the functions `__getitem__` and `__len__`. <br>\n",
    "Then we also define the transformation of data from normal array to `Tensor` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mk5O6i8fnPaS"
   },
   "outputs": [],
   "source": [
    "class NpDataset(Dataset):\n",
    "\n",
    "    def __init__(self, Arr2D, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Arr2D(numpy array): 2-columns array in the format [input, label].\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.data = Arr2D\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIA9oetfnPaW"
   },
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        return (torch.tensor([x]).float(),\n",
    "                torch.tensor([y]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfqTCqeNV0z5",
    "outputId": "6517d9af-4fcb-40fd-a7ea-5c65e73992d6"
   },
   "outputs": [],
   "source": [
    "# bulding train datasets \n",
    "composed_transform = transforms.Compose([ToTensor()])\n",
    "train_dataset = NpDataset(train_df.values, transform=composed_transform)\n",
    "\n",
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKDdyEEC8H8e"
   },
   "source": [
    "## Network model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines the model that will be used to fit the data, `FC2_Net`. It allows to define a fully-connected network with two hidden layers with customizable number of neurons in each layer. Also dropout probabilities after first hidden layer (`Pdrop1`) and second hidden layer (`Pdrop2`) are passed as arguments to the `__init__` function, allowing to activate the respective dropout layers. Other things customizable are: the activation function, with `ReLU` as default; the initialization scheme; also batch normalization layers after each linear layer can be added by means of a boolean flag, `batch_norm`.  <br>\n",
    "\n",
    "For simple regression tasks, also a network with only one hidden layer could be sufficient to fit the data, but our dataset is noisy and also presents some holes in its range, hence a 2 hidden layers model has been adopted. Eventual overfitting will be mitigated with the use of regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBzoQW2AtZ8H"
   },
   "outputs": [],
   "source": [
    "# network with 2 hidden layers fully connected\n",
    "class FC2_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, Nh1, Nh2, Pdrop1=0, Pdrop2=0, Ni=1, No=1, \n",
    "                 activ=nn.ReLU(), init_scheme=\"default\", batch_norm=False):\n",
    "        \"\"\"\n",
    "        Nh1          - number of Neurons in first hidden layer\n",
    "        Nh2          - number of Neurons in second hidden layer\n",
    "        Pdrop1       - dropout probability on first layer (optional)\n",
    "        Pdrop2       - dropout probability on second layer (optional)\n",
    "        Ni           - Input size\n",
    "        No           - Output size\n",
    "        activ        - activation function to use (default is torch.nn.ReLU())\n",
    "        init_scheme  - initialization scheme to use (torch.nn.init object).\n",
    "                       default is Uniform(-k, k), with k = sqrt[ 1/(Ni) ]\n",
    "        batch_norm   - boolean to activate batch normalization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # setting random state (needed to ensure reproducibility of randomized search)\n",
    "        torch.manual_seed(magic_num)\n",
    "        \n",
    "        self.Nlyrs = 2  # number of hidden layers\n",
    "        \n",
    "        ### linear layers\n",
    "        self.fc_h1  = nn.Linear(in_features=Ni , out_features=Nh1)   # hidden layer 1\n",
    "        self.fc_h2  = nn.Linear(in_features=Nh1, out_features=Nh2)   # hidden layer 2\n",
    "        self.fc_out = nn.Linear(in_features=Nh2, out_features=No )   # output layer\n",
    "        \n",
    "        ### dropout option\n",
    "        self.drp1 = nn.Dropout(p=Pdrop1)\n",
    "        self.drp2 = nn.Dropout(p=Pdrop2)\n",
    "        \n",
    "        ### activation function\n",
    "        self.act = activ\n",
    "        \n",
    "        ### initialization scheme selection (optional)\n",
    "        if init_scheme != \"default\":\n",
    "            # use init_scheme to initialize weights\n",
    "            init_scheme(self.fc_h1.weight)\n",
    "            init_scheme(self.fc_h2.weight)\n",
    "            init_scheme(self.fc_out.weight)\n",
    "            \n",
    "        ### batch normalization (optional)\n",
    "        self.batch_norm = batch_norm\n",
    "        if self.batch_norm:            \n",
    "            # define batch normalization layers\n",
    "            self.bn_h1 = nn.BatchNorm1d(Nh1)\n",
    "            self.bn_h2 = nn.BatchNorm1d(Nh2)\n",
    "            \n",
    "    def forward(self, x, additional_out=False):\n",
    "        \n",
    "        # first layer\n",
    "        x = self.fc_h1(x)\n",
    "        x = self.drp1(x)     # applied only if Pdrop1 != 0\n",
    "        x = self.act(x)      # activation\n",
    "        if self.batch_norm:\n",
    "            x = self.bn_h1(x)\n",
    "        \n",
    "        # second layer\n",
    "        x = self.fc_h2(x)\n",
    "        x = self.drp2(x)     # applied only if Pdrop2 != 0\n",
    "        x = self.act(x)      # activation \n",
    "        if self.batch_norm:\n",
    "            x = self.bn_h2(x)\n",
    "        \n",
    "        # output layer without activation\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ggz0lGekVb1"
   },
   "source": [
    "## Randomized Search for hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate which is the best hyper-parameters configuration to adopt a Randomized Search procedure has been implemented. A Random Search strategy should perform better than a Grid Search one in case of a large number of hyper-parameters to optimize, especially if some hyper-parameters are more important than others. <br>\n",
    "Since our dataset is very small, a good approach to evaluate the hyper-parameters is through **cross validation**. For this purpose we used the `skorch` package which allows us to wrap the *PyTorch* model inside another object (`skorch.NeuralNetRegressor`) that provides an interface compatible with `sklearn` framework. This way we can use the function `RandomizedSearchCV` from *sklearn*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKsDqIkh7SKO"
   },
   "source": [
    "### Hyper-parameters space definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3D8MEBXV00I"
   },
   "source": [
    "In the cells below the hyper-parameters space is defined. For some parameters a list of possible values has been defined, while for continuous parameters it is also possible to provide a continuous distribution from the `scipy.stats` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHhNkDNT7SKP"
   },
   "outputs": [],
   "source": [
    "### number of units in each hidden layer\n",
    "N_units = [160,192,224,256,288]\n",
    "\n",
    "### list of optimizers to test\n",
    "# adding momentum = 0.9 to the base class SGD \n",
    "class SGDmomentum09(torch.optim.SGD):\n",
    "    def __init__(self, params, lr=0.001, momentum=0.9, dampening=0,\n",
    "                 weight_decay=0, nesterov=False):\n",
    "        super().__init__(params=params, lr=lr, momentum=momentum, dampening=dampening,\n",
    "                 weight_decay=weight_decay, nesterov=nesterov)\n",
    "\n",
    "optim_list = [SGDmomentum09,\n",
    "              optim.Adam, \n",
    "             ]\n",
    "\n",
    "### list of activations to test\n",
    "activ_list = [nn.ReLU(),\n",
    "              nn.Tanh(),\n",
    "             ]\n",
    "\n",
    "### hyper-parameters dict\n",
    "# batch size is fixed to be equal to the dataset size because it is small and noisy, so subsampling \n",
    "#   would probably produce gradient instability during training\n",
    "\n",
    "hp_list = []\n",
    "for it in range(len(N_units)):\n",
    "    hp = {\n",
    "        \"module__Nh1\"            : [N_units[it]],                          # number of neurons in 1st layer\n",
    "        \"module__Nh2\"            : [N_units[it]],                          # number of neurons in 2nd layer\n",
    "        \"module__activ\"          : activ_list,                             # list of activations\n",
    "        \"module__Pdrop1\"         : [0.]*2+list(np.linspace(0.,0.25,num=11)), # dropout probability of 1st layer\n",
    "        \"module__Pdrop2\"         : [0.]*2+list(np.linspace(0.,0.25,num=11)), # dropout probability of 2nd layer\n",
    "        \"module__batch_norm\"     : [False, True],                          # batch normalization flag   \n",
    "        \"max_epochs\"             : [100,150,200],                          # number of epochs\n",
    "        \"lr\"                     : stats.loguniform(1e-5, 1e-2),           # learning rates space\n",
    "        \"optimizer\"              : optim_list,                             # set of optimizers\n",
    "        \"optimizer__weight_decay\": [0.]+list(np.logspace(-3,-0.3,num=10)), # L2 regularization\n",
    "    }\n",
    "    hp_list.append(hp)\n",
    "\n",
    "hyperparams_space = hp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSi41gg-V00M"
   },
   "source": [
    "### Random search procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below the actual random search is run. After resetting the random state, we define the skorch model `rand_search_net`, the score function `score_MSE` and the random search object `rand_search`. We also added a gradient clipping callback in order to solve some numerical problems with SGD and some particular configurations of hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "As0RgGz8kVb3",
    "outputId": "c4d38709-2599-4698-8e74-1a7054fa71fa"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import GradientNormClipping  # needed because, with SGD and some hyper-parameters\n",
    "                                                   ### configurations, loss explodes to Inf\n",
    "# setting random state\n",
    "np.random.seed(magic_num)\n",
    "torch.manual_seed(magic_num)\n",
    "random.seed(magic_num)\n",
    "if (device.type == \"cuda\"): \n",
    "    torch.cuda.manual_seed(magic_num)\n",
    "\n",
    "# skorch object to be used in randomized search with cross validation\n",
    "rand_search_net = NeuralNetRegressor(module      = FC2_Net,\n",
    "                                     train_split = False,   # no validation (CV splits will be done by rand_search)\n",
    "                                     device      = device,  # use selected device\n",
    "                                     verbose     = 0,\n",
    "                                     batch_size  = -1,      # single batch with all training points\n",
    "                                     callbacks   = [GradientNormClipping(gradient_clip_value=1e6)],\n",
    "                                    )\n",
    "# scoring function\n",
    "score_MSE = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# number of hyper-parameters configurations to test\n",
    "Niterations = 600\n",
    "\n",
    "# random search with cross validation  \n",
    "Ncv_splits  = 4\n",
    "rand_search = RandomizedSearchCV(rand_search_net, \n",
    "                                 hyperparams_space, \n",
    "                                 refit   = True, \n",
    "                                 cv      = Ncv_splits,  # number of CV splits\n",
    "                                 n_iter  = Niterations,\n",
    "                                 scoring = score_MSE,\n",
    "                                 verbose = 0, \n",
    "                                 n_jobs  = Njobs, \n",
    "                                 error_score = \"raise\", # raise error if an error occurs while computing score\n",
    "                                 random_state = magic_num,\n",
    "                                )\n",
    "\n",
    "print(\"Random Search settings: \")\n",
    "print(\"   # of hyper-parameters configurations to sample and test: {}\".format(Niterations))\n",
    "print(\"   # of cross-validation splits: {}\".format(Ncv_splits))\n",
    "print(\"Total number of fits to do: {}\".format(Ncv_splits*Niterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hx5T-cD_kVb5",
    "outputId": "99c45477-313f-4a55-c336-86117d5e284f"
   },
   "outputs": [],
   "source": [
    "# transform training data into numpy arrays (skorch will take care of creating a Dataset and Dataloader objects)\n",
    "XX = train_df[\"input\"].values.reshape(-1,1).astype(np.float32)\n",
    "YY = train_df[\"label\"].values.reshape(-1,1).astype(np.float32)\n",
    "\n",
    "# measure running time\n",
    "RS_begin = time.time()\n",
    "\n",
    "rand_search.fit(XX, YY) # run the randomized search \n",
    "\n",
    "RS_time = time.time() - RS_begin\n",
    "print(f\"Randomized Search time:\", str(datetime.timedelta(seconds=RS_time)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we visualize in a pandas dataframe the results of the random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "\n",
    "rand_search_df = pd.DataFrame.from_dict(rand_search.cv_results_)\n",
    "\n",
    "# sorting the dataframe by rank\n",
    "rand_search_df = rand_search_df.sort_values(by=\"rank_test_score\")\n",
    "\n",
    "rand_search_df.drop(columns=\"params\", inplace=True)\n",
    "rand_search_df.columns = [\"mean_fit_time\",\"std_fit_time\",\"mean_score_time\",\"std_score_time\",\n",
    "                          \"param_lr\",\"max_epochs\",\"Nh1\",\"Nh2\",\"Pdrop1\",\"Pdrop2\",\"activation\",\"batch_norm\",\n",
    "                          \"optimizer\",\"weight_decay\",\"split0_test_score\",\"split1_test_score\",\n",
    "                          \"split2_test_score\",\"split3_test_score\",\"mean_test_score\",\"std_test_score\",\n",
    "                          \"rank_test_score\"\n",
    "                         ]\n",
    "\n",
    "# print best results of hyper-parameters search\n",
    "rand_search_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1Q60vex7SKW"
   },
   "source": [
    "From the dataframe above we can infer that `Adam` is performing better than `SGD` as 8 models in the top-10 were trained with it. In the same way `ReLU()` outperforms `Tanh` activation.\n",
    "\n",
    "Below we report a summary of the hyper-parameters of the best-performing model and plot of training points together with the learned function from the Random Search best estimator (we set `refit = True` in the random search function, so the best estimator is re-trained over the whole dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6gg0qm-kVb7",
    "outputId": "d8d26043-1eb4-4605-dcca-6efd0ffbee22"
   },
   "outputs": [],
   "source": [
    "print(\"SUMMARY OF THE NETWORK ARCHITECTURE WITH BEST PERFORMANCE:\")\n",
    "print(\"  best score:      \", rand_search.best_score_)\n",
    "print(\"  best parameters: \")\n",
    "for (key, val) in rand_search.best_params_.items():\n",
    "    print(\"    \", key, \":\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "4mjb0e5vV00R",
    "outputId": "0fe17d5b-4c0c-4ddd-cde3-5ab763fccb7c"
   },
   "outputs": [],
   "source": [
    "# representation of the learned function \n",
    "xline = torch.from_numpy(np.linspace(-5,5,200).reshape(-1,1)).float()\n",
    "\n",
    "yline_rs = rand_search.best_estimator_.predict(xline)\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.title(\"Best Estimator resulting from Random Search\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.scatter(XX, YY, label=\"Train data\") \n",
    "plt.plot(xline, yline_rs, c=\"red\", label=\"best estimator prediction\", lw=2)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3JzgGcxkVb8"
   },
   "source": [
    "## Average of Cross Validation trained networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xImVoQ6zkVcB"
   },
   "source": [
    "Now we want to build a regressor using a different approach: we use the hyper-parameters found above to train 5 models in a 5-fold cross validation setup. This allow us to build a regressor by averaging the predictions of these 5 models, making a better use of the very small dataset. <br>\n",
    "\n",
    "To do this, we firstly define the procedure to train a single model inside the function `train_single_model`, at which we give in input the model architecture and hyper-parameters, the train and validation datasets, the maximum number of epochs and also other parameters of the callbacks, *EarlyStopping* and *Checkpoint*. Inside the function we create the callbacks objects, the skorch regressor and finally call the fit method. <br>\n",
    "\n",
    "The second function we created is `cross_validate`, which takes care of splitting the dataset into the specified number of folds (5) and running the loop where the models are trained. It stores into two lists the trained networks and their checkpoints. <br>\n",
    "\n",
    "The third function is `average_CV_prediction`, which simply computes the mean of the output predictions of the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OTrz4vgkVcC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.callbacks import Checkpoint, EarlyStopping\n",
    "\n",
    "# function to train a single model, exploiting also early stopping and checkpoint callbacks\n",
    "def train_single_model(net_model, hyper_params, train_dataset, val_dataset=None, \n",
    "                       max_epochs=50, verbosity=0, patience=5, iter_id=0, \n",
    "                       threshold=0.00001, dir_checkpt=\"checkpt\"):\n",
    "    ### callbacks definitions ###    \n",
    "    # callback for progress bar display\n",
    "    pbar_callback = ProgressBar_epochs(total_epochs=max_epochs)\n",
    "    callbacks_list = [pbar_callback]\n",
    "    \n",
    "    if val_dataset is not None:\n",
    "        # callback for early stopping\n",
    "        early_stop = EarlyStopping(monitor   = \"valid_loss\",\n",
    "                                   patience  = patience,\n",
    "                                   threshold = threshold, \n",
    "                                  )\n",
    "        # callback for retrieving best model w.r.t valid loss\n",
    "        checkpoint = Checkpoint(monitor = \"valid_loss_best\",\n",
    "                                dirname = dir_checkpt+str(iter_id), \n",
    "                               )\n",
    "        callbacks_list += [early_stop, checkpoint]\n",
    "    \n",
    "\n",
    "    ### setting up the regressor object ###\n",
    "    net = NeuralNetRegressor(module     = net_model,\n",
    "                             criterion  = torch.nn.MSELoss,\n",
    "                             batch_size              = -1, \n",
    "                             max_epochs              = max_epochs,\n",
    "                             lr                      = hyper_params[\"lr\"],\n",
    "                             module__Nh1             = hyper_params[\"module__Nh1\"],\n",
    "                             module__Nh2             = hyper_params[\"module__Nh2\"],\n",
    "                             module__Pdrop1          = hyper_params[\"module__Pdrop1\"],\n",
    "                             module__Pdrop2          = hyper_params[\"module__Pdrop2\"],\n",
    "                             module__batch_norm      = hyper_params[\"module__batch_norm\"],\n",
    "                             module__activ           = hyper_params[\"module__activ\"],\n",
    "                             optimizer               = hyper_params[\"optimizer\"],\n",
    "                             optimizer__weight_decay = hyper_params[\"optimizer__weight_decay\"],\n",
    "                             train_split = predefined_split(val_dataset) if val_dataset is not None else False,\n",
    "                             device      = device, # uses GPU if available\n",
    "                             verbose     = verbosity,\n",
    "                             callbacks   = callbacks_list,\n",
    "                            )\n",
    "\n",
    "    ### fitting the model ###\n",
    "    _ = net.fit(train_dataset, y=None)\n",
    "    \n",
    "    print(\"  Final train loss: {}\".format(net.history[-1][\"train_loss\"]))\n",
    "    \n",
    "    if val_dataset is not None:\n",
    "        print(\"  Final validation loss: {}\".format(net.history[-1][\"valid_loss\"]))\n",
    "        return net, checkpoint\n",
    "    else:\n",
    "        return net\n",
    "\n",
    "\n",
    "# function to train N networks with cross validation\n",
    "def cross_validate(net_model, train_df, best_hp,\n",
    "                   Ncv=5, max_epochs=50, rand_seed=42, verbosity=0, \n",
    "                   patience=5, threshold=0.00001, dir_checkpt=\"checkpt\",\n",
    "                  ): \n",
    "    \n",
    "    # dataset splitting object for cross-validation \n",
    "    shuffle_split = ShuffleSplit(n_splits     = Ncv, \n",
    "                                 test_size    = 1./Ncv, \n",
    "                                 random_state = rand_seed,\n",
    "                                )      \n",
    "    estimators = []\n",
    "    checkpoints = []\n",
    "    iter_id = 0\n",
    "    for train_ids, val_ids in shuffle_split.split(train_df[\"input\"].values):\n",
    "        print(\"Iteration: {}\".format(iter_id))\n",
    "        \n",
    "        ### bulding train and validation datasets ###\n",
    "        train_data = train_df.loc[train_ids].values\n",
    "        val_data   = train_df.loc[val_ids].values      \n",
    "\n",
    "        # defining the transformation to Tensor object\n",
    "        composed_transform = transforms.Compose([ToTensor()])\n",
    "        train_dataset = NpDataset(train_data, transform=composed_transform)\n",
    "        val_dataset   = NpDataset(val_data  , transform=composed_transform)\n",
    "        \n",
    "        ### train the network ###\n",
    "        trained_net, checkpoint = train_single_model(net_model,  \n",
    "                                                     hyper_params  = best_hp,\n",
    "                                                     train_dataset = train_dataset, \n",
    "                                                     val_dataset   = val_dataset,\n",
    "                                                     max_epochs    = max_epochs, \n",
    "                                                     verbosity     = verbosity, \n",
    "                                                     patience      = patience,\n",
    "                                                     threshold     = threshold,\n",
    "                                                     dir_checkpt   = dir_checkpt,\n",
    "                                                     iter_id       = iter_id,\n",
    "                                                    )\n",
    "        # storing the best performing model (to be used later)\n",
    "        checkpoints.append(checkpoint)\n",
    "        \n",
    "        # storing the trained regressor\n",
    "        estimators.append(trained_net)\n",
    "        \n",
    "        iter_id += 1\n",
    "        \n",
    "    return estimators, checkpoints\n",
    "\n",
    "\n",
    "# function that computes the mean of the predictions of the trained nets\n",
    "def average_CV_prediction(nets, input_tensor):     \n",
    "\n",
    "    predicted = []\n",
    "    for net in nets:\n",
    "        predicted.append(net.predict(input_tensor))\n",
    "\n",
    "    mean_prediction = np.mean(predicted, axis=0)\n",
    "    \n",
    "    # return single nets predictions and mean\n",
    "    return predicted, mean_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we defined a simple callback to display a progress bar while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvC_00rG7SKa"
   },
   "outputs": [],
   "source": [
    "from skorch.callbacks import Callback\n",
    "from tqdm.notebook import tqdm  \n",
    "\n",
    "# class to display a progress bar of epochs during training\n",
    "class ProgressBar_epochs(Callback):\n",
    "    def __init__(self, total_epochs):\n",
    "        self.Nep = total_epochs\n",
    "        self.current_ep = 0  \n",
    "        self.pbar = tqdm(total=self.Nep)\n",
    "        \n",
    "    def on_train_begin(self, net, **kwargs):\n",
    "        self.current_ep = 0\n",
    "        \n",
    "    def on_train_end(self, net, **kwargs):\n",
    "        self.pbar.close()\n",
    "    \n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        self.old_ep = self.current_ep\n",
    "        self.current_ep = net.history[-1, \"epoch\"]\n",
    "        self.pbar.update(self.current_ep - self.old_ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to reset the random state and run the training in cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455,
     "referenced_widgets": [
      "73dd91f49dee4b52933a449cba1107f8",
      "ad613c7fc8e247868778bd4f35f7d8a6",
      "40ba0116db574e579803a7abe6de7f10",
      "c7d87fd182fa4c428b2639c9001627fb",
      "75688824cf5f4548af3c4c20c44b776f",
      "1ffd218c68b84eeca339f76231bc2acc",
      "1114a79a6914421583deaeacc6b21f9d",
      "76e4b9e4976b43539cb90a0805961050",
      "a60003db20fe4a85947a72187e2676be",
      "43b48cce91204a80ba0a475ca39ab9f3",
      "bfa09f62e9eb415ebc6c80bbb50ebc78",
      "21f135960de241e1bff74a656b7a1836",
      "991d439dcfc04a29bff79e9f7436c31e",
      "254db4bd1b7e48389ed8d42a0922b9cf",
      "1e94da7666644fcd91df408f4eb3b3ed",
      "5ec1ed6eacfe44dea5f3f09e2f9b0547",
      "2cf72201034d432ebc8cb5d312228ba8",
      "7574c215b3824568aaf1f237d01e9ef2",
      "c4b0e92b2b84445493ee2cc7aeacdbe0",
      "bc597386090a423babd01f2db9743926",
      "5a4a048823c4422f95ca1a73968c2fcc",
      "38a13369ac3c40608212c8f0165ffc45",
      "c6e8ecd32d0c44b2806c786a3734df13",
      "59610d278faf4757907519624753dd2c",
      "09c4d5a7b49f45dca584de5994c15ba1",
      "3b32878764224b519614c51e60ef823d",
      "2cab53ec728c4edaa1f8e034599d2906",
      "ca1ff757f7024ecca2cfb15895e632a5",
      "a9f15f372f3641729baaab9909e067fe",
      "25db25c2fda84b5b852437649d0224da",
      "263eddf9d50f4c6682d39c8e63317510",
      "56dfd581ca7647138878e9e25dcedee7",
      "74968528d0ce4d488278c131af9512a0",
      "213a57e4dd484f029c45c45675b277ff",
      "07f63ef1668845f28627f564fadfd763",
      "1aa9923300a240509ada239c06769555",
      "6599e42f559f4ed28bc148df15a9979c",
      "fce4600075514642b03282f62ea58355",
      "2f9043d6bf69417c9215d603cdc532df",
      "ff8d40fddee641f184ffb53a34aaabf3",
      "a21f56c2660b488cb5b452eeee492f29",
      "9c413f8e5d7f456d81bc3aa78b3733b0",
      "1a52d50842e242819340216ce8603f2a",
      "04175d25fd18417ea1194c345eaa8b54",
      "5d2b4c2989234ac9ad1a3a003dc281cb",
      "5f68992d24e04b7f879cc3c27d1d2951",
      "e949d73a7bba4b79af2896f9e8f56657",
      "1e3fa77ad5ac45868286361d615f4745",
      "3fd6a2fba15a4332aad2e853c47f925b",
      "a44f647beef3436c948f60c7bea04c3c",
      "757d56ddc9524e4b84679563bf2126da",
      "ddc78923383a4ffabb6ffffdc24a9f15",
      "faf8774537414975bd4e49dce822c52b",
      "77cef66d9fcc420395112a3cdb7478b8",
      "0708dc5ba88949f686e4e021aa44be7c"
     ]
    },
    "id": "44KNUR4dkVcD",
    "outputId": "5ca4c7e9-3269-405a-e7d5-e11e7b03b343",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setting random state\n",
    "np.random.seed(magic_num)\n",
    "torch.manual_seed(magic_num)\n",
    "random.seed(magic_num)\n",
    "if (device.type == \"cuda\"): \n",
    "    torch.cuda.manual_seed(magic_num)\n",
    "\n",
    "# fitting \"N_fits\" regressors with CV\n",
    "N_fits = 5\n",
    "max_epochs = 1000\n",
    "\n",
    "# measure running time\n",
    "fit_begin = time.time() \n",
    "\n",
    "trained_nets, checkpts = cross_validate(FC2_Net, train_df, rand_search.best_params_, \n",
    "                                        Ncv         = N_fits, \n",
    "                                        max_epochs  = max_epochs, \n",
    "                                        rand_seed   = magic_num,\n",
    "                                        patience    = 100,\n",
    "                                        dir_checkpt = \"model_checkpoint\",\n",
    "                                        verbosity   = 0,\n",
    "                                       )\n",
    "fit_time = time.time() - fit_begin\n",
    "print(f\"Cross Validation fit time:\", str(datetime.timedelta(seconds=fit_time)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "HsAjGQN3kVcE",
    "outputId": "59f9fed4-e280-4d02-9566-4b760a41675e"
   },
   "outputs": [],
   "source": [
    "# plot of the losses\n",
    "fig, ax = plt.subplots(1,2, figsize=(13,6), sharey='row')\n",
    "\n",
    "ax[0].set_title(\"Training Losses\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss value\")\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].set_title(\"Validation Losses\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "ax[1].grid()\n",
    "\n",
    "for ii, estim in enumerate(trained_nets):\n",
    "    ax[0].plot(estim.history[:,\"epoch\"], estim.history[:,\"train_loss\"], label=(\"net_\"+str(ii)))\n",
    "    ax[0].set_yscale(\"log\")\n",
    "    ax[1].plot(estim.history[:,\"epoch\"], estim.history[:,\"valid_loss\"], label=(\"net_\"+str(ii)))\n",
    "    ax[1].set_yscale(\"log\")\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two cells below we reload the models from the checkpoints and we plot a representation of the learned function for each network and also the function represented by the average prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciPuntWDkVcF"
   },
   "outputs": [],
   "source": [
    "# retrieve best models from checkpoints\n",
    "for net, cp in zip(trained_nets, checkpts):\n",
    "    net.load_params(checkpoint=cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "1J7n3N5gkVcH",
    "outputId": "b12ae056-47ff-4ec7-cc54-83c125088b09",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# representation of the learned function \n",
    "xline = torch.from_numpy(np.linspace(-5,5,200).reshape(-1,1)).float()\n",
    "\n",
    "y_predicted, y_cv_pred = average_CV_prediction(trained_nets, xline)\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.title(\"Fitted models\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.scatter(XX, YY, label=\"Train data\") \n",
    "for ii, pp in enumerate(y_predicted):\n",
    "    plt.plot(xline, pp, label=(\"net_\"+str(ii)), lw=1., c=\"orange\")\n",
    "plt.plot(xline, y_cv_pred, c=\"red\", label=\"cv_prediction\", lw=2.)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtVEPF9GV00a"
   },
   "source": [
    "### Evaluation of performances over test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we load the test dataset and evaluate the MSE loss over it for each of the 5 trained networks, the average model and also the model produced by the random search function. Then we overlap in a plot the test points with the learned functions of both the average model and the one from random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONdo9FHykVcI",
    "outputId": "e4f5e154-369b-4a78-e4aa-f0109e9ad031"
   },
   "outputs": [],
   "source": [
    "# test dataset loading\n",
    "test_df = pd.read_csv('regression_dataset/test_data.csv')\n",
    "print(f\"test dataset shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qSfCjnnnkVcJ",
    "outputId": "07be1933-0d8f-4964-92d2-36d022af8364"
   },
   "outputs": [],
   "source": [
    "### performance over test set\n",
    "\n",
    "x_test      = torch.tensor(test_df[\"input\"].values.reshape(-1,1)).float()\n",
    "y_test_true = torch.tensor(test_df[\"label\"].values.reshape(-1,1)).float()\n",
    "\n",
    "# CV predictions, average prediction\n",
    "y_test_pred, y_cv_test_pred = average_CV_prediction(trained_nets, x_test)\n",
    "\n",
    "# predictions by random search best model\n",
    "y_test_rs = rand_search.best_estimator_.predict(x_test)\n",
    "\n",
    "# loss function\n",
    "metric = nn.MSELoss()\n",
    "\n",
    "test_loss = metric(torch.tensor(y_cv_test_pred), y_test_true)\n",
    "print(\"Loss function value over Test dataset for average CV predictor: {}\".format(test_loss))\n",
    "\n",
    "trained_nets_losses = [metric(torch.tensor(arr), y_test_true) for arr in y_test_pred]\n",
    "print(\"\\nLoss function values over Test dataset for all the CV trained networks:\")\n",
    "for ii, arr in enumerate(y_test_pred):\n",
    "    print(\"   net_{}: {}\".format(ii, trained_nets_losses[ii]))\n",
    "    \n",
    "test_loss_rs = metric(torch.tensor(y_test_rs), y_test_true)\n",
    "print(\"\\nLoss function value over Test dataset for random search best model: {}\".format(test_loss_rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "I8aFVOJ0V00c",
    "outputId": "730ea82a-f61a-4b80-f3a5-679342dc20b2"
   },
   "outputs": [],
   "source": [
    "### plot of test points \n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.title(\"Test dataset predictions\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Label\")\n",
    "\n",
    "# test dataset\n",
    "plt.scatter(x_test, y_test_true, c=\"limegreen\", label=\"Test data\", zorder=0)\n",
    "\n",
    "# mean predictor function\n",
    "plt.plot(xline, y_cv_pred, c=\"orange\", label=\"Average CV learned function\", lw=2, zorder=3)\n",
    "\n",
    "# predictions by average over cross validation networks\n",
    "plt.scatter(x_test, y_cv_test_pred, c=\"red\", marker=\".\", s=60, label=\"Average CV predictions\", zorder=4)\n",
    "\n",
    "# best predictor resulting directly from RandomizedSearchCV function\n",
    "yline_rs = rand_search.best_estimator_.predict(xline)\n",
    "plt.plot(xline, yline_rs, c=\"deepskyblue\", label=\"Rand. Search learned function\", zorder=1)\n",
    "\n",
    "# prediction by random search best model\n",
    "plt.scatter(x_test, y_test_rs, marker=\".\", s=60, c=\"blue\", label=\"Rand. Search predictions\", zorder=2)\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"testpreds.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLeCb0IrV00d"
   },
   "source": [
    "## Network analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this section is to gain some insight about the trained networks. In particular we select the model from the random search to investigate its weights distribution and the activation profiles when applied to different input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the best random search model for analysis \n",
    "network = rand_search.best_estimator_.module_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Apc-jgAVkVcL"
   },
   "source": [
    "### Weights histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot the histograms of the weights of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1Wj1H_wkVcM",
    "outputId": "8156ac4e-1fcf-4e20-bfc9-a99af7f2312f"
   },
   "outputs": [],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WeDfIx5kVcN"
   },
   "outputs": [],
   "source": [
    "def plot_weights_hist(network, log_scale=None, save=False):  \n",
    "    ### retrieving network parameters\n",
    "    # First layer\n",
    "    h1_w = network.fc_h1.weight.data.cpu().numpy()\n",
    "    h1_b = network.fc_h1.bias.data.cpu().numpy()\n",
    "\n",
    "    # Second layer\n",
    "    h2_w = network.fc_h2.weight.data.cpu().numpy()\n",
    "    h2_b = network.fc_h2.bias.data.cpu().numpy()\n",
    "\n",
    "    # Output layer\n",
    "    out_w = network.fc_out.weight.data.cpu().numpy()\n",
    "    out_b = network.fc_out.bias.data.cpu().numpy()\n",
    "\n",
    "    ### histograms of the weights of the network\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10,12))\n",
    "    # first layer\n",
    "    axs[0].hist(h1_w.flatten(), 50)\n",
    "    axs[0].set_title(\"First layer weights\")\n",
    "\n",
    "    # second layer\n",
    "    axs[1].hist(h2_w.flatten(), 50)\n",
    "    axs[1].set_title(\"Second layer weights\")\n",
    "\n",
    "    # output layer\n",
    "    axs[-1].hist(out_w.flatten(), 50)\n",
    "    axs[-1].set_title('Output layer weights')\n",
    "    \n",
    "    if log_scale is not None: # setting log scale on specified images\n",
    "        [axs[idx-1].set_yscale('log') for idx in log_scale]\n",
    "    [ax.grid() for ax in axs]\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if save:\n",
    "        fig.savefig(\"weights.pdf\", bbox_inches='tight')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 873
    },
    "id": "M0pZbyB_V00g",
    "outputId": "de224940-6dce-4e1e-82b5-aa7310abe098"
   },
   "outputs": [],
   "source": [
    "plot_weights_hist(network, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gekiWPi9kVcO"
   },
   "source": [
    "### Activations profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here instead we use a hook function to retrieve the activations of hidden layers when an input value is forwarded through the network. Then we plot the activations obtained with different input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tcUg7vad15Oz",
    "outputId": "ba36d29e-6c25-4f89-aa88-6ac510659233"
   },
   "outputs": [],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQJ8tXAakVcP"
   },
   "outputs": [],
   "source": [
    "class ActivationProfiles():\n",
    "    def __init__(self, network):\n",
    "        self.net = network\n",
    "        self.activation_func = self.net.act  #retrieving used activation function\n",
    "        \n",
    "        # Register hook on first and second hidden layer\n",
    "        self.hook_h1 = self.net.fc_h1.register_forward_hook(self.get_activation_h1)\n",
    "        self.hook_h2 = self.net.fc_h2.register_forward_hook(self.get_activation_h2)\n",
    "        self.activation_h1 = 0\n",
    "        self.activation_h2 = 0\n",
    "    \n",
    "    def get_activation_h1(self, layer, input, output):\n",
    "        self.activation_h1 = self.activation_func(output)\n",
    "\n",
    "    def get_activation_h2(self, layer, input, output):\n",
    "        self.activation_h2 = self.activation_func(output)\n",
    "        \n",
    "    def close(self):\n",
    "        self.hook_h1.remove()\n",
    "        self.hook_h2.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cn72zqFpkVcQ"
   },
   "outputs": [],
   "source": [
    "def plot_activations(network, pts, save=False):\n",
    "    activ_cache = ActivationProfiles(network)\n",
    "\n",
    "    # Analyze activations on points in pts\n",
    "    network.eval()\n",
    "    ys, zs_1, zs_2 = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for pi in pts:\n",
    "            if network.batch_norm: ##\n",
    "                ys.append( network( torch.tensor([pi]).float().unsqueeze(0).to(device) ) )\n",
    "                zs_1.append(activ_cache.activation_h1.squeeze())\n",
    "                zs_2.append(activ_cache.activation_h2.squeeze())\n",
    "            else:\n",
    "                ys.append( network( torch.tensor([pi]).float().to(device) ) )\n",
    "                zs_1.append(activ_cache.activation_h1)\n",
    "                zs_2.append(activ_cache.activation_h2)\n",
    "                \n",
    "    # remove hooks\n",
    "    activ_cache.close()\n",
    "\n",
    "    # Plot activations\n",
    "    k = len(pts)\n",
    "    fig, axs = plt.subplots(k, 2, figsize=(12,9), sharey=True)\n",
    "\n",
    "    for idx, ax in enumerate(axs):\n",
    "        ax[0].stem(zs_1[idx].cpu(), use_line_collection=True)\n",
    "        ax[0].set_title(\"1st hidden layer activations for input x= %.2f\" % pts[idx])\n",
    "        ax[1].stem(zs_2[idx].cpu(), use_line_collection=True)\n",
    "        ax[1].set_title(\"2nd hidden layer activations for input x= %.2f\" % pts[idx])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if save:\n",
    "        fig.savefig(\"activations.pdf\", bbox_inches='tight')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "qgcTjdsoV00k",
    "outputId": "02e54757-68ed-4ddd-974f-7768d1ec0a47"
   },
   "outputs": [],
   "source": [
    "# points for which retrieve activations\n",
    "pts = [-4., -2., 0.1, 2.5, 4.]\n",
    "\n",
    "plot_activations(network, pts, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruwe2n1EZ7Y9"
   },
   "source": [
    "## Testing the effect of a different activation or regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we try to change some of the hyper-parameters used in the previous trainings and see the effects on the weights distribution and activations profiles. We tested:\n",
    "* `Tanh` activation in place of *ReLU*;\n",
    "* `Strong L2 penalty`, setting the L2 coefficient to 0.5;\n",
    "* without `Batch normalization`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ksCxVLJV00l"
   },
   "source": [
    "### 1) Tanh activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "d3ee99ede3fe423da0d8b518be45324d",
      "3c2957fc12414b87a1ae48c928e574ee",
      "98adfa4b790c426c99a1ebc56da17b15",
      "109802b20db54f3780bed00341d7a48c",
      "299278f530af4311869900fc5446178f",
      "def781400daf4f7a8dcafe25448c1b96",
      "5178dc89425e4e878d90159aed1f3675",
      "f85d3ebd063d498981bb21692667d1e5",
      "0e67beee4c37430a8011ef2e1bd49dde",
      "7f9a0842d021458ca0ac0a09a1869af3",
      "b3a0ca639c644d9d9517dac19565f45f"
     ]
    },
    "id": "vKlddundZ7Y9",
    "outputId": "040a56b1-967f-4aa3-cf2f-b5aae4382c06"
   },
   "outputs": [],
   "source": [
    "# setting random state\n",
    "np.random.seed(magic_num)\n",
    "torch.manual_seed(magic_num)\n",
    "random.seed(magic_num)\n",
    "if (device.type == \"cuda\"): \n",
    "    torch.cuda.manual_seed(magic_num)\n",
    "\n",
    "# changing activation from ReLU to Tanh\n",
    "params_tanh = rand_search.best_params_.copy()\n",
    "params_tanh[\"module__activ\"] = nn.Tanh()\n",
    "\n",
    "# training\n",
    "tanh_net = train_single_model(net_model     = FC2_Net,  \n",
    "                              hyper_params  = params_tanh,\n",
    "                              train_dataset = train_dataset, \n",
    "                              val_dataset   = None,\n",
    "                              max_epochs    = params_tanh[\"max_epochs\"], \n",
    "                              verbosity     = 0, \n",
    "                             )\n",
    "\n",
    "# test loss\n",
    "metric = nn.MSELoss()\n",
    "\n",
    "y_test_tanh = tanh_net.predict(x_test)\n",
    "test_loss = metric(torch.tensor(y_test_tanh), y_test_true)\n",
    "print(\"  Loss over test dataset: {}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "o2I1uE2rV00n",
    "outputId": "5be47993-7c05-439f-9c48-da15d4a3cfc9"
   },
   "outputs": [],
   "source": [
    "# representation of the learned function \n",
    "xline = torch.from_numpy(np.linspace(-5,5,200).reshape(-1,1)).float()\n",
    "yline_tanh = tanh_net.predict(xline)\n",
    "\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.scatter(test_df[\"input\"].values , test_df[\"label\"].values , c=\"limegreen\", label=\"Test data\")\n",
    "plt.scatter(train_df[\"input\"].values, train_df[\"label\"].values, label=\"Train data\") \n",
    "plt.plot(xline, yline_tanh, c=\"red\", label=\"tanh_net prediction\", lw=2)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 873
    },
    "id": "vvoPpfAlV00o",
    "outputId": "96eb24c1-ebb4-4907-d2c4-9265b9ca372f"
   },
   "outputs": [],
   "source": [
    "plot_weights_hist(tanh_net.module_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 840
    },
    "id": "sIlhxg-SV00p",
    "outputId": "b8771386-8904-4086-fa3d-d0165bebfd4d"
   },
   "outputs": [],
   "source": [
    "plot_activations(tanh_net.module_, pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above we can say that `Tanh` activation produces a smoother prediction function with respect to `ReLU`, although not as good at representing the curve. In the activations profiles it can be noticed that positive and negative input values presents opposite activation patterns. Also, for inputs close to zero activations are almost never saturated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaJXl24MV00q"
   },
   "source": [
    "### 2) Strong L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "08f31e703ecb42a581ce2c9c21d555a3",
      "9a2668615bb84fbcbabe539fc7e65947",
      "47348f321b2645e0baa740c7b2012c9e",
      "63cd09a2dff944d5bf83e72931b86cf2",
      "6d181b41d8e3454bb44fd3e16c377e84",
      "9801b9da46604b57bd9ba41f3d5fc75c",
      "22c61c56457846c1af537679b161376a",
      "d35d5bec2da64253b215d1a8f9689c20",
      "d084aada3c194ef2a0bcd0ca905cd815",
      "022713efb03043ee9ad23326938da06c",
      "84b760c8a21f40e59a1ec865f796ec9f"
     ]
    },
    "id": "SpXwTkM1V00q",
    "outputId": "b3376e34-a649-43c4-892c-196cf2bdf6e8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setting random state\n",
    "np.random.seed(magic_num)\n",
    "torch.manual_seed(magic_num)\n",
    "random.seed(magic_num)\n",
    "if (device.type == \"cuda\"): \n",
    "    torch.cuda.manual_seed(magic_num)\n",
    "\n",
    "# changing L2 penalty value\n",
    "params_high_L2 = rand_search.best_params_.copy()\n",
    "params_high_L2[\"optimizer__weight_decay\"] = 0.5\n",
    "\n",
    "# L2 regularization slows down the learning, so here we increase the number of epochs to compensate\n",
    "params_high_L2[\"max_epochs\"] = 300\n",
    "\n",
    "# training\n",
    "high_L2_net = train_single_model(net_model     = FC2_Net,  \n",
    "                                 hyper_params  = params_high_L2,\n",
    "                                 train_dataset = train_dataset, \n",
    "                                 val_dataset   = None,\n",
    "                                 max_epochs    = params_high_L2[\"max_epochs\"], \n",
    "                                 verbosity     = 0, \n",
    "                                )\n",
    "\n",
    "# test loss\n",
    "metric = nn.MSELoss()\n",
    "\n",
    "y_test_high_L2 = high_L2_net.predict(x_test)\n",
    "test_loss = metric(torch.tensor(y_test_high_L2), y_test_true)\n",
    "print(\"  Loss over test dataset: {}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "BT2hkOT4V00r",
    "outputId": "e87a8a8c-223d-4df4-f960-030a0f8f59b7"
   },
   "outputs": [],
   "source": [
    "# representation of the learned function \n",
    "xline = torch.from_numpy(np.linspace(-5,5,200).reshape(-1,1)).float()\n",
    "yline_high_L2 = high_L2_net.predict(xline)\n",
    "\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.scatter(test_df[\"input\"].values , test_df[\"label\"].values , c=\"limegreen\", label=\"Test data\")\n",
    "plt.scatter(train_df[\"input\"].values, train_df[\"label\"].values, label=\"Train data\") \n",
    "plt.plot(xline, yline_high_L2, c=\"red\", label=\"high_L2_net prediction\", lw=2)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 873
    },
    "id": "PIfeVIG4V00s",
    "outputId": "1cab5e40-c6bc-4648-ec46-0f8a90db1086"
   },
   "outputs": [],
   "source": [
    "plot_weights_hist(high_L2_net.module_, log_scale=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "id": "pdn90Y5wV00s",
    "outputId": "56880e2d-3a6c-4d8a-ee2f-602aeadbd583"
   },
   "outputs": [],
   "source": [
    "plot_activations(high_L2_net.module_, pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strong `L2 penalty` is restricting the weights to a very narrow range around 0, especially for the second layer, but not for the output one. Also activation patterns are heavily flattened towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRLCm0M1V00t"
   },
   "source": [
    "### 3) Without Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "62512736750f46bd98c3ebd94f97211c",
      "33de1234b6124b43a65788a70a794ac0",
      "d52bc198123f49ccb68d68dc1a6ec903",
      "d1fc2cae660a4ad0bb63026a06807cf5",
      "7901e30a0c0549d999609ab464f3f122",
      "8ee9c82c1f1a46e795cd780b95e24ef7",
      "2f49fa3df4aa49fcad1d702b40daf350",
      "9df090e144484019b0a40563285d32bb",
      "23baad3e661e473691217ca3cfed7c68",
      "2b847d4919d74cfdb28e5cd3972b9d81",
      "904f503a2fb9488aaf6a5dc955463cb0"
     ]
    },
    "id": "ZgRS1LQ_V00u",
    "outputId": "038ff6ba-b611-4024-f872-05784d1d3053"
   },
   "outputs": [],
   "source": [
    "# setting random state\n",
    "np.random.seed(magic_num)\n",
    "torch.manual_seed(magic_num)\n",
    "random.seed(magic_num)\n",
    "if (device.type == \"cuda\"): \n",
    "    torch.cuda.manual_seed(magic_num)\n",
    "\n",
    "# removing batch normalization\n",
    "params_NO_BatchNorm = rand_search.best_params_.copy()\n",
    "params_NO_BatchNorm[\"module__batch_norm\"] = False\n",
    "\n",
    "# as above we increase the number of epochs\n",
    "params_NO_BatchNorm[\"max_epochs\"] = 300\n",
    "\n",
    "# training\n",
    "NO_BatchNorm_net = train_single_model(net_model     = FC2_Net,  \n",
    "                                      hyper_params  = params_NO_BatchNorm,\n",
    "                                      train_dataset = train_dataset, \n",
    "                                      val_dataset   = None,\n",
    "                                      max_epochs    = params_NO_BatchNorm[\"max_epochs\"], \n",
    "                                      verbosity     = 0, \n",
    "                                     )\n",
    "# test loss\n",
    "metric = nn.MSELoss()\n",
    "\n",
    "y_test_NO_BN = NO_BatchNorm_net.predict(x_test)\n",
    "test_loss = metric(torch.tensor(y_test_NO_BN), y_test_true)\n",
    "print(\"  Loss over test dataset: {}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "lPBkuza2V00v",
    "outputId": "1ac45aa0-7a4e-46d0-a201-154b3294d20b"
   },
   "outputs": [],
   "source": [
    "# representation of the learned function \n",
    "xline = torch.from_numpy(np.linspace(-5,5,200).reshape(-1,1)).float()\n",
    "yline_NO_bn = NO_BatchNorm_net.predict(xline)\n",
    "\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.scatter(test_df[\"input\"].values , test_df[\"label\"].values , c=\"limegreen\", label=\"Test data\")\n",
    "plt.scatter(train_df[\"input\"].values, train_df[\"label\"].values, label=\"Train data\") \n",
    "plt.plot(xline, yline_NO_bn, c=\"red\", label=\"NO_BatchNorm_net prediction\", lw=2)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 873
    },
    "id": "FxqLvb2HV00w",
    "outputId": "786f732f-8a06-4df1-d713-a8a51952ed66"
   },
   "outputs": [],
   "source": [
    "plot_weights_hist(NO_BatchNorm_net.module_, log_scale=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 847
    },
    "id": "dpkyGiLuV00x",
    "outputId": "7d5c4b9e-2bcf-4f18-fce6-1d96ce10dce2"
   },
   "outputs": [],
   "source": [
    "plot_activations(NO_BatchNorm_net.module_, pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing batch normalization resulted in a wider distribution of weights of the second layer and also a less defined distribution for weights of output layer (with batch normalization active, the distribution was clearly bi-modal). Activation profiles instead remain quite similar to the ones of the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMVrnAyNkVcR"
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of a neural network as a regression model has been studied. While the provided train dataset was very noisy and small, the network has been able to well approximate the underlying function. <br>\n",
    "Hyper-parameters search has been done with a randomized search in a cross validation setup, wrapping up the `PyTorch` network into a `skorch` object and using the function `RandomizedSearchCV` of the popular framework `sklearn`. <br>\n",
    "The random search function already provided a good predictor, but also another approach has been followed: an ensemble of 5 networks with same hyper-parameters were trained in a cross validation setup and used to make predictions by averaging over their single predictions. It can be noticed that the test loss of the average predictions is smaller than the average of the single test losses; this confirms the chosen approach, although the difference is not so high. Indeed, even if sometimes there is a single net with a better test loss than the the average, it must be said that test points are not sampled uniformly in the range [-5,5], so an error in a region with more test points counts more than errors in less represented regions. Also, taking the mean of the predictions seems to mitigate the overfitting (knees in the function plot) that is present in some of the single models. <br>\n",
    "Looking at the two regions in the train dataset where points were missing it can be noticed that the first one (in range [-3,-1]) can be acceptably approximated by both the obtained predictor, while the second one (in range [2,3]) is more difficult to learn. Note that in both cases the average model is performing better, probably due to weaker overfitting thanks to cross validation and also to the use of checkpoint.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RegressionTask_MicheleGuadagnini_Mt_1230663_colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

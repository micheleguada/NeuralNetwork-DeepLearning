{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9nMupgOkVa8"
   },
   "source": [
    "# NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "---\n",
    "A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "---\n",
    "\n",
    "\n",
    "# Homework 2 - Unsupervised Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx94AFKGkVbQ"
   },
   "source": [
    "### Author: Michele Guadagnini - Mt.1230663"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TC5QuD24kVbW"
   },
   "outputs": [],
   "source": [
    "### ADDITIONAL LIBRARIES THAT NEED INSTALLATION (uncomment if needed)\n",
    "\n",
    "#!pip install optuna\n",
    "#!pip install pytorch-lightning\n",
    "\n",
    "### the followings are required to plot and save figures about optuna study\n",
    "#!pip install plotly\n",
    "#!pip install kaleido\n",
    "\n",
    "### the following one is required to print a model summary\n",
    "#!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-xNC0ynkVba"
   },
   "outputs": [],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# python imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# additional libraries\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import torchinfo\n",
    "import optuna\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# reduce verbosity \n",
    "logging.getLogger(\"optuna\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to have reproducible results: \n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "\n",
    "### 'seed_everything' internally calls the followings:\n",
    "#    random.seed(seed)\n",
    "#    np.random.seed(seed)\n",
    "#    torch.manual_seed(seed)\n",
    "#    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load user settings from file 'settings.py'\n",
    "import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc\"></a>\n",
    "# Table of contents:  \n",
    "---\n",
    "\n",
    "1. [**Autoencoder**](#autoencoder)\n",
    "    1. [Hyper-parameters optimization with Optuna](#optunaAE)\n",
    "    1. [Model training](#trainingAE)\n",
    "    1. [Model testing and analysis](#testingAE)   \n",
    "\n",
    "1. [**Transfer Learning**](#transfer_learning)\n",
    "    1. [Encoder fine-tuning for classification](#finetuning)\n",
    "    1. [Model testing and analysis](#finetesting)\n",
    "    \n",
    "1. [**Denoising autoencoder**](#denoising_autoencoder)\n",
    "    1. [Model training](#trainingDAE)\n",
    "    1. [Model testing and analysis](#testingDAE)\n",
    "\n",
    "1. [**Variational Autoencoder ($\\beta$-VAE)**](#VAE)\n",
    "    1. [Model training](#trainingVAE)\n",
    "    1. [Model testing and analysis](#testingVAE)\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYQGjzJlkVbT"
   },
   "source": [
    "<a name=\"autoencoder\"></a> \n",
    "# Autoencoder \n",
    "     \n",
    "[Table of contents](#toc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"optunaAE\"></a> \n",
    "## Hyper-parameters optimization with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(seed=settings.MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_management.fashion_mnist import FashionMNISTDataModule\n",
    "\n",
    "datamodule = FashionMNISTDataModule(data_dir   = settings.DATASETS_DIR, \n",
    "                                    batch_size = 256,\n",
    "                                    Nsamples   = None,\n",
    "                                    valid_frac = 8800./60000.,\n",
    "                                    random_state = settings.MAGIC_NUM,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder.symmetric_autoencoder import SymmetricAutoencoder, SymmetricAutoencoderHPS\n",
    "\n",
    "# convolutional architectures definition\n",
    "### each line is: kernel size, stride, padding\n",
    "proposed_conv = [ [[3, 2, 1], # shape through network: 28   -> 14 ->  7 -> 5\n",
    "                   [3, 2, 1],\n",
    "                   [3, 1, 0],                  \n",
    "                  ],\n",
    "                  [[5, 2, 1], # shape through network: 28   -> 13 ->  6 -> 6\n",
    "                   [5, 2, 1],\n",
    "                   [3, 1, 1],                   \n",
    "                  ],\n",
    "                  [[7, 2, 1], # shape through network: 28   -> 12 ->  5 -> 5\n",
    "                   [5, 2, 1],\n",
    "                   [3, 1, 1],                   \n",
    "                  ],\n",
    "                ]\n",
    "\n",
    "# channels configurations definition\n",
    "proposed_channels = [[16,32,32],\n",
    "                     [16,32,64],\n",
    "                     [32,32,64],\n",
    "                    ]\n",
    "\n",
    "# hyper-parameters space dictionary\n",
    "# Notes:\n",
    "### 1. no batch norm to ensure independence of encodings for different images. Instance norm instead.\n",
    "### 2. dropout is applied only after a linear layer; some tests showed that a small dropout rate help the model\n",
    "###    learn a better clustered latent space\n",
    "### 3. we will use the pytorch lightning functionality \"auto_lr_find\" for the learning rate\n",
    "\n",
    "hps_dict = dict(conv_configs        = proposed_conv,  \n",
    "                channels_configs    = proposed_channels,  \n",
    "                n_linear            = [1],                # number of linear layers #FIXED\n",
    "                linear_units_range  = [128, 256, 8],      # min, max, step\n",
    "                latent_space_range  = [10, 40, 2],        # min, max, step\n",
    "                instance_norm       = [True, False],      # instance norm\n",
    "                Pdropout_range      = [0.16, 0.16],       # dropout                 #FIXED\n",
    "                activations         = [\"leaky_relu\"],                               #FIXED\n",
    "                optimizers          = [\"adam\", \"sgd\", \"adamax\"], \n",
    "                learning_rate_range = [1., 1.],           # we will use \"auto_lr_find\" functionality of Pytorch Lightning \n",
    "                L2_penalty_range    = [1e-7, 1e-4],\n",
    "                momentum_range      = [0.91, 0.91],       # momentum paramater (used only with sgd)  #FIXED\n",
    "               )\n",
    "\n",
    "hp_space = SymmetricAutoencoderHPS(hps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.train_tools import Objective\n",
    "\n",
    "### optuna study objective function\n",
    "objective = Objective(model_class = SymmetricAutoencoder, \n",
    "                      datamodule  = datamodule, \n",
    "                      hp_space    = hp_space,\n",
    "                      max_epochs  = 60, \n",
    "                      use_gpu     = settings.USE_GPU,\n",
    "                      early_stop_patience = 10,\n",
    "                     )\n",
    "\n",
    "### MedianPruner\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials = 10,    # trials to complete before starting to prune\n",
    "                                     n_warmup_steps   = 20,    # steps to take before evaluating pruning\n",
    "                                     interval_steps   = 10,    # steps between trial pruning checks\n",
    "                                    )\n",
    "\n",
    "# Make the default sampler behave in a deterministic way\n",
    "sampler = optuna.samplers.TPESampler(n_startup_trials = 30,    # use random sampling at beginning\n",
    "                                     #seed = settings.MAGIC_NUM,\n",
    "                                    )\n",
    "### create study\n",
    "os.makedirs(settings.autoencoder.OPTUNA_DIR, exist_ok=True)\n",
    "\n",
    "study_name = settings.autoencoder.OPTUNA_STUDY_NAME + \"_local\"\n",
    "study = optuna.create_study(study_name = study_name, \n",
    "                            direction  = \"minimize\",\n",
    "                            pruner     = pruner,\n",
    "                            sampler    = sampler,\n",
    "                            storage    = \"sqlite:///\"+settings.autoencoder.OPTUNA_DIR+\"/\"+study_name+\".db\",\n",
    "                            load_if_exists = True,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run optimization\n",
    "logging.captureWarnings(True)\n",
    "\n",
    "Ntrials = 5\n",
    "MaxTime = None\n",
    "\n",
    "### In the following cells we will load a database with the results of the study runned on COLAB with \n",
    "###   GPU support. To run the study instead, uncomment the lines below.\n",
    "\n",
    "#print(\"Starting study '\"+study.study_name+f\"' with n_trials={Ntrials} and timeout={MaxTime}\")\n",
    "#study.optimize(objective, \n",
    "#               n_trials       = Ntrials, \n",
    "#               timeout        = MaxTime, # timeout in seconds\n",
    "#               gc_after_trial = True,    # run garbage collection \n",
    "#              ) \n",
    "\n",
    "logging.captureWarnings(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the COLAB study database (change 'study_name' to load a different study)\n",
    "study_name = settings.autoencoder.OPTUNA_STUDY_NAME + \"_COLAB\"\n",
    "\n",
    "study = optuna.load_study(study_name, \n",
    "                          storage = \"sqlite:///\"+settings.autoencoder.OPTUNA_DIR+\"/\"+study_name+\".db\",\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print dataframe with top-K trials\n",
    "K = 10\n",
    "\n",
    "study_df = study.trials_dataframe()\n",
    "study_df.drop(columns=\"user_attrs_hypers\", inplace=True)\n",
    "study_df.drop(columns=\"datetime_complete\", inplace=True)\n",
    "study_df = study_df.sort_values(by=\"value\")\n",
    "\n",
    "study_df.head(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.train_tools import OptimizationInspector\n",
    "\n",
    "optuna_inspector = OptimizationInspector(study, settings.autoencoder.OPTUNA_DIR, figsize=(900,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters sets for parallel plots \n",
    "parallel_sets = [[\"architecture\", #name suffix\n",
    "                  \"channels_config_id\",\"conv_config_id\",\"linear_units_0\",\"latent_space_dim\",\n",
    "                 ],\n",
    "                 [\"optimization\", #name suffix\n",
    "                  \"optimizer\",\"L2_penalty\",\"learning_rate\",\"instance_norm\",#\"Pdropout\",#\"momentum\",\n",
    "                 ],\n",
    "                ]\n",
    "\n",
    "# parameters sets for contour plots\n",
    "contour_sets = [[\"channels_config_id\",\"conv_config_id\"],\n",
    "                [\"linear_units_0\", \"latent_space_dim\"],\n",
    "                [\"learning_rate\",\"L2_penalty\"],\n",
    "               ]\n",
    "\n",
    "# parameters sets for slice plots\n",
    "slice_sets   = [[\"conv_config_id\",\"channels_config_id\",\"linear_units_0\",\"latent_space_dim\",\"optimizer\"],\n",
    "               ]\n",
    "\n",
    "# parameters to use for importance plot\n",
    "importance_params = [\"conv_config_id\",\"channels_config_id\",\"linear_units_0\",\"latent_space_dim\",\n",
    "                     \"optimizer\",\"L2_penalty\",\"instance_norm\",\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optuna_inspector.plot_all(parallel_sets     = parallel_sets,\n",
    "                          contour_sets      = contour_sets,\n",
    "                          slice_sets        = slice_sets,\n",
    "                          importance_params = importance_params,\n",
    "                          save = True,\n",
    "                          show = \"111111111\", #\"110001000\",    # show options\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_inspector.print_summary()\n",
    "optuna_inspector.save_best_hypers_json(settings.autoencoder.BEST_HYPERS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"trainingAE\"></a> \n",
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(seed=settings.MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# read best hyper-parameters from file\n",
    "with open(settings.autoencoder.BEST_HYPERS_FILE, \"r\") as file:\n",
    "    best_hypers = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_management.fashion_mnist import FashionMNISTDataModule\n",
    "\n",
    "### define datamodule\n",
    "datamodule = FashionMNISTDataModule(data_dir   = settings.DATASETS_DIR, \n",
    "                                    batch_size = 256,\n",
    "                                    Nsamples   = None,\n",
    "                                    valid_frac = 8800./60000.,\n",
    "                                    random_state = settings.MAGIC_NUM,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below a summary of the model to train is showed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from autoencoder.symmetric_autoencoder import SymmetricAutoencoder\n",
    "\n",
    "# print model summary\n",
    "shape = datamodule.get_sample_size()\n",
    "net   = SymmetricAutoencoder(shape, \n",
    "                             params        = best_hypers[\"params\"],\n",
    "                             optimizer     = best_hypers[\"optimizer\"],\n",
    "                             learning_rate = best_hypers[\"learning_rate\"],\n",
    "                             L2_penalty    = best_hypers[\"L2_penalty\"],\n",
    "                             momentum      = best_hypers[\"momentum\"],\n",
    "                            )\n",
    "\n",
    "# build dummy data batch \n",
    "dummy_batch = list(shape)\n",
    "dummy_batch.insert(0, 256)\n",
    "\n",
    "# print summary\n",
    "torchinfo.summary(net, \n",
    "                  dummy_batch, \n",
    "                  col_width = 20, \n",
    "                  col_names = (\"output_size\",\"num_params\",\"mult_adds\",),  #\"input_size\",\n",
    "                  depth     = 4, \n",
    "                  row_settings = (\"var_names\",),\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we define some callbacks that will be useful during and after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.train_tools import LossesTracker\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from utilities.encoder_tools import ImageReconstruction\n",
    "\n",
    "### callbacks\n",
    "# track the losses (train and valid) during training\n",
    "losses_tracker = LossesTracker()\n",
    "\n",
    "# early stopping\n",
    "early_stop = EarlyStopping(monitor   = \"val_loss\", \n",
    "                           min_delta = 0.0001, \n",
    "                           patience  = 10, \n",
    "                           verbose   = False, \n",
    "                           check_on_train_epoch_end=False, # check early_stop at end of training epoch\n",
    "                          )\n",
    "\n",
    "# checkpoint the model if the monitored value improved\n",
    "checkpoint = ModelCheckpoint(dirpath  = settings.CHECKPOINT_DIR, \n",
    "                             filename = \"checkpt_{epoch}_{val_loss:.2f}\", \n",
    "                             monitor  = \"val_loss\",\n",
    "                            )\n",
    "\n",
    "# reconstruction of a test sample at every epoch\n",
    "sample_id = 34    # bag\n",
    "dataset = torchvision.datasets.FashionMNIST(settings.DATASETS_DIR, \n",
    "                                            train    = False, \n",
    "                                            download = True,\n",
    "                                           )\n",
    "sample = transforms.functional.to_tensor(dataset[sample_id][0]).unsqueeze(dim=0)\n",
    "if settings.USE_GPU:\n",
    "    sample = sample.to(\"cuda\")\n",
    "\n",
    "rec_callback = ImageReconstruction(sample, \n",
    "                                   to_show   = False, \n",
    "                                   save_path = settings.autoencoder.RECONSTRUCTIONS_DIR,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define model and hyper-parameters\n",
    "model = SymmetricAutoencoder(input_size    = datamodule.get_sample_size(),\n",
    "                             params        = best_hypers[\"params\"],\n",
    "                             optimizer     = best_hypers[\"optimizer\"],\n",
    "                             learning_rate = best_hypers[\"learning_rate\"],\n",
    "                             L2_penalty    = best_hypers[\"L2_penalty\"],\n",
    "                             momentum      = best_hypers[\"momentum\"],\n",
    "                            )\n",
    "\n",
    "### define trainer\n",
    "trainer = pl.Trainer(logger     = False,\n",
    "                     max_epochs = 200,\n",
    "                     gpus       = 1 if settings.USE_GPU else None,\n",
    "                     callbacks  = [losses_tracker, early_stop, checkpoint, rec_callback],\n",
    "                     val_check_interval   = 1.,\n",
    "                     enable_model_summary = False,\n",
    "                     num_sanity_val_steps = 0,     # disable validation sanity check before training\n",
    "                     auto_lr_find = False,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Training started at:\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") )\n",
    "fit_begin = time.time()  # measure running time\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule) # run the training\n",
    "\n",
    "fit_time = time.time() - fit_begin\n",
    "print( \"Training ended at:\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") )\n",
    "print(f\"Fit time:\", str(datetime.timedelta(seconds=fit_time)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.plot_tools import plot_history\n",
    "\n",
    "save_path = settings.autoencoder.ROOT_DIR + \"/train_history.pdf\"\n",
    "\n",
    "plot_history(losses_tracker.train, losses_tracker.valid, ylog=True, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# copy best model checkpoint into the results directory\n",
    "shutil.copy(checkpoint.best_model_path, settings.autoencoder.BEST_MODEL_CKPT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"testingAE\"></a> \n",
    "## Model testing and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(seed=settings.MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from data_management.fashion_mnist import FashionMNISTDataModule\n",
    "from autoencoder.symmetric_autoencoder import SymmetricAutoencoder\n",
    "\n",
    "### define datamodule\n",
    "datamodule = FashionMNISTDataModule(data_dir   = settings.DATASETS_DIR, \n",
    "                                    batch_size = 256,\n",
    "                                    Nsamples   = None,\n",
    "                                    valid_frac = 8800./60000.,\n",
    "                                    random_state = settings.MAGIC_NUM,\n",
    "                                   )\n",
    "\n",
    "### load model from checkpoint\n",
    "model = SymmetricAutoencoder.load_from_checkpoint( settings.autoencoder.BEST_MODEL_CKPT_FILE )\n",
    "model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.encoder_tools import EncodedRepresentation\n",
    "\n",
    "# callback to save the encoded representations of the test samples\n",
    "encoded_test_repr = EncodedRepresentation()\n",
    "\n",
    "trainer = pl.Trainer(logger     = False,\n",
    "                     gpus       = 1 if settings.USE_GPU else None,\n",
    "                     callbacks  = [encoded_test_repr],\n",
    "                    )\n",
    "\n",
    "result = trainer.test(model, datamodule=datamodule, verbose=False)\n",
    "test_loss = result[0][\"test_loss\"]\n",
    "print(\"TEST LOSS: \", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Space exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.encoder_tools import LatentSpaceAnalyzer\n",
    "\n",
    "# test dataset encoded samples\n",
    "Latent_analyzer = LatentSpaceAnalyzer(encoded_test_repr.encoded_samples,\n",
    "                                      encoded_test_repr.labels,\n",
    "                                      label_names = datamodule.get_label_names(),\n",
    "                                      save_path = settings.autoencoder.ROOT_DIR,\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_analyzer.PCA_reduce(n_components=2, filename=\"PCA_reduced_space.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_analyzer.TSNE_reduce(n_components=2, perplexity=80, filename=\"TSNE_reduced_space.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_analyzer.Isomap_reduce(n_components=2, filename=\"Isomap_reduced_space.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generation from latent codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings_df = pd.DataFrame(encoded_test_repr.encoded_samples)\n",
    "test_enc_max = max(test_encodings_df.max())\n",
    "test_enc_min = min(test_encodings_df.min())\n",
    "\n",
    "print(f\"Range of test dataset encoded values: [{test_enc_min}, {test_enc_max}]\")\n",
    "\n",
    "# latent dimension\n",
    "latent_dim = model.enc_hp[\"latent_space_dim\"]\n",
    "\n",
    "### generate some examples of images from random latent codes\n",
    "examples  = 20\n",
    "images    = []\n",
    "encodings = []\n",
    "for ii in range(examples):\n",
    "    # randomly sample from latent space\n",
    "    latent_code = np.random.uniform(test_enc_min*0.6, test_enc_max*0.6, latent_dim)\n",
    "    # append encoding\n",
    "    encodings.append(latent_code)                                    \n",
    "    # generate image\n",
    "    encoded_sample = torch.tensor(latent_code).float().unsqueeze(dim=0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.decoder(encoded_sample)\n",
    "    # append image\n",
    "    images.append(generated)     \n",
    "    \n",
    "encodings_df = pd.DataFrame(encodings)\n",
    "encodings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.plot_tools import plot_img_grid\n",
    "\n",
    "_ = plot_img_grid((4,5), images, to_show=True, axis_off=False, figsize=(10,8), \n",
    "                  folder_path = settings.autoencoder.ROOT_DIR,\n",
    "                  filename    = \"generated_images.pdf\",\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlap sampled points to the PCA plot by applying to them the same transformation\n",
    "Latent_analyzer.PCA_overlap_points(encodings, to_show=True, filename=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space path along centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute centroids of clusters in latent space for each label and generate images\n",
    "examples = len(datamodule.get_label_names())\n",
    "centroids_imgs = []\n",
    "centroids      = []\n",
    "for ii in range(examples):\n",
    "    # compute centroid for i-esim label    \n",
    "    mask = [(ll == ii) for ll in encoded_test_repr.labels]\n",
    "    filtered_samples = np.array(encoded_test_repr.encoded_samples)[mask] \n",
    "    latent_code = np.mean(filtered_samples, axis=0)    \n",
    "    # append encoding\n",
    "    centroids.append(latent_code)                                    \n",
    "    # generate image\n",
    "    encoded_sample = torch.tensor(latent_code).float().unsqueeze(dim=0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.decoder(encoded_sample)\n",
    "    # append image\n",
    "    centroids_imgs.append(generated)     \n",
    "    \n",
    "centroids_df = pd.DataFrame(centroids)\n",
    "centroids_df\n",
    "\n",
    "### Plot the images generated from the centroids \n",
    "#  NB: clusters does not have a particular shape (not hyper-spheric nor convex in general), \n",
    "#      so the obtained centroids could still produce a bad representation of the object  \n",
    "from utilities.plot_tools import plot_img_grid\n",
    "\n",
    "_ = plot_img_grid((2,5), centroids_imgs, to_show=True, axis_off=False, figsize=(10,4.5), \n",
    "                  folder_path = settings.autoencoder.ROOT_DIR,\n",
    "                  titles      = datamodule.get_label_names(),\n",
    "                  filename    = \"centroids_images.pdf\",\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif for path along centroids\n",
    "order = [7,5,9,8,6,2,4,0,3,1]\n",
    "steps = 15\n",
    "pause = 1\n",
    "\n",
    "# sample latent codes on path\n",
    "latent_codes = [centroids[order[0]]]*pause\n",
    "for it in range(10-1):\n",
    "    # path between two centroids\n",
    "    path = np.linspace(centroids[order[it]], centroids[order[it+1]], steps)\n",
    "    \n",
    "    # latent code of intermediate images\n",
    "    for pt in range(steps):\n",
    "        latent_codes.append(path[pt])  #row\n",
    "        \n",
    "    # repeat the centroid code (like a pause)\n",
    "    latent_codes.extend([centroids[order[it+1]]]*pause)\n",
    "    \n",
    "# decode into images\n",
    "path_images = []\n",
    "for code in latent_codes:\n",
    "    # generate image\n",
    "    encoded_sample = torch.tensor(code).float().unsqueeze(dim=0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.decoder(encoded_sample)\n",
    "    # append image\n",
    "    path_images.append(generated)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onward and backward \n",
    "path_images = path_images + path_images[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# resize images\n",
    "bigger_images = [transforms.functional.resize(img=img, size=28*4) for img in path_images]\n",
    "\n",
    "# convert tensors to PIL\n",
    "imgs = [transforms.functional.to_pil_image(img.cpu().squeeze()) for img in bigger_images]\n",
    "\n",
    "# duration is the number of milliseconds between frames\n",
    "save_path = settings.autoencoder.ROOT_DIR + \"/centroids_path.gif\"\n",
    "imgs[0].save(save_path, save_all=True, append_images=imgs[1:], duration=80, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and display the animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "if os.path.isfile(save_path):\n",
    "    gif = HTML(f'<img src=\"{save_path}\">')\n",
    "    \n",
    "gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of learned manifold reduced with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we plot a set of images sampled from the latent space reduced with PCA. We will compare it with the one obtained from the variational model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ranges of latent variables\n",
    "PCA_reduced_df = pd.DataFrame(Latent_analyzer.PCA_reduced_samples)\n",
    "PCA_enc_max = PCA_reduced_df.max()\n",
    "PCA_enc_min = PCA_reduced_df.min()\n",
    "\n",
    "# build a uniform grid for sampling\n",
    "samples_h = 20\n",
    "samples_v = 20\n",
    "x_latent_code = np.linspace(PCA_enc_min[0]*0.6, PCA_enc_max[0]*0.6, samples_h)\n",
    "y_latent_code = np.linspace(PCA_enc_min[1]*0.6, PCA_enc_max[1]*0.6, samples_v)\n",
    "\n",
    "### generate some images\n",
    "images    = []\n",
    "encodings = []\n",
    "for jj in range(samples_v):\n",
    "    for ii in range(samples_h):\n",
    "        # get 2-dim PCA reduced code\n",
    "        PCA_code = np.array([x_latent_code[ii], y_latent_code[-jj-1]])\n",
    "        # inverse transform of code\n",
    "        latent_code = Latent_analyzer.pca.inverse_transform(PCA_code)\n",
    "        # append encoding\n",
    "        encodings.append(latent_code)                                    \n",
    "        # generate image\n",
    "        encoded_sample = torch.tensor(latent_code).float().unsqueeze(dim=0)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            generated = model.decoder(encoded_sample)\n",
    "        # append image\n",
    "        images.append(generated)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_img_grid((samples_h,samples_v), images, to_show=True, axis_off=True, figsize=(12,12), \n",
    "                  folder_path = settings.autoencoder.ROOT_DIR,\n",
    "                  filename    = \"manifold_images_PCA.pdf\",\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"transfer_learning\"></a>\n",
    "# Transfer Learning \n",
    "     \n",
    "[Table of contents](#toc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(seed=settings.MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"finetuning\"></a> \n",
    "## Encoder fine-tuning for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create a simple fully-connected network that takes as input the flattened output of the convolutional part of the encoder trained above and outputs a class label. This approach is equivalent to create a CNN with freezed parameters in the convolutional layers, but it allows us to call the forward pass of the convolutional layers just once for each samples instead that at every epoch, saving a lot of repeated computations and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from autoencoder.symmetric_autoencoder import SymmetricAutoencoder\n",
    "from data_management.fashion_mnist import FashionMNISTDataModule\n",
    "\n",
    "### define datamodule\n",
    "datamodule = FashionMNISTDataModule(data_dir   = settings.DATASETS_DIR, \n",
    "                                    batch_size = 256,\n",
    "                                    Nsamples   = None,\n",
    "                                    valid_frac = 8800./60000.,\n",
    "                                    random_state = settings.MAGIC_NUM,\n",
    "                                   )\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "\n",
    "### load model from checkpoint\n",
    "autoencoder_model = SymmetricAutoencoder.load_from_checkpoint( settings.autoencoder.BEST_MODEL_CKPT_FILE )\n",
    "autoencoder_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check gradient tracking of layers\n",
    "for param_name, param in autoencoder_model.encoder.named_parameters():\n",
    "    ww = 28\n",
    "    print(f\"{param_name: <{ww}} : requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we compute the preprocessed inputs of the train and validation datasets to be passed to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_management.data_tools import DefaultDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# compute encoded representation of train dataset\n",
    "train_encodings = []\n",
    "with torch.no_grad():\n",
    "    for data,target in datamodule.train_dataloader():\n",
    "        encoded = autoencoder_model.encoder.flatten(autoencoder_model.encoder.encoder_cnn(data))\n",
    "        for it in range(len(target)):\n",
    "            train_encodings.append( (encoded[it], target[it]) )\n",
    "\n",
    "train_encodings = DefaultDataset(train_encodings)\n",
    "train_encs = DataLoader(train_encodings, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute encoded representation of valid dataset\n",
    "val_encodings = []\n",
    "with torch.no_grad():\n",
    "    for data,target in datamodule.val_dataloader():\n",
    "        encoded = autoencoder_model.encoder.flatten(autoencoder_model.encoder.encoder_cnn(data))\n",
    "        for it in range(len(target)):\n",
    "            val_encodings.append( (encoded[it], target[it]) )\n",
    "\n",
    "val_encodings = DefaultDataset(val_encodings)\n",
    "val_encs = DataLoader(val_encodings, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the classifier to be trained. It is a fully-connected neural network with 2 linear layers. The first layer is cloned from the first linear block of the encoder, on which training and gradient tracking is activated. The second layer is a simple linear layer that outputs the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder.transfer_learning import EncoderClassifier\n",
    "\n",
    "num_classes = len(datamodule.get_label_names())\n",
    "\n",
    "# compute flatten layer dimension\n",
    "cnn_out = autoencoder_model._compute_shapes()[0]\n",
    "out_dim = np.prod(cnn_out)\n",
    "\n",
    "net = EncoderClassifier(input_dim     = out_dim,\n",
    "                        activation    = \"leaky_relu\",\n",
    "                        linear_config = [autoencoder_model.enc_hp[\"linear_config\"][0]],\n",
    "                        num_classes   = num_classes,\n",
    "                        optimizer     = \"adamax\",\n",
    "                        learning_rate = 1.,      # lr_finder will change it\n",
    "                        L2_penalty    = 0.,\n",
    "                       )\n",
    "\n",
    "# substitute first layer with pretrained one\n",
    "net.classifier[0] = autoencoder_model.encoder.encoder_lin[0]\n",
    "\n",
    "# activate gradient tracking on first (pretrained) layer\n",
    "for param in net.classifier[0].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check gradient tracking of layers\n",
    "for param_name, param in net.named_parameters():\n",
    "    ww = 28\n",
    "    print(f\"{param_name: <{ww}} : requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.train_tools import LossesTracker\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "### callbacks\n",
    "# track the losses (train and valid) during training\n",
    "losses_tracker = LossesTracker()\n",
    "\n",
    "# early stopping\n",
    "early_stop = EarlyStopping(monitor   = \"val_loss\", \n",
    "                           min_delta = 0.0005, \n",
    "                           patience  = 5, \n",
    "                           verbose   = False, \n",
    "                           check_on_train_epoch_end=False, # check early_stop at end of validation epoch\n",
    "                          )\n",
    "\n",
    "# checkpoint the model if the monitored value improved\n",
    "checkpoint = ModelCheckpoint(dirpath  = settings.CHECKPOINT_DIR, \n",
    "                             filename = \"checkpt_transfer_learning_{epoch}_{val_loss:.2f}\", \n",
    "                             monitor  = \"val_loss\",\n",
    "                            )\n",
    "\n",
    "# define trainer\n",
    "trainer = pl.Trainer(logger     = False,\n",
    "                     default_root_dir = settings.transfer_learning.ROOT_DIR,\n",
    "                     max_epochs = 50,\n",
    "                     gpus       = 1 if settings.USE_GPU else None,\n",
    "                     callbacks  = [losses_tracker, early_stop, checkpoint],\n",
    "                     enable_model_summary = False,\n",
    "                     num_sanity_val_steps = 0,     # disable validation sanity check before training\n",
    "                     auto_lr_find = True,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells below we run a `PyTorch-Lightning` utility to estimate a good learning rate. The suggested value is chosen as the point with the steepest descent in a plot *loss* vs *learning rate*. For reference see:\n",
    "[learning_rate_finder](https://pytorch-lightning.readthedocs.io/en/stable/advanced/training_tricks.html#learning-rate-finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(settings.transfer_learning.ROOT_DIR, exist_ok=True)\n",
    "\n",
    "# Run learning rate finder\n",
    "lr_finder = trainer.tune(net, train_dataloaders=train_encs, val_dataloaders=val_encs)[\"lr_find\"]\n",
    "\n",
    "# plot \n",
    "fig = px.scatter(lr_finder.results, \n",
    "                 x=\"lr\", y=\"loss\",\n",
    "                 labels = {\"lr\"   :\"Learning Rate\",\n",
    "                           \"loss\" :\"Loss\",\n",
    "                          },\n",
    "                 title = \"Learning Rate finder\",\n",
    "                 log_x = True,\n",
    "                )\n",
    "if lr_finder._optimal_idx is not None:\n",
    "    index = lr_finder._optimal_idx\n",
    "    fig.add_trace(go.Scatter(x=[lr_finder.results[\"lr\"][index]], \n",
    "                             y=[lr_finder.results[\"loss\"][index]], \n",
    "                             mode = 'markers',\n",
    "                             marker_symbol = 'star',\n",
    "                             marker_size = 15,\n",
    "                             showlegend = False,\n",
    "                             name = \"Suggestion\",\n",
    "                 )          )\n",
    "fig.show()\n",
    "fig.write_image(settings.transfer_learning.ROOT_DIR + \"/lr_finder.pdf\")  \n",
    "\n",
    "print(\"Suggested learning rate: \", net.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Training started at:\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") )\n",
    "\n",
    "trainer.fit(net, \n",
    "            train_dataloaders=train_encs,\n",
    "            val_dataloaders  =val_encs,\n",
    "           )\n",
    "\n",
    "print( \"Training ended at:\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.plot_tools import plot_history\n",
    "\n",
    "os.makedirs(settings.transfer_learning.ROOT_DIR, exist_ok=True)\n",
    "save_path = settings.transfer_learning.ROOT_DIR + \"/train_history.pdf\"\n",
    "\n",
    "plot_history(losses_tracker.train, losses_tracker.valid, ylog=False, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# copy best model checkpoint into the results directory\n",
    "shutil.copy(checkpoint.best_model_path, settings.transfer_learning.BEST_MODEL_CKPT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"finetesting\"></a> \n",
    "## Model testing and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loss and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from autoencoder.transfer_learning import EncoderClassifier\n",
    "from autoencoder.symmetric_autoencoder import SymmetricAutoencoder\n",
    "from data_management.fashion_mnist import FashionMNISTDataModule\n",
    "\n",
    "# define datamodule\n",
    "datamodule = FashionMNISTDataModule(data_dir   = settings.DATASETS_DIR, \n",
    "                                    batch_size = 256,\n",
    "                                    Nsamples   = None,\n",
    "                                    valid_frac = 8800./60000.,\n",
    "                                    random_state = settings.MAGIC_NUM,\n",
    "                                   )\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "\n",
    "# load model from checkpoint\n",
    "autoencoder_model = SymmetricAutoencoder.load_from_checkpoint( settings.autoencoder.BEST_MODEL_CKPT_FILE )\n",
    "autoencoder_model.freeze()\n",
    "\n",
    "net = EncoderClassifier.load_from_checkpoint(settings.transfer_learning.BEST_MODEL_CKPT_FILE)\n",
    "net.freeze()\n",
    "\n",
    "# define trainer\n",
    "trainer = pl.Trainer(logger = False,\n",
    "                     gpus   = 1 if settings.USE_GPU else None,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_management.data_tools import DefaultDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# compute encoded representation of test dataset\n",
    "test_encodings = []\n",
    "with torch.no_grad():\n",
    "    for data,target in datamodule.test_dataloader():\n",
    "        encoded = autoencoder_model.encoder.flatten(autoencoder_model.encoder.encoder_cnn(data))\n",
    "        for it in range(len(target)):\n",
    "            test_encodings.append( (encoded[it], target[it]) )\n",
    "            \n",
    "test_encodings = DefaultDataset(test_encodings)\n",
    "test_encs = DataLoader(test_encodings, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_output = trainer.predict(net, dataloaders=test_encs, return_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "accuracy = torchmetrics.Accuracy()\n",
    "\n",
    "losses  = []\n",
    "labels  = []\n",
    "outputs = []\n",
    "predictions = []\n",
    "accuracies  = []\n",
    "for batch_output in predict_output:\n",
    "    outputs.append(batch_output[\"outputs\"])\n",
    "    labels.append(batch_output[\"labels\"])\n",
    "    \n",
    "    # test losses (cross entropy)\n",
    "    losses.append(batch_output[\"test_loss\"])\n",
    "    \n",
    "    # predictions\n",
    "    preds = batch_output[\"outputs\"].argmax(dim=1, keepdim=True)\n",
    "    predictions.append(preds)\n",
    "    \n",
    "    # accuracy\n",
    "    accs = accuracy(preds.view_as(batch_output[\"labels\"]), batch_output[\"labels\"])\n",
    "    accuracies.append(accs)\n",
    "    \n",
    "\n",
    "final_test_loss = np.mean(losses)\n",
    "final_test_acc  = np.mean(accuracies)\n",
    "print(\"FINAL TEST LOSS VALUE: {}\".format(final_test_loss))\n",
    "print(\"FINAL TEST ACCURACY  : {}\".format(final_test_acc ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.plot_tools import plot_confusion_matrix\n",
    "\n",
    "label_names = datamodule.get_label_names()\n",
    "\n",
    "# plot confusion matrix\n",
    "true_labels = torch.cat(labels)\n",
    "predictions = torch.cat(predictions)\n",
    "plot_confusion_matrix(predictions, \n",
    "                      true_labels, \n",
    "                      label_names, \n",
    "                      figsize   = (8,8), \n",
    "                      save_path = settings.transfer_learning.ROOT_DIR + \"/confusion_mat.pdf\",\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional kernels visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the trained kernels of the encoder in order to compare them with the ones of the supervised classifier in homework 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utilities.plot_tools import plot_img_grid\n",
    "\n",
    "# retrieve convolutional layers\n",
    "ConvLayers = [module for module in autoencoder_model.encoder.modules() if isinstance(module, nn.Conv2d)]\n",
    "print(f\"Model has {len(ConvLayers)} convolutional layers\")\n",
    "\n",
    "# plot filters \n",
    "for it, layer in enumerate(ConvLayers):\n",
    "    # retrieve kernels from layer\n",
    "    kernels = layer.weight.detach().cpu().clone()\n",
    "    \n",
    "    title = f\"Filters of Convolutional layer #{it+1}\"\n",
    "    \n",
    "    if kernels.size(1) != 1:  # if there is more than 1 input channel, select random channel_id\n",
    "        channel_id = np.random.randint(layer.in_channels)\n",
    "        kernels = kernels[:, channel_id].unsqueeze(dim=1)\n",
    "        title = title + f\" - channel #{channel_id}\"\n",
    "        \n",
    "    # normalize to range [0,1] for better visualization\n",
    "    kmin = torch.min(kernels).item()\n",
    "    kmax = torch.max(kernels).item()\n",
    "    kernels = (kernels - kmin)/(kmax - kmin)\n",
    "    \n",
    "    # plot filters\n",
    "    cols = 16\n",
    "    rows = kernels.size(0) // cols \n",
    "    \n",
    "    figsize = (12, 12*rows/(cols-3)+0.4)\n",
    "    \n",
    "    plot_img_grid(grid_shape = (rows,cols), \n",
    "                  images     = kernels, \n",
    "                  titles     = None, \n",
    "                  folder_path= settings.transfer_learning.ROOT_DIR, \n",
    "                  filename   = f\"conv_filters_layer_{it+1}.pdf\", \n",
    "                  to_show    = True, \n",
    "                  figsize    = figsize, \n",
    "                  suptitle   = title, \n",
    "                  cmap       = \"Greys\", \n",
    "                  axis_off   = True,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"denoising_autoencoder\"></a>\n",
    "# Denoising Autoencoder\n",
    "\n",
    "[Table of contents](#toc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"trainingDAE\"></a> \n",
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(seed=settings.MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_management.fashion_mnist import FashionMNISTDataModule\n",
    "\n",
    "datamodule = FashionMNISTDataModule(data_dir   = settings.DATASETS_DIR, \n",
    "                                    batch_size = 256,\n",
    "                                    Nsamples   = None,\n",
    "                                    valid_frac = 8800./60000.,\n",
    "                                    random_state = settings.MAGIC_NUM,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# read best hyper-parameters of the autoencoder model from file (we will use the already optimized \n",
    "#   hyper-parameters as a starting point for the denoising autoencoder)\n",
    "with open(settings.autoencoder.BEST_HYPERS_FILE, \"r\") as file:\n",
    "    best_hypers = json.load(file)\n",
    "\n",
    "# Modifications to hyper-parameters\n",
    "#best_hypers[\"params\"][\"Pdropout\"] = 0.\n",
    "#best_hypers[\"params\"][\"instance_norm\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.train_tools import LossesTracker\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from utilities.encoder_tools import ImageReconstruction\n",
    "\n",
    "### callbacks\n",
    "# track the losses (train and valid) during training\n",
    "losses_tracker = LossesTracker()\n",
    "\n",
    "# early stopping\n",
    "early_stop = EarlyStopping(monitor   = \"val_loss\", \n",
    "                           min_delta = 0.0001, \n",
    "                           patience  = 10, \n",
    "                           verbose   = False, \n",
    "                           check_on_train_epoch_end=False, # check early_stop at end of training epoch\n",
    "                          )\n",
    "\n",
    "# checkpoint the model if the monitored value improved\n",
    "checkpoint = ModelCheckpoint(dirpath  = settings.CHECKPOINT_DIR, \n",
    "                             filename = \"checkpt_denoising_{epoch}_{val_loss:.2f}\", \n",
    "                             monitor  = \"val_loss\",\n",
    "                            )\n",
    "\n",
    "# reconstruction of a test sample at every epoch\n",
    "sample_id = 34    # bag\n",
    "dataset = torchvision.datasets.FashionMNIST(settings.DATASETS_DIR, \n",
    "                                            train    = False, \n",
    "                                            download = True,\n",
    "                                           )\n",
    "sample = transforms.functional.to_tensor(dataset[sample_id][0]).unsqueeze(dim=0)\n",
    "if settings.USE_GPU:\n",
    "    sample = sample.to(\"cuda\")\n",
    "\n",
    "rec_callback = ImageReconstruction(sample, \n",
    "                                   to_show   = False, \n",
    "                                   save_path = settings.denoisingAE.RECONSTRUCTIONS_DIR,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define transformation to corrupt the input images\n",
    "from data_management.data_tools import AddGaussianNoise\n",
    "\n",
    "corruption_p = 0.5   # probability of a transform to be applied\n",
    "\n",
    "# sequence of transformation\n",
    "corruption = transforms.Compose([transforms.RandomHorizontalFlip( p=corruption_p ),\n",
    "                                 transforms.RandomVerticalFlip( p=corruption_p ),\n",
    "                                 AddGaussianNoise( p=corruption_p, mean=0, std=0.4 ),     # custom\n",
    "                                 transforms.RandomErasing( p=corruption_p ),\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder.denoising_autoencoder import DenoisingAutoencoder\n",
    "\n",
    "### define model and hyper-parameters\n",
    "model = DenoisingAutoencoder(input_size    = datamodule.get_sample_size(),\n",
    "                             params        = best_hypers[\"params\"],\n",
    "                             optimizer     = best_hypers[\"optimizer\"],\n",
    "                             learning_rate = best_hypers[\"learning_rate\"],\n",
    "                             L2_penalty    = best_hypers[\"L2_penalty\"],\n",
    "                             momentum      = best_hypers[\"momentum\"],\n",
    "                             corruption    = corruption,\n",
    "                            )\n",
    "### define trainer\n",
    "trainer = pl.Trainer(logger     = False,\n",
    "                     max_epochs = 200,\n",
    "                     gpus       = 1 if settings.USE_GPU else None,\n",
    "                     callbacks  = [losses_tracker, early_stop, checkpoint, rec_callback],\n",
    "                     val_check_interval   = 1.,\n",
    "                     enable_model_summary = False,\n",
    "                     num_sanity_val_steps = 0,     # disable validation sanity check before training\n",
    "                     auto_lr_find = True,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(settings.denoisingAE.ROOT_DIR, exist_ok=True)\n",
    "\n",
    "# Run learning rate finder\n",
    "lr_finder = trainer.tune(model, datamodule=datamodule)[\"lr_find\"]\n",
    "\n",
    "# plot \n",
    "fig = px.scatter(lr_finder.results, \n",
    "                 x=\"lr\", y=\"loss\",\n",
    "                 labels = {\"lr\"   :\"Learning Rate\",\n",
    "                           \"loss\" :\"Loss\",\n",
    "                          },\n",
    "                 title = \"Learning Rate finder\",\n",
    "                 log_x = True,\n",
    "                )\n",
    "if lr_finder._optimal_idx is not None:\n",
    "    index = lr_finder._optimal_idx\n",
    "    fig.add_trace(go.Scatter(x=[lr_finder.results[\"lr\"][index]], \n",
    "                             y=[lr_finder.results[\"loss\"][index]], \n",
    "                             mode = 'markers',\n",
    "                             marker_symbol = 'star',\n",
    "                             marker_size = 15,\n",
    "                             showlegend = False,\n",
    "                             name = \"Suggestion\",\n",
    "                 )          )\n",
    "fig.show()\n",
    "fig.write_image(settings.denoisingAE.ROOT_DIR + \"/lr_finder.pdf\")  \n",
    "\n",
    "print(\"Suggested learning rate: \", model.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Training started at:\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") )\n",
    "fit_begin = time.time()  # measure running time\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule) # run the training\n",
    "\n",
    "fit_time = time.time() - fit_begin\n",
    "print( \"Training ended at:\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") )\n",
    "print(f\"Fit time:\", str(datetime.timedelta(seconds=fit_time)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.plot_tools import plot_history\n",
    "\n",
    "save_path = settings.denoisingAE.ROOT_DIR + \"/train_history.pdf\"\n",
    "\n",
    "plot_history(losses_tracker.train, losses_tracker.valid, ylog=True, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# copy best model checkpoint into the results directory\n",
    "shutil.copy(checkpoint.best_model_path, settings.denoisingAE.BEST_MODEL_CKPT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"testingDAE\"></a> \n",
    "## Model testing and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(seed=settings.MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from data_management.fashion_mnist import FashionMNISTDataModule\n",
    "from autoencoder.denoising_autoencoder import DenoisingAutoencoder\n",
    "\n",
    "### define datamodule\n",
    "datamodule = FashionMNISTDataModule(data_dir   = settings.DATASETS_DIR, \n",
    "                                    batch_size = 256,\n",
    "                                    Nsamples   = None,\n",
    "                                    valid_frac = 8800./60000.,\n",
    "                                    random_state = settings.MAGIC_NUM,\n",
    "                                   )\n",
    "### load model from checkpoint\n",
    "model = DenoisingAutoencoder.load_from_checkpoint( settings.denoisingAE.BEST_MODEL_CKPT_FILE )\n",
    "model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.encoder_tools import EncodedRepresentation\n",
    "\n",
    "# callback to save the encoded representations of the test samples\n",
    "encoded_test_repr = EncodedRepresentation()\n",
    "\n",
    "trainer = pl.Trainer(logger     = False,\n",
    "                     gpus       = 1 if settings.USE_GPU else None,\n",
    "                     callbacks  = [encoded_test_repr],\n",
    "                    )\n",
    "\n",
    "result = trainer.test(model, datamodule=datamodule, verbose=False)\n",
    "test_loss = result[0][\"test_loss\"]\n",
    "print(\"TEST LOSS: \", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Space exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.encoder_tools import LatentSpaceAnalyzer\n",
    "\n",
    "# test dataset encoded samples\n",
    "Latent_analyzer = LatentSpaceAnalyzer(encoded_test_repr.encoded_samples,\n",
    "                                      encoded_test_repr.labels,\n",
    "                                      label_names = datamodule.get_label_names(),\n",
    "                                      save_path = settings.denoisingAE.ROOT_DIR,\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_analyzer.PCA_reduce(n_components=2, filename=\"PCA_reduced_space.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_analyzer.TSNE_reduce(n_components=2, perplexity=80, filename=\"TSNE_reduced_space.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_analyzer.Isomap_reduce(n_components=2, filename=\"Isomap_reduced_space.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generation from latent codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings_df = pd.DataFrame(encoded_test_repr.encoded_samples)\n",
    "test_enc_max = max(test_encodings_df.max())\n",
    "test_enc_min = min(test_encodings_df.min())\n",
    "\n",
    "print(f\"Range of test dataset encoded values: [{test_enc_min}, {test_enc_max}]\")\n",
    "\n",
    "# latent dimension\n",
    "latent_dim = model.enc_hp[\"latent_space_dim\"]\n",
    "\n",
    "### generate some examples of images from random latent codes\n",
    "examples  = 20\n",
    "images    = []\n",
    "encodings = []\n",
    "for ii in range(examples):\n",
    "    # randomly sample from latent space\n",
    "    latent_code = np.random.uniform(test_enc_min, test_enc_max, latent_dim)\n",
    "    # append encoding\n",
    "    encodings.append(latent_code)                                    \n",
    "    # generate image\n",
    "    encoded_sample = torch.tensor(latent_code).float().unsqueeze(dim=0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.decoder(encoded_sample)\n",
    "    # append image\n",
    "    images.append(generated)     \n",
    "    \n",
    "encodings_df = pd.DataFrame(encodings)\n",
    "encodings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.plot_tools import plot_img_grid\n",
    "\n",
    "_ = plot_img_grid((4,5), images, to_show=True, axis_off=False, figsize=(10,8), \n",
    "                  folder_path = settings.denoisingAE.ROOT_DIR,\n",
    "                  filename    = \"generated_images.pdf\",\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of learned manifold reduced with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ranges of latent variables\n",
    "PCA_reduced_df = pd.DataFrame(Latent_analyzer.PCA_reduced_samples)\n",
    "PCA_enc_max = PCA_reduced_df.max()\n",
    "PCA_enc_min = PCA_reduced_df.min()\n",
    "\n",
    "# build a uniform grid for sampling\n",
    "samples_h = 20\n",
    "samples_v = 20\n",
    "x_latent_code = np.linspace(PCA_enc_min[0]*0.6, PCA_enc_max[0]*0.6, samples_h)\n",
    "y_latent_code = np.linspace(PCA_enc_min[1]*0.6, PCA_enc_max[1]*0.6, samples_v)\n",
    "\n",
    "### generate some images\n",
    "images    = []\n",
    "encodings = []\n",
    "for jj in range(samples_v):\n",
    "    for ii in range(samples_h):\n",
    "        # get 2-dim PCA reduced code\n",
    "        PCA_code = np.array([x_latent_code[ii], y_latent_code[-jj-1]])\n",
    "        # inverse transform of code\n",
    "        latent_code = Latent_analyzer.pca.inverse_transform(PCA_code)\n",
    "        # append encoding\n",
    "        encodings.append(latent_code)                                    \n",
    "        # generate image\n",
    "        encoded_sample = torch.tensor(latent_code).float().unsqueeze(dim=0)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            generated = model.decoder(encoded_sample)\n",
    "        # append image\n",
    "        images.append(generated)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_img_grid((samples_h,samples_v), images, to_show=True, axis_off=True, figsize=(12,12), \n",
    "                  folder_path = settings.denoisingAE.ROOT_DIR,\n",
    "                  filename    = \"manifold_images_PCA.pdf\",\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space path along centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute centroids of clusters for each label and generate images\n",
    "\n",
    "# generate some examples of images from centroids of latent space clusters\n",
    "examples = len(datamodule.get_label_names())\n",
    "centroids_imgs = []\n",
    "centroids      = []\n",
    "for ii in range(examples):\n",
    "    # compute centroid for i-esim label    \n",
    "    mask = [(ll == ii) for ll in encoded_test_repr.labels]\n",
    "    filtered_samples = np.array(encoded_test_repr.encoded_samples)[mask] \n",
    "    latent_code = np.mean(filtered_samples, axis=0)    \n",
    "    # append encoding\n",
    "    centroids.append(latent_code)                                    \n",
    "    # generate image\n",
    "    encoded_sample = torch.tensor(latent_code).float().unsqueeze(dim=0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.decoder(encoded_sample)\n",
    "    # append image\n",
    "    centroids_imgs.append(generated)     \n",
    "    \n",
    "centroids_df = pd.DataFrame(centroids)\n",
    "centroids_df\n",
    "\n",
    "### Plot the images generated from the centroids \n",
    "from utilities.plot_tools import plot_img_grid\n",
    "\n",
    "_ = plot_img_grid((2,5), centroids_imgs, to_show=True, axis_off=False, figsize=(10,4.5), \n",
    "                  folder_path = settings.denoisingAE.ROOT_DIR,\n",
    "                  titles      = datamodule.get_label_names(),\n",
    "                  filename    = \"centroids_images.pdf\",\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif for path along centroids\n",
    "order = [7,5,9,8,6,2,4,0,3,1]\n",
    "steps = 15\n",
    "pause = 1\n",
    "\n",
    "# sample latent codes on path\n",
    "latent_codes = [centroids[order[0]]]*pause\n",
    "for it in range(10-1):\n",
    "    # path between two centroids\n",
    "    path = np.linspace(centroids[order[it]], centroids[order[it+1]], steps)\n",
    "    \n",
    "    # latent code of intermediate images\n",
    "    for pt in range(steps):\n",
    "        latent_codes.append(path[pt])  #row\n",
    "        \n",
    "    # repeat the centroid code (like a pause)\n",
    "    latent_codes.extend([centroids[order[it+1]]]*pause)\n",
    "    \n",
    "# decode into images\n",
    "path_images = []\n",
    "for code in latent_codes:\n",
    "    # generate image\n",
    "    encoded_sample = torch.tensor(code).float().unsqueeze(dim=0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.decoder(encoded_sample)\n",
    "    # append image\n",
    "    path_images.append(generated)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onward and backward \n",
    "path_images = path_images + path_images[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# resize images\n",
    "bigger_images = [transforms.functional.resize(img=img, size=28*4) for img in path_images]\n",
    "\n",
    "# convert tensors to PIL\n",
    "imgs = [transforms.functional.to_pil_image(img.cpu().squeeze()) for img in bigger_images]\n",
    "\n",
    "# duration is the number of milliseconds between frames\n",
    "save_path = settings.denoisingAE.ROOT_DIR + \"/centroids_path.gif\"\n",
    "imgs[0].save(save_path, save_all=True, append_images=imgs[1:], duration=80, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "if os.path.isfile(save_path):\n",
    "    gif = HTML(f'<img src=\"{save_path}\">')\n",
    "    \n",
    "gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising capability test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define transformation to corrupt the input images\n",
    "from data_management.data_tools import AddGaussianNoise\n",
    "\n",
    "corruption_p = 0.5   # probability of a transform to be applied\n",
    "\n",
    "# sequence of transformation\n",
    "corruption = transforms.Compose([transforms.RandomHorizontalFlip( p=corruption_p ),\n",
    "                                 transforms.RandomVerticalFlip( p=corruption_p ),\n",
    "                                 AddGaussianNoise( p=corruption_p, mean=0, std=0.4 ),     # custom\n",
    "                                 transforms.RandomErasing( p=corruption_p ),\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.FashionMNIST(settings.DATASETS_DIR, \n",
    "                                            train    = False, \n",
    "                                            download = True,\n",
    "                                           )\n",
    "# select samples at random\n",
    "Nsamples   = 8\n",
    "sample_ids = np.random.randint(0, 10000, Nsamples)\n",
    "\n",
    "images = []\n",
    "titles = []\n",
    "for idx in sample_ids:\n",
    "    sample = transforms.functional.to_tensor(dataset[idx][0]).unsqueeze(dim=0)\n",
    "    if settings.USE_GPU:\n",
    "        sample = sample.to(\"cuda\")                           \n",
    "    # apply corruption\n",
    "    noisy = corruption(sample)\n",
    "    # reconstruct image\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model(noisy)\n",
    "    # store results\n",
    "    images += [sample, noisy, generated]\n",
    "    titles += [f\"original (id: {idx})\", \"corrupted\", \"reconstr.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.plot_tools import plot_img_grid\n",
    "\n",
    "_ = plot_img_grid((4,6), images, to_show=True, axis_off=True, figsize=(10,8), \n",
    "                  titles      = titles,\n",
    "                  folder_path = settings.denoisingAE.ROOT_DIR,\n",
    "                  filename    = \"denoising_images.pdf\",\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize convolutional layers kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve convolutional layers\n",
    "ConvLayers = [module for module in model.encoder.modules() if isinstance(module, nn.Conv2d)]\n",
    "print(f\"Model has {len(ConvLayers)} convolutional layers\")\n",
    "\n",
    "# plot filters \n",
    "for it, layer in enumerate(ConvLayers):\n",
    "    # retrieve kernels from layer\n",
    "    kernels = layer.weight.detach().cpu().clone()\n",
    "    \n",
    "    title = f\"Filters of Convolutional layer #{it+1}\"\n",
    "    \n",
    "    if kernels.size(1) != 1:  # if there is more than 1 input channel, select random channel_id\n",
    "        channel_id = np.random.randint(layer.in_channels)\n",
    "        kernels = kernels[:, channel_id].unsqueeze(dim=1)\n",
    "        title = title + f\" - channel #{channel_id}\"\n",
    "        \n",
    "    # normalize to range [0,1] for better visualization\n",
    "    kmin = torch.min(kernels).item()\n",
    "    kmax = torch.max(kernels).item()\n",
    "    kernels = (kernels - kmin)/(kmax - kmin)\n",
    "    \n",
    "    # plot filters\n",
    "    cols = 16\n",
    "    rows = kernels.size(0) // cols \n",
    "    \n",
    "    figsize = (12, 12*rows/(cols-3)+0.4)\n",
    "    \n",
    "    plot_img_grid(grid_shape = (rows,cols), \n",
    "                  images     = kernels, \n",
    "                  titles     = None, \n",
    "                  folder_path= settings.denoisingAE.ROOT_DIR, \n",
    "                  filename   = f\"conv_filters_layer_{it+1}.pdf\", \n",
    "                  to_show    = True, \n",
    "                  figsize    = figsize, \n",
    "                  suptitle   = title, \n",
    "                  cmap       = \"Greys\", \n",
    "                  axis_off   = True,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"VAE\"></a>\n",
    "# Variational Autoencoder ($\\beta$-VAE)\n",
    "     \n",
    "[Table of contents](#toc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new model architecture we aim at a more disentangled representation of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"trainingVAE\"></a> \n",
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(seed=settings.MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# read best hyper-parameters from file\n",
    "with open(settings.autoencoder.BEST_HYPERS_FILE, \"r\") as file:\n",
    "    best_hypers = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the beta parameter\n",
    "best_hypers[\"params\"][\"beta\"] = 2.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_management.fashion_mnist import FashionMNISTDataModule\n",
    "\n",
    "### define datamodule\n",
    "datamodule = FashionMNISTDataModule(data_dir   = settings.DATASETS_DIR, \n",
    "                                    batch_size = 256,\n",
    "                                    Nsamples   = None,\n",
    "                                    valid_frac = 8800./60000.,\n",
    "                                    random_state = settings.MAGIC_NUM,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below a summary of the model to train is showed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from autoencoder.variational_autoencoder import VariationalAutoencoder\n",
    "\n",
    "# print model summary\n",
    "shape = datamodule.get_sample_size()\n",
    "net   = VariationalAutoencoder(shape, \n",
    "                               params        = best_hypers[\"params\"],\n",
    "                               optimizer     = best_hypers[\"optimizer\"],\n",
    "                               learning_rate = best_hypers[\"learning_rate\"],\n",
    "                               L2_penalty    = best_hypers[\"L2_penalty\"],\n",
    "                               momentum      = best_hypers[\"momentum\"],\n",
    "                              )\n",
    "# build dummy data batch \n",
    "dummy_batch = list(shape)\n",
    "dummy_batch.insert(0, 256)\n",
    "\n",
    "# print summary\n",
    "torchinfo.summary(net, \n",
    "                  dummy_batch, \n",
    "                  col_width = 20, \n",
    "                  col_names = (\"output_size\",\"num_params\",\"mult_adds\",),  #\"input_size\",\n",
    "                  depth     = 4, \n",
    "                  row_settings = (\"var_names\",),\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we define some callbacks that will be useful during and after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.train_tools import LossesTracker\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from utilities.encoder_tools import ImageReconstruction\n",
    "\n",
    "### callbacks\n",
    "# track the losses (train and valid) during training\n",
    "losses_tracker = LossesTracker()\n",
    "\n",
    "# early stopping\n",
    "early_stop = EarlyStopping(monitor   = \"val_loss\", \n",
    "                           min_delta = 0.0001, \n",
    "                           patience  = 10, \n",
    "                           verbose   = False, \n",
    "                           check_on_train_epoch_end=False, # check early_stop at end of training epoch\n",
    "                          )\n",
    "\n",
    "# checkpoint the model if the monitored value improved\n",
    "checkpoint = ModelCheckpoint(dirpath  = settings.CHECKPOINT_DIR, \n",
    "                             filename = \"checkpt_variational_{epoch}_{val_loss:.2f}\", \n",
    "                             monitor  = \"val_loss\",\n",
    "                            )\n",
    "\n",
    "# reconstruction of a test sample at every epoch\n",
    "sample_id = 34    # bag\n",
    "dataset = torchvision.datasets.FashionMNIST(settings.DATASETS_DIR, \n",
    "                                            train    = False, \n",
    "                                            download = True,\n",
    "                                           )\n",
    "sample = transforms.functional.to_tensor(dataset[sample_id][0]).unsqueeze(dim=0)\n",
    "if settings.USE_GPU:\n",
    "    sample = sample.to(\"cuda\")\n",
    "\n",
    "rec_callback = ImageReconstruction(sample, \n",
    "                                   to_show   = False, \n",
    "                                   save_path = settings.variationalAE.RECONSTRUCTIONS_DIR,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define model and hyper-parameters\n",
    "model = VariationalAutoencoder(input_size    = datamodule.get_sample_size(),\n",
    "                               params        = best_hypers[\"params\"],\n",
    "                               optimizer     = best_hypers[\"optimizer\"],\n",
    "                               learning_rate = best_hypers[\"learning_rate\"],\n",
    "                               L2_penalty    = best_hypers[\"L2_penalty\"],\n",
    "                               momentum      = best_hypers[\"momentum\"],\n",
    "                              )\n",
    "\n",
    "### define trainer\n",
    "trainer = pl.Trainer(logger     = False,\n",
    "                     max_epochs = 200,\n",
    "                     gpus       = 1 if settings.USE_GPU else None,\n",
    "                     callbacks  = [losses_tracker, early_stop, checkpoint, rec_callback],\n",
    "                     val_check_interval   = 1.,\n",
    "                     enable_model_summary = False,\n",
    "                     num_sanity_val_steps = 0,     # disable validation sanity check before training\n",
    "                     auto_lr_find = False,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( \"Training started at:\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") )\n",
    "fit_begin = time.time()  # measure running time\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule) # run the training\n",
    "\n",
    "fit_time = time.time() - fit_begin\n",
    "print( \"Training ended at:\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") )\n",
    "print(f\"Fit time:\", str(datetime.timedelta(seconds=fit_time)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.plot_tools import plot_history\n",
    "\n",
    "save_path = settings.variationalAE.ROOT_DIR + \"/train_history.pdf\"\n",
    "\n",
    "plot_history(losses_tracker.train, losses_tracker.valid, ylog=True, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# copy best model checkpoint into the results directory\n",
    "shutil.copy(checkpoint.best_model_path, settings.variationalAE.BEST_MODEL_CKPT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"testingVAE\"></a> \n",
    "## Model testing and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(seed=settings.MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from data_management.fashion_mnist import FashionMNISTDataModule\n",
    "from autoencoder.variational_autoencoder import VariationalAutoencoder\n",
    "\n",
    "### define datamodule\n",
    "datamodule = FashionMNISTDataModule(data_dir   = settings.DATASETS_DIR, \n",
    "                                    batch_size = 256,\n",
    "                                    Nsamples   = None,\n",
    "                                    valid_frac = 8800./60000.,\n",
    "                                    random_state = settings.MAGIC_NUM,\n",
    "                                   )\n",
    "\n",
    "### load model from checkpoint\n",
    "model = VariationalAutoencoder.load_from_checkpoint( settings.variationalAE.BEST_MODEL_CKPT_FILE )\n",
    "model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.encoder_tools import EncodedRepresentation\n",
    "\n",
    "# callback to save the encoded representations of the test samples\n",
    "encoded_test_repr = EncodedRepresentation()\n",
    "\n",
    "trainer = pl.Trainer(logger     = False,\n",
    "                     gpus       = 1 if settings.USE_GPU else None,\n",
    "                     callbacks  = [encoded_test_repr],\n",
    "                    )\n",
    "\n",
    "result = trainer.test(model, datamodule=datamodule, verbose=False)\n",
    "test_loss = result[0][\"test_loss\"]\n",
    "print(\"TEST LOSS: \", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Space exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.encoder_tools import LatentSpaceAnalyzer\n",
    "\n",
    "# test dataset encoded samples\n",
    "Latent_analyzer = LatentSpaceAnalyzer(encoded_test_repr.encoded_samples,\n",
    "                                      encoded_test_repr.labels,\n",
    "                                      label_names = datamodule.get_label_names(),\n",
    "                                      save_path = settings.variationalAE.ROOT_DIR,\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_analyzer.PCA_reduce(n_components=2, filename=\"PCA_reduced_space.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_analyzer.TSNE_reduce(n_components=2, perplexity=50, filename=\"TSNE_reduced_space.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_analyzer.Isomap_reduce(n_components=2, filename=\"Isomap_reduced_space.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generation from latent codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings_df = pd.DataFrame(encoded_test_repr.encoded_samples)\n",
    "test_enc_max = max(test_encodings_df.max())\n",
    "test_enc_min = min(test_encodings_df.min())\n",
    "\n",
    "print(f\"Range of test dataset encoded values: [{test_enc_min}, {test_enc_max}]\")\n",
    "\n",
    "# latent dimension\n",
    "latent_dim = model.enc_hp[\"latent_space_dim\"]\n",
    "\n",
    "### generate some examples of images from random latent codes\n",
    "examples  = 20\n",
    "images    = []\n",
    "encodings = []\n",
    "for ii in range(examples):\n",
    "    # randomly sample from latent space\n",
    "    latent_code = np.random.uniform(test_enc_min*0.6, test_enc_max*0.6, latent_dim)\n",
    "    # append encoding\n",
    "    encodings.append(latent_code)                                    \n",
    "    # generate image\n",
    "    encoded_sample = torch.tensor(latent_code).float().unsqueeze(dim=0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.decoder(encoded_sample)\n",
    "    # append image\n",
    "    images.append(generated)     \n",
    "    \n",
    "encodings_df = pd.DataFrame(encodings)\n",
    "encodings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.plot_tools import plot_img_grid\n",
    "\n",
    "_ = plot_img_grid((4,5), images, to_show=True, axis_off=False, figsize=(10,8), \n",
    "                  folder_path = settings.variationalAE.ROOT_DIR,\n",
    "                  filename    = \"generated_images.pdf\",\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlap sampled points to the PCA plot by applying to them the same transformation\n",
    "Latent_analyzer.PCA_overlap_points(encodings, to_show=True, filename=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of learned manifold reduced with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ranges of latent variables\n",
    "PCA_reduced_df = pd.DataFrame(Latent_analyzer.PCA_reduced_samples)\n",
    "PCA_enc_max = PCA_reduced_df.max()\n",
    "PCA_enc_min = PCA_reduced_df.min()\n",
    "\n",
    "# build a uniform grid for sampling\n",
    "samples_h = 20\n",
    "samples_v = 20\n",
    "x_latent_code = np.linspace(PCA_enc_min[0]*0.6, PCA_enc_max[0]*0.6, samples_h)\n",
    "y_latent_code = np.linspace(PCA_enc_min[1]*0.6, PCA_enc_max[1]*0.6, samples_v)\n",
    "\n",
    "### generate some images\n",
    "images    = []\n",
    "encodings = []\n",
    "for jj in range(samples_v):\n",
    "    for ii in range(samples_h):\n",
    "        # get 2-dim PCA reduced code\n",
    "        PCA_code = np.array([x_latent_code[ii], y_latent_code[-jj-1]])\n",
    "        # inverse transform of code\n",
    "        latent_code = Latent_analyzer.pca.inverse_transform(PCA_code)\n",
    "        # append encoding\n",
    "        encodings.append(latent_code)                                    \n",
    "        # generate image\n",
    "        encoded_sample = torch.tensor(latent_code).float().unsqueeze(dim=0)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            generated = model.decoder(encoded_sample)\n",
    "        # append image\n",
    "        images.append(generated)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_img_grid((samples_h,samples_v), images, to_show=True, axis_off=True, figsize=(12,12), \n",
    "                  folder_path = settings.variationalAE.ROOT_DIR,\n",
    "                  filename    = \"manifold_images_PCA.pdf\",\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space path along centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute centroids of clusters for each label and generate images\n",
    "\n",
    "# generate some examples of images from centroids of latent space clusters\n",
    "examples = len(datamodule.get_label_names())\n",
    "centroids_imgs = []\n",
    "centroids      = []\n",
    "for ii in range(examples):\n",
    "    # compute centroid for i-esim label    \n",
    "    mask = [(ll == ii) for ll in encoded_test_repr.labels]\n",
    "    filtered_samples = np.array(encoded_test_repr.encoded_samples)[mask] \n",
    "    latent_code = np.mean(filtered_samples, axis=0)    \n",
    "    # append encoding\n",
    "    centroids.append(latent_code)                                    \n",
    "    # generate image\n",
    "    encoded_sample = torch.tensor(latent_code).float().unsqueeze(dim=0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.decoder(encoded_sample)\n",
    "    # append image\n",
    "    centroids_imgs.append(generated)     \n",
    "    \n",
    "centroids_df = pd.DataFrame(centroids)\n",
    "centroids_df\n",
    "\n",
    "### Plot the images generated from the centroids \n",
    "from utilities.plot_tools import plot_img_grid\n",
    "\n",
    "_ = plot_img_grid((2,5), centroids_imgs, to_show=True, axis_off=False, figsize=(10,4.5), \n",
    "                  folder_path = settings.variationalAE.ROOT_DIR,\n",
    "                  titles      = datamodule.get_label_names(),\n",
    "                  filename    = \"centroids_images.pdf\",\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif for path along centroids\n",
    "order = [7,5,9,8,6,2,4,0,3,1]\n",
    "steps = 15\n",
    "pause = 1\n",
    "\n",
    "# sample latent codes on path\n",
    "latent_codes = [centroids[order[0]]]*pause\n",
    "for it in range(10-1):\n",
    "    # path between two centroids\n",
    "    path = np.linspace(centroids[order[it]], centroids[order[it+1]], steps)\n",
    "    \n",
    "    # latent code of intermediate images\n",
    "    for pt in range(steps):\n",
    "        latent_codes.append(path[pt])  #row\n",
    "        \n",
    "    # repeat the centroid code (like a pause)\n",
    "    latent_codes.extend([centroids[order[it+1]]]*pause)\n",
    "    \n",
    "# decode into images\n",
    "path_images = []\n",
    "for code in latent_codes:\n",
    "    # generate image\n",
    "    encoded_sample = torch.tensor(code).float().unsqueeze(dim=0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.decoder(encoded_sample)\n",
    "    # append image\n",
    "    path_images.append(generated)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onward and backward \n",
    "path_images = path_images + path_images[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# resize images\n",
    "bigger_images = [transforms.functional.resize(img=img, size=28*4) for img in path_images]\n",
    "\n",
    "# convert tensors to PIL\n",
    "imgs = [transforms.functional.to_pil_image(img.cpu().squeeze()) for img in bigger_images]\n",
    "\n",
    "# duration is the number of milliseconds between frames\n",
    "save_path = settings.variationalAE.ROOT_DIR + \"/centroids_path.gif\"\n",
    "imgs[0].save(save_path, save_all=True, append_images=imgs[1:], duration=80, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "if os.path.isfile(save_path):\n",
    "    gif = HTML(f'<img src=\"{save_path}\">')\n",
    "    \n",
    "gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

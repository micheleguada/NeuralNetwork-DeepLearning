{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "---\n",
    "A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "---\n",
    "\n",
    "\n",
    "# Homework 3 - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Michele Guadagnini - Mt.1230663"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: CartPole-v1 with pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADDITIONAL LIBRARIES THAT NEED INSTALLATION (uncomment if needed)\n",
    "\n",
    "#!pip install gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "import matplotlib\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import deque\n",
    "import gym\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "### 'seed_everything(seed)' internally calls the followings:\n",
    "#    random.seed(seed)\n",
    "#    np.random.seed(seed)\n",
    "#    torch.manual_seed(seed)\n",
    "#    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# reduce verbosity\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
    "\n",
    "MAGIC_NUM = 23   #seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to plot some statistics about the training\n",
    "def plot_results(results, x_label=\"Episode\", figsize=(6,4), avg_window=None, show=False, savepath=None):\n",
    "    \n",
    "    keys   = list(results.keys())\n",
    "    Nplots = len(keys)\n",
    "    full_fig_size = (figsize[0]*Nplots, figsize[1])\n",
    "\n",
    "    fig, axs = plt.subplots(1, Nplots, figsize=full_fig_size)\n",
    "\n",
    "    for idx, ax in enumerate(axs):\n",
    "        if keys[idx] == \"Temperature\":\n",
    "            ax.plot(results[keys[idx]], label=\"Temperature profile\", color=\"blue\")\n",
    "\n",
    "        if keys[idx] != \"Temperature\" and avg_window is not None:\n",
    "            ax.plot(results[keys[idx]], label=keys[idx], color=\"lightblue\")\n",
    "\n",
    "            # compute and plot moving average of score\n",
    "            moving_avg = np.convolve(results[keys[idx]], np.ones(avg_window), 'valid') / avg_window\n",
    "\n",
    "            x_space = np.arange(avg_window/2,len(moving_avg)+avg_window/2)\n",
    "            ax.plot(x_space, moving_avg, label=keys[idx]+\" (smoothed)\", lw=2, color=\"blue\")\n",
    "\n",
    "        ax.grid()\n",
    "        ax.set_xlabel(x_label)\n",
    "        ax.set_ylabel(keys[idx])\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()   \n",
    "\n",
    "    if savepath is not None:\n",
    "        #save picture\n",
    "        plt.savefig(savepath)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the classes implemented to solve this task\n",
    "from cartpolefrompixels.agent import DQNAgent\n",
    "from cartpolefrompixels.callbacks import RLResults, MaxEpisodesStop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"top-shortcuts\"></a>\n",
    "## Table of contents:\n",
    "\n",
    "1. [**Train the agent**](#Train-the-agent)\n",
    "1. [**Test the trained agent**](#Test-the-trained-agent)\n",
    "1. [**Assistance of a pretrained policy**](#Assistance-of-a-pretrained-policy)\n",
    "    1. [*Train the agent with assistance*](#Train-the-agent-with-assistance)\n",
    "    1. [*Test the agent trained with assistance*](#Test-the-agent-trained-with-assistance)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent\n",
    "[Table of contents](#top-shortcuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the agent and initialize training environment\n",
    "# network hyper-parameters\n",
    "policy_params = {\"conv_channels\" : [16,32],\n",
    "                 \"linear_units\"  : [512,128],\n",
    "                 \"activation\"    : \"relu\",\n",
    "                 \"batch_norm\"    : True,\n",
    "                 \"dropout\"       : 0.,\n",
    "                 \"conv_config\"   : [[8,4,0],  \n",
    "                                    [4,2,0],\n",
    "                                   ],\n",
    "                }\n",
    "# exploration profile parameters\n",
    "behaviour_params = {\"initial_temperature\"    : 4.,\n",
    "                    \"decay_const_in_interval\": 8 ,\n",
    "                   }\n",
    "\n",
    "# other hyper-parameters\n",
    "penalty_type     = \"none\"   # \"state\"  or  \"pixels\" or \"none\"\n",
    "N_episodes       = 2000\n",
    "mem_capacity     = 15360\n",
    "target_sync_rate = 200      # number of steps between target net updates\n",
    "batch_size       = 128\n",
    "learning_rate    = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the agent\n",
    "agent = DQNAgent(env_name         = \"CartPole-v1\", \n",
    "                 N_episodes       = N_episodes,\n",
    "                 mem_capacity     = mem_capacity,\n",
    "                 policy_params    = policy_params,\n",
    "                 behaviour_params = behaviour_params,\n",
    "                 target_sync_rate = target_sync_rate,   # number of steps between target net updates\n",
    "                 batch_size       = batch_size,\n",
    "                 gamma            = 0.97,\n",
    "                 optimizer        = \"sgd\",   \n",
    "                 learning_rate    = learning_rate, \n",
    "                 L2_penalty       = 0.,\n",
    "                 momentum         = 0.,\n",
    "                 seed             = MAGIC_NUM,\n",
    "                 penalty_type     = penalty_type,    # \"state\"  or  \"pixels\" or \"none\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill memory with initial random steps\n",
    "warm_up_steps = 5120\n",
    "agent.fill_memory(warm_up_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup trainer and callbacks\n",
    "results_callback = RLResults(\"results\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger = False,\n",
    "    max_epochs=1e6,   # we use a callback to stop when completed the required number of episodes\n",
    "    callbacks = [MaxEpisodesStop(), results_callback],\n",
    "    enable_model_summary = False,\n",
    "    enable_checkpointing = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( \"Training started at:\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") )\n",
    "fit_begin = time.time()  # measure running time\n",
    "\n",
    "trainer.fit(agent) # run the training\n",
    "\n",
    "fit_time = time.time() - fit_begin\n",
    "print( \"Training ended at:\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") )\n",
    "print(f\"Fit time:\", str(datetime.timedelta(seconds=fit_time)) )\n",
    "\n",
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoint on disk\n",
    "trainer.save_checkpoint(\"CartPolePixels/BestAgent_none_penalty_sgd_relu_2000.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"Temperature\"   : results_callback.temperatures,\n",
    "           \"Loss\"          : results_callback.losses,\n",
    "           \"Episode reward\": results_callback.rewards,\n",
    "           \"Score\"         : results_callback.scores,\n",
    "          }\n",
    "\n",
    "plot_results(results, \n",
    "             show       = True, \n",
    "             avg_window = 20, \n",
    "             savepath   = \"CartPolePixels/History_none_penalty_sgd_relu_2000.pdf\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained agent\n",
    "[Table of contents](#top-shortcuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model checkpoint (uncomment and change path if needed)\n",
    "path = \"CartPolePixels/BestAgent_none_penalty_sgd_relu_2000.ckpt\"\n",
    "agent = DQNAgent.load_from_checkpoint(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a set of episodes\n",
    "N_iters = 10\n",
    "results = agent.run(N_iters, record=False)\n",
    "\n",
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print results\n",
    "average_score = []\n",
    "for it in results:\n",
    "    average_score.append(it['score'])\n",
    "    print(f\"ID: {it['episode_id']: <{4}}\"+\n",
    "          f\" - Reward: {it['final_reward']: <{20}}\"+\n",
    "          f\" - Score: {it['score']: <{8}}\")\n",
    "    \n",
    "print(\"\")\n",
    "print(f\"Average score: {np.mean(average_score)} +/- {np.std(average_score)}\")\n",
    "print(\"Solved episodes: \", sum([av==500 for av in average_score]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a set of episodes and record videos\n",
    "N_iters = 10\n",
    "results = agent.run(N_iters, record=True, video_folder=\"CartPolePixels/Videos_agent_6\")\n",
    "\n",
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assistance of a pretrained agent\n",
    "[Table of contents](#top-shortcuts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we test the usage of a pretrained network as a *teacher* for the new agent. The teacher network at the beginning of the training select the action to take with a probability of `0.6`. This probability decreases linearly until it reaches 0 when completed the `60%` of the episodes assigned. <br>\n",
    "As teacher we have used the network trained from the environment state variables, which can easily solve the task. We import the network class and the weights in the following cells. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher model class\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_space_dim, action_space_dim, \n",
    "                 hidden_units = [128,128],\n",
    "                 activation   = \"tanh\",\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # activation\n",
    "        if activation == \"tanh\":\n",
    "            self.act = nn.Tanh\n",
    "        elif activation == \"relu\":\n",
    "            self.act = nn.ReLU\n",
    "        \n",
    "        # layers units\n",
    "        units = [state_space_dim]+hidden_units+[action_space_dim]\n",
    "        \n",
    "        layers_list = []\n",
    "        for idx in range(len(units)-2):\n",
    "            layers_list.append(nn.Linear(units[idx], units[idx+1]))\n",
    "            layers_list.append(self.act())\n",
    "        layers_list.append(nn.Linear(units[-2], units[-1]))\n",
    "\n",
    "        self.linear = nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load teacher model\n",
    "teacher_net = DQN(state_space_dim  = 4, \n",
    "                  action_space_dim = 2, \n",
    "                  hidden_units     = [256, 64],\n",
    "                  activation       = \"tanh\"\n",
    "                 )\n",
    "# load model weights from file\n",
    "teacher_net.load_state_dict(torch.load(\"CartPolePixels/TeacherNet_weights\"))\n",
    "teacher_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent with assistance\n",
    "[Table of contents](#top-shortcuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the agent and initialize training environment\n",
    "# network hyper-parameters\n",
    "policy_params = {\"conv_channels\" : [16,32],\n",
    "                 \"linear_units\"  : [512,128],\n",
    "                 \"activation\"    : \"relu\", \n",
    "                 \"batch_norm\"    : True,\n",
    "                 \"dropout\"       : 0.,\n",
    "                 \"conv_config\"   : [[8,4,0],  \n",
    "                                    [4,2,0],\n",
    "                                   ],\n",
    "                }\n",
    "# exploration profile parameters\n",
    "behaviour_params = {\"initial_temperature\"    : 4.,\n",
    "                    \"decay_const_in_interval\": 12,   #higher for teacher-assisted training\n",
    "                   }\n",
    "\n",
    "# other hyper-parameters\n",
    "penalty_type     = \"none\"   # \"state\"  or  \"pixels\" or \"none\"\n",
    "N_episodes       = 1000\n",
    "mem_capacity     = 15360\n",
    "target_sync_rate = 200      # number of steps between target net updates\n",
    "batch_size       = 128\n",
    "learning_rate    = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the agent\n",
    "agent = DQNAgent(env_name         = \"CartPole-v1\", \n",
    "                 N_episodes       = N_episodes,\n",
    "                 mem_capacity     = mem_capacity,\n",
    "                 policy_params    = policy_params,\n",
    "                 behaviour_params = behaviour_params,\n",
    "                 target_sync_rate = target_sync_rate,   # number of steps between target net updates\n",
    "                 batch_size       = batch_size,\n",
    "                 gamma            = 0.97,\n",
    "                 optimizer        = \"sgd\",   \n",
    "                 learning_rate    = learning_rate, \n",
    "                 L2_penalty       = 0.,\n",
    "                 momentum         = 0.,\n",
    "                 seed             = MAGIC_NUM,\n",
    "                 penalty_type     = penalty_type,    # \"state\"  or  \"pixels\" or \"none\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set teacher net\n",
    "agent.set_teacher(teacher_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill memory with initial random steps\n",
    "warm_up_steps = 5120\n",
    "agent.fill_memory(warm_up_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup trainer and callbacks\n",
    "results_callback = RLResults(\"results\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger = False,\n",
    "    max_epochs=1e6,   # we use a callback to stop when completed the required number of episodes\n",
    "    callbacks = [MaxEpisodesStop(), results_callback],\n",
    "    enable_model_summary = False,\n",
    "    enable_checkpointing = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Training started at:\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") )\n",
    "fit_begin = time.time()  # measure running time\n",
    "\n",
    "trainer.fit(agent) # run the training\n",
    "\n",
    "fit_time = time.time() - fit_begin\n",
    "print( \"Training ended at:\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") )\n",
    "print(f\"Fit time:\", str(datetime.timedelta(seconds=fit_time)) )\n",
    "\n",
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoint on disk\n",
    "agent.teacher_net = None #remove teacher as we don't need to checkpoint it\n",
    "trainer.save_checkpoint(\"CartPolePixels/BestAgent_none_penalty_sgd_relu_teacher.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"Temperature\"   : results_callback.temperatures,\n",
    "           \"Loss\"          : results_callback.losses,\n",
    "           \"Episode reward\": results_callback.rewards,\n",
    "           \"Score\"         : results_callback.scores,\n",
    "          }\n",
    "\n",
    "plot_results(results, \n",
    "             show       = True, \n",
    "             avg_window = 20, \n",
    "             savepath   = \"CartPolePixels/History_none_penalty_sgd_relu_teacher.pdf\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the agent trained with assistance\n",
    "[Table of contents](#top-shortcuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random state\n",
    "seed_everything(MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model checkpoint (uncomment and change path if needed)\n",
    "path = \"CartPolePixels/BestAgent_none_penalty_sgd_relu_teacher.ckpt\"\n",
    "agent = DQNAgent.load_from_checkpoint(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a set of episodes\n",
    "N_iters = 10\n",
    "results = agent.run(N_iters, record=False)\n",
    "\n",
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print results\n",
    "average_score = []\n",
    "for it in results:\n",
    "    average_score.append(it['score'])\n",
    "    print(f\"ID: {it['episode_id']: <{4}}\"+\n",
    "          f\" - Reward: {it['final_reward']: <{20}}\"+\n",
    "          f\" - Score: {it['score']: <{8}}\")\n",
    "    \n",
    "print(\"\")\n",
    "print(f\"Average score: {np.mean(average_score)} +/- {np.std(average_score)}\")\n",
    "print(\"Solved episodes: \", sum([av==500 for av in average_score]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a set of episodes and record videos\n",
    "N_iters = 10\n",
    "results = agent.run(N_iters, record=True, video_folder=\"CartPolePixels/Videos_teacher_8\")\n",
    "\n",
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

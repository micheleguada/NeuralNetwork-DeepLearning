{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "---\n",
    "A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "---\n",
    "\n",
    "\n",
    "# Homework 3 - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Michele Guadagnini - Mt.1230663"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: CartPole-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the exercise is based on the notebook of `LAB 07` about Reinforcement Learning and part of the code used here is taken or adapted from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBelL8nBZVsn"
   },
   "outputs": [],
   "source": [
    "### ADDITIONAL LIBRARIES THAT NEED INSTALLATION (uncomment if needed)\n",
    "\n",
    "#!pip install gym\n",
    "#!pip install optuna\n",
    "\n",
    "### the followings are required to plot and save figures about optuna study\n",
    "#!pip install plotly\n",
    "#!pip install kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import deque # this python module implements exactly what we need for the replay memory\n",
    "import gym\n",
    "import optuna\n",
    "\n",
    "MAGIC_NUM = 23   #seed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"top-shortcuts\"></a>\n",
    "## Table of contents:\n",
    "\n",
    "1. [**Model and tools implementation**](#Model-and-tools-implementation)\n",
    "1. [**Impact of exploration profile**](#Impact-of-exploration-profile)\n",
    "    1. [Exploration profiles with softmax and $\\epsilon$-greedy behaviours](#Exploration-profiles-with-softmax-and-$\\epsilon$-greedy-behaviours)\n",
    "    1. [Comparison of different profiles results](#Comparison-of-different-profiles-results)\n",
    "\n",
    "1. [**Tuning model hyper-parameters and reward function**](#Tuning-model-hyper-parameters-and-reward-function)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and tools implementation\n",
    "[Table of contents](#top-shortcuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity) # Define a queue with maxlen \"capacity\"\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        #  Add the tuple (state, action, next_state, reward) to the queue\n",
    "        self.memory.append( (state, action, next_state, reward) )\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self)) # Get all the samples if the requested batch_size is higher than the number of sample currently in the memory\n",
    "        return random.sample(self.memory, batch_size) # Randomly select \"batch_size\" samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory) # Return the number of samples currently stored in the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W41rXekb8x7K"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_space_dim, action_space_dim, \n",
    "                 hidden_units = [128,128],\n",
    "                 activation   = \"tanh\",\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # activation\n",
    "        if activation == \"tanh\":\n",
    "            self.act = nn.Tanh\n",
    "        elif activation == \"relu\":\n",
    "            self.act = nn.ReLU\n",
    "        \n",
    "        # layers units\n",
    "        units = [state_space_dim]+hidden_units+[action_space_dim]\n",
    "        \n",
    "        layers_list = []\n",
    "        for idx in range(len(units)-2):\n",
    "            layers_list.append(nn.Linear(units[idx], units[idx+1]))\n",
    "            layers_list.append(self.act())\n",
    "        layers_list.append(nn.Linear(units[-2], units[-1]))\n",
    "\n",
    "        self.linear = nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionChoice(object):\n",
    "    \"\"\"\n",
    "    Class that implements both epsilon-greedy and softmax behaviours.\n",
    "    \"\"\"\n",
    "    def __init__(self, behaviour=\"eps-greedy\", exploration_profile=None, behaviour_param=0.05):\n",
    "        \n",
    "        if behaviour == \"eps-greedy\":\n",
    "            #print(\"Behaviour set to 'eps-greedy'.\")\n",
    "            self.behaviour = \"eps-greedy\"\n",
    "            self._choice_func = self._eps_greedy_choice\n",
    "        elif behaviour == \"softmax\":\n",
    "            #print(\"Behaviour set to 'softmax'.\")\n",
    "            self.behaviour = \"softmax\"\n",
    "            self._choice_func = self._softmax_choice\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown behaviour type: {behaviour}.\")\n",
    "        \n",
    "        self.exploration_profile = exploration_profile\n",
    "        self.behaviour_param     = behaviour_param    # used only if 'exploration_profile' is None \n",
    "                                                                           \n",
    "    \n",
    "    def choose_action(self, net, state, iter_id):\n",
    "        \n",
    "        # evaluate network output\n",
    "        net_out = self._evaluate_net_output(net, state)\n",
    "        # get parameter (eps or temperature) based on profile\n",
    "        param  = self._get_behaviour_param(iter_id)\n",
    "        # apply choice\n",
    "        action = self._choice_func(net_out, param)\n",
    "        \n",
    "        return action, net_out.numpy()   \n",
    "    \n",
    "    def choose_optimal_action(self, net, state):\n",
    "        \n",
    "        # evaluate network output\n",
    "        net_out = self._evaluate_net_output(net, state)\n",
    "        # apply choice\n",
    "        action = int(net_out.argmax())\n",
    "        \n",
    "        return action, net_out.numpy()  \n",
    "    \n",
    "    \n",
    "    def _get_behaviour_param(self, iter_id):\n",
    "        \n",
    "        if self.exploration_profile is not None:\n",
    "            return self.exploration_profile[int(iter_id)]\n",
    "        else:\n",
    "            return self.behaviour_param  \n",
    "    \n",
    "    def _evaluate_net_output(self, net, state):\n",
    "        # Evaluate the network output from the current state\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            state = torch.tensor(state, dtype=torch.float32) # Convert the state to tensor\n",
    "            net_out = net(state)\n",
    "        return net_out\n",
    "          \n",
    "    def _eps_greedy_choice(self, net_out, epsilon):\n",
    "        if epsilon > 1 or epsilon < 0:\n",
    "            raise Exception('The epsilon value must be between 0 and 1')\n",
    "        \n",
    "        # Get the best action (argmax of the network output)\n",
    "        best_action = int(net_out.argmax())\n",
    "        \n",
    "        # Get the number of possible actions\n",
    "        action_space_dim = net_out.shape[-1]\n",
    "        \n",
    "        # Select a non optimal action with probability epsilon, otherwise choose the best action\n",
    "        if random.random() < epsilon:\n",
    "            # List of non-optimal actions\n",
    "            non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]\n",
    "            # Select randomly\n",
    "            action = random.choice(non_optimal_actions)\n",
    "        else:\n",
    "            # Select best action\n",
    "            action = best_action\n",
    "\n",
    "        return action     \n",
    "         \n",
    "    def _softmax_choice(self, net_out, temperature):\n",
    "        if temperature < 0:\n",
    "            raise Exception('The temperature value must be greater than or equal to 0 ')\n",
    "\n",
    "        # If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0\n",
    "        if temperature == 0.:\n",
    "            best_action = int(net_out.argmax())\n",
    "            return best_action\n",
    "\n",
    "        # Apply softmax with temp\n",
    "        temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability\n",
    "        softmax_out = nn.functional.softmax(net_out / temperature, dim=0).numpy()\n",
    "\n",
    "        # Sample the action using softmax output as mass pdf\n",
    "        all_possible_actions = np.arange(0, softmax_out.shape[-1])\n",
    "        # this samples a random element from \"all_possible_actions\" with the probability \n",
    "        #      distribution p (softmax_out in this case)\n",
    "        action = np.random.choice(all_possible_actions, p=softmax_out) \n",
    "\n",
    "        return action         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_profile(initial_value=5., num_iterations=1000, k=6):\n",
    "    # y = N * exp(-t / tau), where:\n",
    "    #   N    = initial value\n",
    "    #   tau  = characteristic length     -> (num_iterations / k)\n",
    "    #   k    = number of characteristic length to be represented in the interval\n",
    "    #   t    = step \n",
    "    \n",
    "    tau = num_iterations / k\n",
    "    exploration_profile = [initial_value * np.exp(-ii/tau) for ii in range(num_iterations)]\n",
    "\n",
    "    return exploration_profile\n",
    "\n",
    "\n",
    "def linear_profile(initial_value=1., num_iterations=1000, decay_frac = 0.5, bottom=0.):\n",
    "    # linear decay for a certain fraction of iterations, then set to 'bottom'\n",
    "    \n",
    "    num_decay_iters = int(num_iterations*decay_frac)\n",
    "    decay_profile = [bottom+(initial_value-bottom)*(num_decay_iters - ii)/num_decay_iters for ii in range(num_decay_iters)]\n",
    "    exploration_profile = decay_profile + [bottom for ii in range(num_iterations - num_decay_iters)]\n",
    "    \n",
    "    return exploration_profile\n",
    "\n",
    "\n",
    "def noisy_profile(initial_value=1., num_iterations=1000, noise_frac=0.6, clip_value=None):\n",
    "    # noisy profile from half-normal distribution\n",
    "    decay = linear_profile(1., num_iterations, noise_frac)\n",
    "    \n",
    "    exploration_profile = []\n",
    "    for ii in range(num_iterations):\n",
    "        noise = np.abs( np.random.randn() )\n",
    "        point = initial_value*noise*decay[ii]\n",
    "        if clip_value is not None:\n",
    "            point = min(point, clip_value)\n",
    "        exploration_profile.append( point )        \n",
    "    \n",
    "    return exploration_profile   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size):\n",
    "        \n",
    "    # Sample the data from the replay memory\n",
    "    batch = replay_mem.sample(batch_size)\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    # Create tensors for each element of the batch\n",
    "    states      = torch.tensor([s[0] for s in batch], dtype=torch.float32)\n",
    "    actions     = torch.tensor([s[1] for s in batch], dtype=torch.int64)\n",
    "    rewards     = torch.tensor([s[3] for s in batch], dtype=torch.float32)\n",
    "\n",
    "    # Compute a mask of non-final states (all the elements where the next state is not None)\n",
    "    non_final_next_states = torch.tensor([s[2] for s in batch if s[2] is not None], dtype=torch.float32) # the next state can be None if the game has ended\n",
    "    non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool)\n",
    "\n",
    "    # Compute all the Q values (forward pass)\n",
    "    policy_net.train()\n",
    "    q_values = policy_net(states)\n",
    "    # Select the proper Q value for the corresponding action taken Q(s_t, a)\n",
    "    state_action_values = q_values.gather(1, actions.unsqueeze(1))\n",
    "\n",
    "    # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n",
    "    with torch.no_grad():\n",
    "        target_net.eval()\n",
    "        q_values_target = target_net(non_final_next_states)\n",
    "    next_state_max_q_values = torch.zeros(batch_size)\n",
    "    next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n",
    "    expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n",
    "\n",
    "    # Compute the Huber loss\n",
    "    loss = loss_fn(state_action_values, expected_state_action_values)  # or try L2, L1, ...\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n",
    "    nn.utils.clip_grad_norm_(policy_net.parameters(), 2)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_correction(reward, state, pos_weight=1., angle_weight=0.):\n",
    "    # position penalty\n",
    "    reward -= pos_weight * np.abs(state[0])\n",
    "    \n",
    "    # angle penalty\n",
    "    reward -= angle_weight * np.abs(state[2])\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(behaviour, env, policy_net, target_net, loss_fn, optimizer, \n",
    "               replay_mem, gamma, batch_size, seed=MAGIC_NUM, pos_weight=1., angle_weight=0.,\n",
    "               render=False, verbose=False, bad_state_penalty=0., target_net_update_steps=10,\n",
    "              ):\n",
    "    \n",
    "    final_scores  = []  # final score of each episode\n",
    "    final_losses  = []  # final loss of each episode\n",
    "    final_rewards = []  # final cumulative rewards\n",
    "    \n",
    "    N_iters = len(behaviour.exploration_profile)  # number of iterations    \n",
    "    for episode_num in tqdm(range(N_iters)):\n",
    "\n",
    "        # Reset the environment and get the initial state\n",
    "        state = env.reset()\n",
    "        # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "        score = 0\n",
    "        loss  = 0.        \n",
    "        cumulative_reward = 0.\n",
    "        \n",
    "        done  = False\n",
    "\n",
    "        # Go on until the pole falls off\n",
    "        while not done:\n",
    "            # Choose the action following the policy and the behaviour profile\n",
    "            action, q_values = behaviour.choose_action(policy_net, state, iter_id=episode_num)\n",
    "            # Apply the action and get the next state, the reward and a flag \"done\" that is \n",
    "            #      True if the game is ended\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Update the final score (+1 for each step)\n",
    "            score += 1           \n",
    "\n",
    "            # Apply penalty for bad state\n",
    "            if done: # if the pole has fallen down \n",
    "                reward += bad_state_penalty\n",
    "                next_state = None\n",
    "            # reward correction\n",
    "            reward = reward_correction(reward, state, pos_weight=1., angle_weight=0.)\n",
    "            cumulative_reward += reward\n",
    "\n",
    "            # Update the replay memory\n",
    "            replay_mem.push(state, action, next_state, reward)\n",
    "            # Update the network\n",
    "            if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough\n",
    "                # samples in the replay memory, otherwise the training will use the same samples too often\n",
    "                loss = update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)\n",
    "\n",
    "            # Visually render the environment (disable to speed up the training)\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Set the current state for the next iteration\n",
    "            state = next_state\n",
    "\n",
    "        # Update the target network every target_net_update_steps episodes\n",
    "        if episode_num % target_net_update_steps == 0:\n",
    "            if verbose:\n",
    "                print('Updating target network...')\n",
    "            # This will copy the weights of the policy network to the target network\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # Print the final score of the episode\n",
    "        if verbose:\n",
    "            print(f\"EPISODE: {episode_num + 1} - FINAL SCORE: {score}\") # Print the final score\n",
    "            \n",
    "        # store episode results\n",
    "        final_scores.append(score)\n",
    "        final_losses.append(loss)\n",
    "        final_rewards.append(cumulative_reward)   \n",
    "        \n",
    "    # close environment\n",
    "    env.close()\n",
    "            \n",
    "    return policy_net, final_scores, final_losses, final_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(path_to_model, n_episodes=10, seed=23, video_folder=\"Videos\",\n",
    "              name_prefix=\"rl\", model_hypers=None, render=False,\n",
    "             ):\n",
    "    # Initialize the Gym environment\n",
    "    env = gym.make('CartPole-v1') \n",
    "    env.seed(seed) # Set a random seed for the environment (reproducible results)\n",
    "    \n",
    "    # Get the shapes of the state space (observation_space) and action space (action_space)\n",
    "    state_space_dim  = env.observation_space.shape[0]\n",
    "    action_space_dim = env.action_space.n   \n",
    "\n",
    "    # Initialize the policy network\n",
    "    if model_hypers is None:\n",
    "        policy_net = DQN(state_space_dim, action_space_dim)\n",
    "        angle_penalty = 0.\n",
    "    else:\n",
    "        policy_net = DQN(state_space_dim, action_space_dim, \n",
    "                         model_hypers[\"DQN_units\"], model_hypers[\"DQN_activ\"],\n",
    "                        )\n",
    "        angle_penalty = model_hypers[\"angle_weight\"]\n",
    "\n",
    "    # load model weights from file\n",
    "    policy_net.load_state_dict(torch.load(path_to_model))\n",
    "    policy_net.eval()\n",
    "    \n",
    "    # define behaviour\n",
    "    behaviour = ActionChoice()\n",
    "\n",
    "    for num_episode in range(n_episodes): \n",
    "        \n",
    "        # save only last episode video\n",
    "        if num_episode == (n_episodes-1):\n",
    "            # wrapping env to save video \n",
    "            os.makedirs(video_folder, exist_ok=True)\n",
    "            env = gym.wrappers.RecordVideo(env, video_folder=video_folder, \n",
    "                                           name_prefix=name_prefix,\n",
    "                                           episode_trigger=lambda idx: True,\n",
    "                                          )        \n",
    "        \n",
    "        # Reset the environment and get the initial state\n",
    "        state = env.reset()\n",
    "        # Reset the score. The final score will be the total amount of steps before the pole falls\n",
    "        score = 0\n",
    "        cumulated_reward = 0\n",
    "        done = False\n",
    "        # Go on until the pole falls off or the score reach 490\n",
    "        while not done:\n",
    "            # Choose the best action\n",
    "            action, q_values = behaviour.choose_optimal_action(policy_net, state)\n",
    "            # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # Update the final score (+1 for each step)\n",
    "            score += reward \n",
    "            # add penalty to reward\n",
    "            reward = reward_correction(reward, state, angle_weight=angle_penalty)\n",
    "            cumulated_reward += reward\n",
    "            # Set the current state for the next iteration\n",
    "            state = next_state\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "            # Check if the episode ended (the pole fell down)\n",
    "        # Print the final score\n",
    "        print(f\"EPISODE {num_episode + 1} - FINAL SCORE: {score} \" + \n",
    "              f\"- FINAL REWARD (with position and/or angle penalty): {cumulated_reward}\") \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGbVwpVfSr4p"
   },
   "source": [
    "## Impact of exploration profile\n",
    "[Table of contents](#top-shortcuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(MAGIC_NUM)\n",
    "np.random.seed(MAGIC_NUM)\n",
    "random.seed(MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder to store trained policy nets\n",
    "save_folder = \"ProfileStudy/Models\"\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVpk-g0i9d-B"
   },
   "outputs": [],
   "source": [
    "### PARAMETERS\n",
    "gamma                    = 0.97  # gamma parameter for the long term reward\n",
    "replay_memory_capacity   = 10000 # Replay memory capacity\n",
    "lr                       = 1e-2  # Optimizer learning rate\n",
    "target_net_update_steps  = 10    # Number of episodes to wait before updating the target network\n",
    "batch_size               = 128   # Number of samples to take from the replay memory for each update\n",
    "bad_state_penalty        = 0     # Penalty to the reward when we are in a bad state \n",
    "                                 #     (in this case when the pole falls down) \n",
    "min_samples_for_training = 1000  # Minimum samples in the replay memory to enable the training\n",
    "\n",
    "N_episodes = 1000                # Number of episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration profiles with softmax and $\\epsilon$-greedy behaviours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we define the `exploration profiles` to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour_types = [\"softmax\"]*4 + [\"eps-greedy\"]*4 \n",
    "decay_profiles  = [\"exp\",\"linear\",\"noisy\",\"const\"]*2\n",
    "\n",
    "profiles = []\n",
    "\n",
    "# define profiles for softmax (parameter is temperature)\n",
    "temp_init_value = 5.\n",
    "k = 10  #number of exponential decay characteristic length to be included in N_episodes range \n",
    "profiles.append( exponential_profile(initial_value=temp_init_value, num_iterations=N_episodes, k=k) )\n",
    "profiles.append( linear_profile(     initial_value=temp_init_value, num_iterations=N_episodes, decay_frac=0.4) )\n",
    "profiles.append( noisy_profile(      initial_value=temp_init_value, num_iterations=N_episodes) )\n",
    "profiles.append( linear_profile(initial_value  = temp_init_value/10., #constant temperature profile\n",
    "                                num_iterations = N_episodes, \n",
    "                                bottom         = temp_init_value/10.,\n",
    "                               ) )\n",
    "\n",
    "# define profiles for eps-greedy (parameter is eps probability)\n",
    "eps_init_value = 1.\n",
    "k = 10\n",
    "profiles.append( exponential_profile(initial_value=eps_init_value , num_iterations=N_episodes,  k=k) )\n",
    "profiles.append( linear_profile(     initial_value=eps_init_value , num_iterations=N_episodes, decay_frac=0.4) )\n",
    "profiles.append( noisy_profile(      initial_value=eps_init_value , num_iterations=N_episodes, clip_value=1.) )\n",
    "profiles.append( linear_profile(initial_value  = eps_init_value/10., #constant eps profile\n",
    "                                num_iterations = N_episodes, \n",
    "                                bottom         = eps_init_value/10.,\n",
    "                               ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one profile as example\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(profiles[0])\n",
    "plt.grid()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Exploration profile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop train over different parameter decay profiles and behaviour types\n",
    "\n",
    "results = []\n",
    "for idx, profile in enumerate(profiles):\n",
    "    # reset random seeds\n",
    "    torch.manual_seed(MAGIC_NUM)\n",
    "    np.random.seed(MAGIC_NUM)\n",
    "    random.seed(MAGIC_NUM)\n",
    "    \n",
    "    btype = behaviour_types[idx]\n",
    "    dtype = decay_profiles[idx]\n",
    "    \n",
    "    print(f\"Starting iteration: {idx+1} with '{btype}' behaviour and '{dtype}' parameter decay...\")\n",
    "    \n",
    "    ### INITIALIZATION ### -----------------------------------------------------------------\n",
    "    # Initialize the Gym environment\n",
    "    env = gym.make('CartPole-v1') \n",
    "    env.seed(MAGIC_NUM) # Set a random seed for the environment (reproducible results)\n",
    "    \n",
    "    # Get the shapes of the state space (observation_space) and action space (action_space)\n",
    "    state_space_dim  = env.observation_space.shape[0]\n",
    "    action_space_dim = env.action_space.n\n",
    "    \n",
    "    # Initialize the replay memory\n",
    "    replay_mem = ReplayMemory(replay_memory_capacity)    \n",
    "\n",
    "    # Initialize the policy network\n",
    "    policy_net = DQN(state_space_dim, action_space_dim)\n",
    "\n",
    "    # Initialize the target network with the same weights of the policy network\n",
    "    target_net = DQN(state_space_dim, action_space_dim)\n",
    "    target_net.load_state_dict(policy_net.state_dict()) # copy weights from policy network to target network\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.SGD(policy_net.parameters(), lr=lr) \n",
    "    # NB: the optimizer will update ONLY the parameters of the policy network\n",
    "\n",
    "    # Initialize the loss function (Huber loss)\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "    \n",
    "    # define behaviour\n",
    "    behaviour = ActionChoice(behaviour=btype, exploration_profile=profile)\n",
    "    \n",
    "    ### TRAIN LOOP ### ----------------------------------------------------------------------\n",
    "    policy_net, scores, losses, rewards = train_loop(behaviour, env, policy_net, target_net, loss_fn, \n",
    "                                                     optimizer, replay_mem, gamma, batch_size, \n",
    "                                                     seed=MAGIC_NUM, render=False, verbose=False,\n",
    "                                                    )\n",
    "    # store results and metrics\n",
    "    results.append({\"decay_type\": dtype,\n",
    "                    \"behaviour\" : btype,\n",
    "                    \"profile\":profile,\n",
    "                    \"scores\" :scores,\n",
    "                    \"losses\" :losses,\n",
    "                    \"rewards\":rewards,  #different from score since it includes also the position penalty\n",
    "                   })\n",
    "    # save DQN model\n",
    "    torch.save(policy_net.state_dict(), save_folder+\"/DQN-profile_\"+btype+\"-decay_\"+dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of different profiles results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare softmax with eps-greedy\n",
    "def plot_compare_results(softmax_r, eps_greedy_r, to_plot, y_labels, \n",
    "                         x_label=\"Episode\", figsize=(8,6), folder=\"Models\", avg_window=None,\n",
    "                        ):\n",
    "    \n",
    "    Nplots = len(to_plot)\n",
    "    full_fig_size = (figsize[0]*Nplots, figsize[1])\n",
    "    \n",
    "    fig, axs = plt.subplots(1, Nplots, figsize=full_fig_size)\n",
    "    \n",
    "    for idx, ax in enumerate(axs):\n",
    "        if to_plot[idx] == \"profile\":\n",
    "            ax.plot(softmax_r[to_plot[idx]], label=\"softmax\", color=\"blue\")\n",
    "            ax.plot(eps_greedy_r[to_plot[idx]], label=\"eps-greedy\", color=\"red\")\n",
    "        \n",
    "        if to_plot[idx] != \"profile\" and avg_window is not None:\n",
    "            ax.plot(softmax_r[to_plot[idx]], label=\"softmax\", color=\"lightblue\")    \n",
    "            # compute and plot moving average of score\n",
    "            softmax_avg = np.convolve(softmax_r[to_plot[idx]], np.ones(avg_window), 'valid') / avg_window            \n",
    "            x_space = np.arange(avg_window/2,len(softmax_avg)+avg_window/2)\n",
    "            ax.plot(x_space, softmax_avg, label=\"softmax (smoothed)\", lw=2, color=\"blue\")\n",
    "            \n",
    "            ax.plot(eps_greedy_r[to_plot[idx]], label=\"eps-greedy\", color=\"orange\")\n",
    "            # compute and plot moving average of score\n",
    "            eps_greedy_avg = np.convolve(eps_greedy_r[to_plot[idx]], np.ones(avg_window), 'valid') / avg_window            \n",
    "            ax.plot(x_space, eps_greedy_avg, label=\"eps-greedy (smoothed)\", lw=2, color=\"red\")\n",
    "        \n",
    "        ax.grid()\n",
    "        ax.set_xlabel(x_label)\n",
    "        ax.set_ylabel(y_labels[idx])\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()   \n",
    "    \n",
    "    #save picture\n",
    "    tt = softmax_r[\"decay_type\"]\n",
    "    plt.savefig(folder+f\"/policy_compare-{tt}_decay.pdf\")\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot  = [\"profile\",\"losses\",\"scores\",\"rewards\"]\n",
    "y_labels = ['Exploration profile', 'Loss', 'Score', 'Episode cumulated reward']\n",
    "\n",
    "for idx in range(len(results)//2):\n",
    "    plot_compare_results(results[idx], results[idx+len(results)//2], \n",
    "                         to_plot, y_labels, folder=save_folder,\n",
    "                         avg_window = 20,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "behaviour_types = [\"softmax\"]*4 + [\"eps-greedy\"]*4 \n",
    "decay_profiles  = [\"exp\",\"linear\",\"noisy\",\"const\"]*2\n",
    "\n",
    "# testing the models\n",
    "for idx in range(len(behaviour_types)):\n",
    "    torch.manual_seed(MAGIC_NUM)\n",
    "    np.random.seed(MAGIC_NUM)\n",
    "    random.seed(MAGIC_NUM)\n",
    "    \n",
    "    btype = behaviour_types[idx]\n",
    "    dtype = decay_profiles[idx]    \n",
    "    print(f\"### MODEL: {btype} with {dtype} decay: ###\")\n",
    "    \n",
    "    path_to_model = save_folder+\"/DQN-profile_\"+btype+\"-decay_\"+dtype\n",
    "    \n",
    "    # TEST\n",
    "    test_loop(path_to_model, \n",
    "              n_episodes=10, \n",
    "              seed=MAGIC_NUM, \n",
    "              video_folder=\"ProfileStudy/Videos\"\n",
    "              name_prefix=\"profile_\"+btype+\"-decay_\"+dtype,\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning model hyper-parameters and reward function\n",
    "[Table of contents](#top-shortcuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(MAGIC_NUM)\n",
    "np.random.seed(MAGIC_NUM)\n",
    "random.seed(MAGIC_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna settings \n",
    "OPTUNA_DIR = \"OptunaStudy\"\n",
    "study_name = \"CartPole-v1_optuna_study\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells below we define the hyper-parameter space and the *Objective* function to be used in the Optuna study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed hyper-parameters\n",
    "replay_memory_capacity   = 10000\n",
    "min_samples_for_training = 1000   # Minimum samples in the replay memory to enable the training\n",
    "batch_size = 128\n",
    "\n",
    "N_episodes = 1000  # Number of episodes\n",
    "\n",
    "# define possible hyper-parameters values \n",
    "### list : categorical sampling\n",
    "### tuple: uniform sampling \n",
    "hyperparameters_space = {\"optim\"       : [\"sgd\", \"adam\"],     # optimizer\n",
    "                         \"lr\"          : (1e-4, 1e-1),        # learning rate\n",
    "                         \"DQN_units\"   : [[128,128],[256,64],[128,32],[64,64]],\n",
    "                         \"DQN_activ\"   : [\"tanh\", \"relu\"],\n",
    "                         \"softmax_init\": (2, 10),\n",
    "                         \"softmax_k\"   : (4, 10),\n",
    "                         \"gamma\"       : (0.9, 0.99),         # discount parameter\n",
    "                         \"target_net_update_steps\": [2, 5, 10],\n",
    "                         \"bad_state_penalty\": [0, 1, 2],\n",
    "                         \"angle_weight\": (0., 1.),            # penalty weight for pole angle \n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective(object):\n",
    "    \n",
    "    def __init__(self, model_class, memory_class, hp_space, env_name, env_seed=0, folder=\"OptunaTrials\"):  \n",
    "        self.model_class  = model_class\n",
    "        self.memory_class = memory_class\n",
    "        self.hp_space     = hp_space \n",
    "        self.env_name     = env_name\n",
    "        self.env_seed     = env_seed\n",
    "        \n",
    "        self.folder = folder\n",
    "        self.best_solved_episodes = 0\n",
    "        \n",
    "    def keep_best_model(self, net, scores, trial_id):\n",
    "        \n",
    "        counter = 0\n",
    "        for idx, score in enumerate(scores):\n",
    "            if score >= 500:\n",
    "                counter += 1\n",
    "                \n",
    "        if (counter >= self.best_solved_episodes) and (counter > 0):\n",
    "            # obtained a new best model\n",
    "            self.best_solved_episodes = counter\n",
    "            torch.save(net.state_dict(), self.folder + f\"/BestDQN-TrialID_{trial_id}-Solved_{counter}\")  \n",
    "                \n",
    "        return counter\n",
    "        \n",
    "    def _sample_param(self, trial, param_name, param_space):\n",
    "        if type(param_space) is list:\n",
    "            if param_name == \"DQN_units\":\n",
    "                param_id = trial.suggest_categorical(param_name+\"_ID\", list(range(len(param_space))))\n",
    "                param = param_space[param_id]\n",
    "            else:                \n",
    "                param = trial.suggest_categorical(param_name, param_space)\n",
    "            \n",
    "        elif type(param_space) is tuple:\n",
    "            param = trial.suggest_uniform(param_name, param_space[0], param_space[1])\n",
    "            \n",
    "        return param\n",
    "    \n",
    "\n",
    "    def plot_trial_results(self, trial_id, results, x_label=\"Episode\", figsize=(8,6), \n",
    "                           avg_window=None, show=False,\n",
    "                          ):\n",
    "        keys   = list(results.keys())\n",
    "        Nplots = len(keys)\n",
    "        full_fig_size = (figsize[0]*Nplots, figsize[1])\n",
    "\n",
    "        fig, axs = plt.subplots(1, Nplots, figsize=full_fig_size)\n",
    "\n",
    "        for idx, ax in enumerate(axs):\n",
    "            if keys[idx] == \"Profile\":\n",
    "                ax.plot(results[keys[idx]], label=\"Temperature profile\", color=\"blue\")\n",
    "\n",
    "            if keys[idx] != \"Profile\" and avg_window is not None:\n",
    "                ax.plot(results[keys[idx]], label=keys[idx], color=\"lightblue\")\n",
    "\n",
    "                # compute and plot moving average of score\n",
    "                moving_avg = np.convolve(results[keys[idx]], np.ones(avg_window), 'valid') / avg_window\n",
    "                \n",
    "                x_space = np.arange(avg_window/2,len(moving_avg)+avg_window/2)\n",
    "                ax.plot(x_space, moving_avg, label=keys[idx]+\" (smoothed)\", lw=2, color=\"blue\")\n",
    "\n",
    "            ax.grid()\n",
    "            ax.set_xlabel(x_label)\n",
    "            ax.set_ylabel(keys[idx])\n",
    "            ax.legend()\n",
    "\n",
    "        plt.tight_layout()   \n",
    "\n",
    "        #save picture\n",
    "        full_path = self.folder+\"/TrialsPlots\"\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "        plt.savefig(full_path+f\"/Results-Trial_{trial_id}.pdf\")\n",
    "        \n",
    "        if show:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "            \n",
    "        return\n",
    "    \n",
    "            \n",
    "    def __call__(self, trial):\n",
    "        \n",
    "        print(f\"Trial [{trial.number}] started at:\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "    \n",
    "        # sample hyper-parameters\n",
    "        params_dict = {}\n",
    "        for key in self.hp_space:\n",
    "            value = self._sample_param(trial, key, self.hp_space[key])\n",
    "            params_dict.update({key:value})            \n",
    "            \n",
    "        # initialize environment\n",
    "        env = gym.make(self.env_name)\n",
    "        env.seed(self.env_seed)\n",
    "        \n",
    "        # Get the shapes of the state space (observation_space) and action space (action_space)\n",
    "        state_space_dim  = env.observation_space.shape[0]\n",
    "        action_space_dim = env.action_space.n\n",
    "\n",
    "        # Initialize the replay memory\n",
    "        replay_mem = self.memory_class(replay_memory_capacity)    \n",
    "\n",
    "        # Initialize the policy network\n",
    "        policy_net = self.model_class(state_space_dim, \n",
    "                                      action_space_dim, \n",
    "                                      hidden_units = params_dict[\"DQN_units\"],\n",
    "                                      activation   = params_dict[\"DQN_activ\"],\n",
    "                                     )\n",
    "\n",
    "        # Initialize the target network with the same weights of the policy network\n",
    "        target_net = self.model_class(state_space_dim, \n",
    "                                      action_space_dim, \n",
    "                                      hidden_units = params_dict[\"DQN_units\"],\n",
    "                                      activation   = params_dict[\"DQN_activ\"],\n",
    "                                     )\n",
    "        target_net.load_state_dict(policy_net.state_dict()) # copy weights from policy network to target network\n",
    "\n",
    "        # Initialize the optimizer\n",
    "        if params_dict[\"optim\"] == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(policy_net.parameters(), lr=params_dict[\"lr\"]) \n",
    "        elif params_dict[\"optim\"] == \"adam\":\n",
    "            optimizer = torch.optim.Adam(policy_net.parameters(), lr=params_dict[\"lr\"]) \n",
    "        # NB: the optimizer will update ONLY the parameters of the policy network\n",
    "\n",
    "        # Initialize the loss function (Huber loss)\n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "        # define behaviour (fixed to softmax with exponential temperature profile)\n",
    "        exploration_profile = exponential_profile(params_dict[\"softmax_init\"], \n",
    "                                                  num_iterations = N_episodes,\n",
    "                                                  k = params_dict[\"softmax_k\"],\n",
    "                                                 )\n",
    "        behaviour = ActionChoice(behaviour=\"softmax\", exploration_profile=exploration_profile)\n",
    "        \n",
    "        ### TRAIN LOOP ### ----------------------------------------------------------------------\n",
    "        policy_net, scores, losses, rewards = train_loop(behaviour, env, policy_net, target_net, loss_fn, \n",
    "                                                         optimizer, replay_mem, params_dict[\"gamma\"],\n",
    "                                                         batch_size, seed=self.env_seed, \n",
    "                                                         render=False, verbose=False,\n",
    "                                                         bad_state_penalty=params_dict[\"bad_state_penalty\"],\n",
    "                                                         target_net_update_steps=params_dict[\"target_net_update_steps\"],\n",
    "                                                         angle_weight=params_dict[\"angle_weight\"],\n",
    "                                                        )\n",
    "        \n",
    "        # build results dictionary\n",
    "        results = {\"Profile\"                 : exploration_profile,\n",
    "                   \"Loss (Huber)\"            : losses,\n",
    "                   \"Score\"                   : scores,\n",
    "                   \"Episode cumulated reward\": rewards,\n",
    "                  } \n",
    "        \n",
    "        # plot and save trial results\n",
    "        self.plot_trial_results(trial_id=trial.number, results=results, avg_window=20)\n",
    "        \n",
    "        # check if it is the best model and store it\n",
    "        solved_episodes = self.keep_best_model(policy_net, scores, trial.number)\n",
    "        \n",
    "        # adding user-attributes\n",
    "        trial.set_user_attr(\"hypers\" , params_dict)\n",
    "        trial.set_user_attr(\"results\", results)\n",
    "        \n",
    "        print(f\"Trial [{trial.number}] ended at:\", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "        print(f\"    Solved episodes: {solved_episodes}\\n\")\n",
    "        \n",
    "        return solved_episodes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna study objective function\n",
    "objective = Objective(model_class  = DQN, \n",
    "                      memory_class = ReplayMemory, \n",
    "                      hp_space     = hyperparameters_space,\n",
    "                      env_name     = 'CartPole-v1',\n",
    "                      env_seed     = MAGIC_NUM,\n",
    "                      folder       = OPTUNA_DIR,\n",
    "                     )\n",
    "\n",
    "# Make the default sampler behave in a deterministic way\n",
    "sampler = optuna.samplers.TPESampler(n_startup_trials = 25,    # use random sampling at beginning\n",
    "                                     seed = MAGIC_NUM,\n",
    "                                    )\n",
    "### create study\n",
    "os.makedirs(OPTUNA_DIR, exist_ok=True)\n",
    "\n",
    "study = optuna.create_study(study_name = study_name, \n",
    "                            direction  = \"maximize\",\n",
    "                            pruner     = None,\n",
    "                            sampler    = sampler,\n",
    "                            storage    = \"sqlite:///\"+OPTUNA_DIR+\"/\"+study_name+\".db\",\n",
    "                            load_if_exists = True,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### run optimization\n",
    "logging.captureWarnings(True)\n",
    "\n",
    "Ntrials = 40\n",
    "MaxTime = None\n",
    "\n",
    "print(\"Starting study '\"+study.study_name+f\"' with n_trials={Ntrials} and timeout={MaxTime}\")\n",
    "study.optimize(objective, \n",
    "               n_trials       = Ntrials, \n",
    "               timeout        = MaxTime, # timeout in seconds\n",
    "               gc_after_trial = True,    # run garbage collection \n",
    "              ) \n",
    "\n",
    "logging.captureWarnings(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the study database \n",
    "study = optuna.load_study(study_name, \n",
    "                          storage = \"sqlite:///\"+OPTUNA_DIR+\"/\"+study_name+\".db\",\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# study results dataframe\n",
    "study_df = study.trials_dataframe()\n",
    "study_df.drop(columns=\"user_attrs_hypers\", inplace=True)\n",
    "study_df.drop(columns=\"user_attrs_results\", inplace=True)\n",
    "study_df = study_df.sort_values(by=\"value\", ascending=False)\n",
    "\n",
    "# print dataframe with top-K trials\n",
    "K = 10\n",
    "study_df.head(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional libraries\n",
    "import optuna\n",
    "import plotly.express as px\n",
    "import optuna.visualization as ov\n",
    "import json\n",
    "\n",
    "# decorator to add a function to a dictionary\n",
    "def make_decorator(dictionary):\n",
    "    def decorator_add_to_dict():\n",
    "        def wrapper(func):\n",
    "            dictionary.update({func.__name__:func})\n",
    "            return func\n",
    "        return wrapper\n",
    "    return decorator_add_to_dict\n",
    "    \n",
    "class OptimizationInspector(object):\n",
    "    \"\"\"\n",
    "    This class provides some plotting functions to analyze the outcome of an optuna study.\n",
    "    \"\"\"\n",
    "    # dictionary of plotting function\n",
    "    plot_dict = {}\n",
    "    _plot_dict_member = make_decorator(plot_dict)\n",
    "    \n",
    "    def __init__(self, study, save_path=\"Results_test\", figsize=(1024,600), fmt=\".pdf\"):\n",
    "        \n",
    "        self.study     = study\n",
    "        self.save_path = save_path + \"/\"\n",
    "        self.data_dict = None\n",
    "        self.fmt       = fmt\n",
    "        \n",
    "        # set figsize\n",
    "        px.defaults.width  = figsize[0]\n",
    "        px.defaults.height = figsize[1]\n",
    "        \n",
    "        # ensure folder existence\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "        \n",
    "    def save_best_hypers_json(self, best_hypers_file):\n",
    "        # save best hyperparameters to file (json)\n",
    "        best_hypers = self.study.best_trial.user_attrs[\"hypers\"]\n",
    "        with open(best_hypers_file, 'w') as fp:\n",
    "            json.dump(best_hypers, fp)\n",
    "        print(\"Best hyper-parameters saved to: '\"+best_hypers_file+\"'.\")\n",
    "    \n",
    "    def _handle_image(self, fig, show, name, save):\n",
    "        # function to plot/save images\n",
    "        if show == \"1\":\n",
    "            fig.show()\n",
    "        if (save) and (name is not None):\n",
    "            full_path = self.save_path + name + self.fmt\n",
    "            fig.write_image(full_path)   \n",
    "            print(\"New image saved: \", full_path)\n",
    "        return\n",
    "            \n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(\"Summary of the Optuna study: \", self.study.study_name)\n",
    "        print(\"   Attempted trials: \", len(self.study.trials) )\n",
    "        study_df  = self.study.trials_dataframe()\n",
    "        completed = len(study_df[study_df[\"state\"]==\"COMPLETE\"])\n",
    "        pruned    = len(study_df[study_df[\"state\"]==\"PRUNED\"  ])\n",
    "        print(\"   Completed trials: \", completed)\n",
    "        print(\"   Pruned trials   : \", pruned   )\n",
    "        print(\"   Best Trial ID   : \", self.study.best_trial.number)\n",
    "        print(\"   Best value      : \", self.study.best_value )\n",
    "        \n",
    "        best_hypers = self.study.best_trial.user_attrs[\"hypers\"]       \n",
    "        print(\"\\nBest set of hyper-parameters:\")\n",
    "        width = max([len(tt) for tt in list(best_hypers)]) # string width when printing param name \n",
    "        for key,var in best_hypers.items():\n",
    "            if key == \"params\":\n",
    "                ww = max([len(tt) for tt in list(var)])\n",
    "                print(\"    Model parameters:\")\n",
    "                for kk,vv in var.items():\n",
    "                    print(f\"        {kk: <{ww}}: {vv}\")\n",
    "            else:\n",
    "                print(f\"    {key: <{width}}: {var}\")\n",
    "        print(\"\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def plot_all(self, parallel_sets = [], contour_sets = [], slice_sets = [], importance_params = [],\n",
    "                 show = \"100011000\", save = True,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Produce all the defined plots in this class. Showing is controlled by the variable 'show'.\n",
    "        It can also save all the plotted pictures. Files names are fixed to some default value.\n",
    "         - show : binary string of lenght equal to the number of methods ('1' to show image, '0' to not show).\n",
    "         - save : if to save the pictures on disk (bool)\n",
    "        \"\"\"\n",
    "        self.data_dict = {\"parallel\"         : parallel_sets    ,\n",
    "                          \"contour\"          : contour_sets     ,\n",
    "                          \"slice\"            : slice_sets       ,\n",
    "                          \"importance_params\": importance_params,\n",
    "                         }\n",
    "        \n",
    "        for idx,key in enumerate(self.plot_dict):\n",
    "            if (show[idx] == \"0\") and not save: #skip plots that are not showed or saved\n",
    "                print(\"   Skipping plot function:\", key)\n",
    "                continue\n",
    "            self.plot_dict[key](self, show=show[idx], save=save)\n",
    "            \n",
    "        self.data_dict = None\n",
    "            \n",
    "        return\n",
    "    \n",
    "    @_plot_dict_member()\n",
    "    def optimization_history(self, show=\"1\", name=\"optimization_history\", save=False):\n",
    "        fig = ov.plot_optimization_history(self.study)\n",
    "        #fig.update_yaxes(type=\"log\")\n",
    "        self._handle_image(fig, show, name, save)\n",
    "        return\n",
    "    \n",
    "    def intermediate_values(self, show=\"1\", name=\"intermediate_values\", save=False):\n",
    "        fig = ov.plot_intermediate_values(self.study)\n",
    "        self._handle_image(fig, show, name, save)\n",
    "        return\n",
    "    \n",
    "    @_plot_dict_member()\n",
    "    def importances(self, params=None, show=\"1\", name=\"importances\", save=False):\n",
    "        if self.data_dict is not None:\n",
    "            params = self.data_dict[\"importance_params\"]\n",
    "        fig = ov.plot_param_importances(self.study, params=params)\n",
    "        self._handle_image(fig, show, name, save)\n",
    "        return\n",
    "    \n",
    "    @_plot_dict_member()\n",
    "    def time_vs_value(self, show=\"1\", name=\"time_vs_value\", save=False):  \n",
    "        \n",
    "        study_df = self.study.trials_dataframe()\n",
    "        \n",
    "        # compute time in minutes and the name for the hover functionality\n",
    "        study_df[\"time\"] = study_df.apply(lambda row: row['duration'].total_seconds()/60, axis=1)\n",
    "        study_df[\"name\"] = study_df.apply(lambda row: \"Trial \"+str(row['number']), axis=1)\n",
    "\n",
    "        # plot picture\n",
    "        fig = px.scatter(study_df, \n",
    "                         x=\"time\", y=\"value\",\n",
    "                         labels     = {\"time\":\"Training Time [min]\", \"value\":\"Objective Value\"},\n",
    "                         color      = \"state\",\n",
    "                         symbol     = \"state\",\n",
    "                         hover_name = \"name\", \n",
    "                         hover_data = {\"time\":True,\"value\":True,\"state\":False},\n",
    "                         log_y      = True,\n",
    "                        )\n",
    "        fig.update_traces(marker={'size': 8})\n",
    "\n",
    "        # plot and save image\n",
    "        self._handle_image(fig, show, name, save)\n",
    "        return\n",
    "    \n",
    "    @_plot_dict_member()\n",
    "    def parallel_plots(self, parallel_sets=[], show=\"1\", name=\"parallel\", save=False):\n",
    "        \n",
    "        if self.data_dict is not None:\n",
    "            parallel_sets = self.data_dict[\"parallel\"]\n",
    "        for conf in parallel_sets:        \n",
    "            # build suffix \n",
    "            suffix = \"_\" + conf[0]   # first is the suffix for the filename\n",
    "            \n",
    "            fig = ov.plot_parallel_coordinate(self.study, params=conf[1:])\n",
    "            self._handle_image(fig, show, name + suffix, save)\n",
    "\n",
    "        return\n",
    "    \n",
    "    @_plot_dict_member() \n",
    "    def contour_plots(self, contour_sets=[], show=\"1\", name=\"contour\", save=False):\n",
    "        \n",
    "        if self.data_dict is not None:\n",
    "            contour_sets = self.data_dict[\"contour\"]\n",
    "        for conf in contour_sets:        \n",
    "            # build suffix based on passed parameters\n",
    "            suffix = \"_\" + \"_\".join(conf)\n",
    "            \n",
    "            fig = ov.plot_contour(self.study, params=conf)\n",
    "            self._handle_image(fig, show, name + suffix, save)\n",
    "\n",
    "        return\n",
    "    \n",
    "    @_plot_dict_member()\n",
    "    def slice_plots(self, slice_sets=[], show=\"1\", name=\"slice\", save=False):\n",
    "        \n",
    "        if self.data_dict is not None:\n",
    "            slice_sets = self.data_dict[\"slice\"]\n",
    "        for conf in slice_sets:        \n",
    "            # build suffix based on passed parameters\n",
    "            suffix = \"_\" + \"_\".join(conf)\n",
    "            \n",
    "            fig = ov.plot_slice(self.study, params=conf)\n",
    "            self._handle_image(fig, show, name + suffix, save)\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_inspector = OptimizationInspector(study, OPTUNA_DIR, figsize=(900,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters sets for parallel plots \n",
    "parallel_sets = [[\"architecture\", #name suffix   \n",
    "                  \"DQN_units_ID\", \"DQN_activ\",\"target_net_update_steps\",\"softmax_init\",\"softmax_k\",\n",
    "                 ],\n",
    "                 [\"optimization\", #name suffix\n",
    "                  \"optim\",\"lr\",\"gamma\",\"bad_state_penalty\",\"angle_weight\",\n",
    "                 ],\n",
    "                ]\n",
    "\n",
    "# parameters sets for contour plots\n",
    "contour_sets = [[\"DQN_units_ID\",\"DQN_activ\"],\n",
    "                [\"lr\",\"softmax_init\"],\n",
    "                [\"lr\", \"gamma\"],\n",
    "                [\"DQN_units_ID\",\"angle_weight\"]\n",
    "               ]\n",
    "\n",
    "# parameters sets for slice plots\n",
    "slice_sets   = [[\"DQN_units_ID\",\"DQN_activ\",\"target_net_update_steps\",\"softmax_init\",\"softmax_k\",\n",
    "                 \"optim\",\"lr\",\"gamma\",\"bad_state_penalty\", \"angle_weight\",\n",
    "                ],\n",
    "               ]\n",
    "\n",
    "# parameters to use for importance plot\n",
    "importance_params = [\"target_net_update_steps\",\"softmax_init\",\"softmax_k\",\"optim\",\"lr\",\n",
    "                     \"gamma\",\"bad_state_penalty\",\"angle_weight\",\"DQN_units_ID\",\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optuna_inspector.plot_all(parallel_sets     = parallel_sets,\n",
    "                          contour_sets      = contour_sets,\n",
    "                          slice_sets        = slice_sets,\n",
    "                          importance_params = importance_params,\n",
    "                          save = True,\n",
    "                          show = \"1111111\", #\"1100010\",    # show options\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_inspector.print_summary()\n",
    "#optuna_inspector.save_best_hypers_json(OPTUNA_DIR+\"/CartPole-v1_best_hypers.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the best agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "hypers = study.best_trial.user_attrs[\"hypers\"]\n",
    "\n",
    "trial_id = study.best_trial.number\n",
    "value    = int(study.best_trial.value)\n",
    "path_to_model = OPTUNA_DIR + f\"/BestDQN-TrialID_{trial_id}-Solved_{value}\"\n",
    "\n",
    "# run test loop and save a video\n",
    "n_episodes = 10\n",
    "test_loop(path_to_model, \n",
    "          n_episodes   = n_episodes, \n",
    "          seed         = MAGIC_NUM, \n",
    "          video_folder = OPTUNA_DIR,\n",
    "          name_prefix  = f\"BestDQN-TrialID_{trial_id}\", \n",
    "          model_hypers = hypers,\n",
    "          render       = True,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nndl_2021_lab_07_deep_reinforcement_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
